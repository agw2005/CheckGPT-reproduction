{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3dcd89d",
   "metadata": {},
   "source": [
    "Contains pipeline to reproduce section 5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60e8ea3",
   "metadata": {},
   "source": [
    "Reproduction's working directory will always be `~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/` unless specified otherwise through comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a84e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.25\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "python -m nltk.downloader punkt\n",
    "date\n",
    "python --version\n",
    "pwd\n",
    "ls -a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0ba272",
   "metadata": {},
   "source": [
    "Adjustment to `dnn.py` for CNN classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfb9432",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "sed -i 's/if MODELID == 14:/if MODELID in [4]:/' dnn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a051e133",
   "metadata": {},
   "source": [
    "# CS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70137530",
   "metadata": {},
   "source": [
    "## Representation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843fbf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you are loading depends on it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n",
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.tar.gz HTTP/1.1\" 200 0\n",
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/default_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n",
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n",
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, memory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_invalid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1, dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_validation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameters=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_epochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate=0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_prob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attention_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_embed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoint_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_source_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi': False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data name: ground, data num: 50000\n",
      "ground. Complete 0/3500 (0.0%). Time used: 0.4073183536529541s. Overlong: 0\n",
      "ground. Complete 200/3500 (5.7%). Time used: 5.459798812866211s. Overlong: 0\n",
      "ground. Complete 400/3500 (11.4%). Time used: 9.99100947380066s. Overlong: 0\n",
      "ground. Complete 600/3500 (17.1%). Time used: 14.514983892440796s. Overlong: 0\n",
      "ground. Complete 800/3500 (22.9%). Time used: 19.386319398880005s. Overlong: 0\n",
      "ground. Complete 1000/3500 (28.6%). Time used: 24.302972555160522s. Overlong: 1\n",
      "ground. Complete 1200/3500 (34.3%). Time used: 29.649991512298584s. Overlong: 3\n",
      "ground. Complete 1400/3500 (40.0%). Time used: 34.562427282333374s. Overlong: 3\n",
      "ground. Complete 1600/3500 (45.7%). Time used: 39.59988236427307s. Overlong: 3\n",
      "ground. Complete 1800/3500 (51.4%). Time used: 44.70708441734314s. Overlong: 3\n",
      "ground. Complete 2000/3500 (57.1%). Time used: 49.73644685745239s. Overlong: 4\n",
      "ground. Complete 2200/3500 (62.9%). Time used: 54.36698627471924s. Overlong: 4\n",
      "ground. Complete 2400/3500 (68.6%). Time used: 58.82297205924988s. Overlong: 4\n",
      "ground. Complete 2600/3500 (74.3%). Time used: 63.37796878814697s. Overlong: 4\n",
      "ground. Complete 2800/3500 (80.0%). Time used: 68.49647521972656s. Overlong: 5\n",
      "ground. Complete 3000/3500 (85.7%). Time used: 73.78374576568604s. Overlong: 6\n",
      "ground. Complete 3200/3500 (91.4%). Time used: 78.86725234985352s. Overlong: 6\n",
      "ground. Complete 3400/3500 (97.1%). Time used: 84.1276183128357s. Overlong: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you are loading depends on it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n",
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.tar.gz HTTP/1.1\" 200 0\n",
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/default_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n",
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n",
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, memory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_invalid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1, dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_validation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameters=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_epochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate=0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_prob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attention_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_embed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoint_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_source_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi': False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data name: ground_task2, data num: 50000\n",
      "ground_task2. Complete 0/3500 (0.0%). Time used: 0.40270447731018066s. Overlong: 0\n",
      "ground_task2. Complete 200/3500 (5.7%). Time used: 5.309662818908691s. Overlong: 0\n",
      "ground_task2. Complete 400/3500 (11.4%). Time used: 9.968811988830566s. Overlong: 0\n",
      "ground_task2. Complete 600/3500 (17.1%). Time used: 14.595539569854736s. Overlong: 0\n",
      "ground_task2. Complete 800/3500 (22.9%). Time used: 19.3695068359375s. Overlong: 0\n",
      "ground_task2. Complete 1000/3500 (28.6%). Time used: 24.104721546173096s. Overlong: 0\n",
      "ground_task2. Complete 1200/3500 (34.3%). Time used: 28.977898120880127s. Overlong: 0\n",
      "ground_task2. Complete 1400/3500 (40.0%). Time used: 33.76575231552124s. Overlong: 0\n",
      "ground_task2. Complete 1600/3500 (45.7%). Time used: 38.58564829826355s. Overlong: 0\n",
      "ground_task2. Complete 1800/3500 (51.4%). Time used: 43.375879764556885s. Overlong: 0\n",
      "ground_task2. Complete 2000/3500 (57.1%). Time used: 48.16213059425354s. Overlong: 0\n",
      "ground_task2. Complete 2200/3500 (62.9%). Time used: 52.80085825920105s. Overlong: 0\n",
      "ground_task2. Complete 2400/3500 (68.6%). Time used: 57.39687204360962s. Overlong: 0\n",
      "ground_task2. Complete 2600/3500 (74.3%). Time used: 62.01806592941284s. Overlong: 0\n",
      "ground_task2. Complete 2800/3500 (80.0%). Time used: 66.83297109603882s. Overlong: 0\n",
      "ground_task2. Complete 3000/3500 (85.7%). Time used: 71.76072955131531s. Overlong: 0\n",
      "ground_task2. Complete 3200/3500 (91.4%). Time used: 76.54662227630615s. Overlong: 0\n",
      "ground_task2. Complete 3400/3500 (97.1%). Time used: 81.36053156852722s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you are loading depends on it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n",
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.tar.gz HTTP/1.1\" 200 0\n",
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/default_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n",
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n",
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, memory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_invalid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1, dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_validation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameters=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_epochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate=0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_prob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attention_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_embed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoint_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_source_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi': False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data name: gpt_task1_prompt1, data num: 50000\n",
      "gpt_task1_prompt1. Complete 0/3500 (0.0%). Time used: 0.4116628170013428s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 200/3500 (5.7%). Time used: 5.364044189453125s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 400/3500 (11.4%). Time used: 10.360404253005981s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 600/3500 (17.1%). Time used: 15.242652177810669s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 800/3500 (22.9%). Time used: 20.226491928100586s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 1000/3500 (28.6%). Time used: 25.02644920349121s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 1200/3500 (34.3%). Time used: 29.849957942962646s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 1400/3500 (40.0%). Time used: 34.76361966133118s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 1600/3500 (45.7%). Time used: 39.617663860321045s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 1800/3500 (51.4%). Time used: 44.55103802680969s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 2000/3500 (57.1%). Time used: 49.81881904602051s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 2200/3500 (62.9%). Time used: 54.81660795211792s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 2400/3500 (68.6%). Time used: 59.84122586250305s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 2600/3500 (74.3%). Time used: 64.87095308303833s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 2800/3500 (80.0%). Time used: 69.87377262115479s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 3000/3500 (85.7%). Time used: 74.91761898994446s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 3200/3500 (91.4%). Time used: 79.82341003417969s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 3400/3500 (97.1%). Time used: 84.85733962059021s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you are loading depends on it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n",
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.tar.gz HTTP/1.1\" 200 0\n",
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/default_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n",
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n",
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, memory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_invalid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1, dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_validation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameters=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_epochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate=0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_prob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attention_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_embed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoint_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_source_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi': False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data name: gpt_task1_prompt2, data num: 50000\n",
      "gpt_task1_prompt2. Complete 0/3500 (0.0%). Time used: 0.402695894241333s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 200/3500 (5.7%). Time used: 5.979631662368774s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 400/3500 (11.4%). Time used: 11.282078742980957s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 600/3500 (17.1%). Time used: 16.676786184310913s. Overlong: 1\n",
      "gpt_task1_prompt2. Complete 800/3500 (22.9%). Time used: 22.1079261302948s. Overlong: 1\n",
      "gpt_task1_prompt2. Complete 1000/3500 (28.6%). Time used: 27.36924147605896s. Overlong: 1\n",
      "gpt_task1_prompt2. Complete 1200/3500 (34.3%). Time used: 32.694849491119385s. Overlong: 1\n",
      "gpt_task1_prompt2. Complete 1400/3500 (40.0%). Time used: 38.18157172203064s. Overlong: 1\n",
      "gpt_task1_prompt2. Complete 1600/3500 (45.7%). Time used: 43.342028856277466s. Overlong: 1\n",
      "gpt_task1_prompt2. Complete 1800/3500 (51.4%). Time used: 48.67314291000366s. Overlong: 2\n",
      "gpt_task1_prompt2. Complete 2000/3500 (57.1%). Time used: 54.07149052619934s. Overlong: 2\n",
      "gpt_task1_prompt2. Complete 2200/3500 (62.9%). Time used: 59.53537321090698s. Overlong: 2\n",
      "gpt_task1_prompt2. Complete 2400/3500 (68.6%). Time used: 64.73688769340515s. Overlong: 2\n",
      "gpt_task1_prompt2. Complete 2600/3500 (74.3%). Time used: 69.97407341003418s. Overlong: 2\n",
      "gpt_task1_prompt2. Complete 2800/3500 (80.0%). Time used: 75.51504921913147s. Overlong: 2\n",
      "gpt_task1_prompt2. Complete 3000/3500 (85.7%). Time used: 80.80834817886353s. Overlong: 2\n",
      "gpt_task1_prompt2. Complete 3200/3500 (91.4%). Time used: 86.1176164150238s. Overlong: 2\n",
      "gpt_task1_prompt2. Complete 3400/3500 (97.1%). Time used: 91.65206027030945s. Overlong: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you are loading depends on it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n",
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.tar.gz HTTP/1.1\" 200 0\n",
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/default_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n",
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n",
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, memory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_invalid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1, dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_validation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameters=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_epochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate=0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_prob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attention_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_embed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoint_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_source_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi': False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data name: gpt_task1_prompt3, data num: 50000\n",
      "gpt_task1_prompt3. Complete 0/3500 (0.0%). Time used: 0.40508103370666504s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 200/3500 (5.7%). Time used: 5.402364015579224s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 400/3500 (11.4%). Time used: 10.233428001403809s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 600/3500 (17.1%). Time used: 14.939854860305786s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 800/3500 (22.9%). Time used: 19.679553031921387s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 1000/3500 (28.6%). Time used: 24.438411235809326s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 1200/3500 (34.3%). Time used: 29.27738666534424s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 1400/3500 (40.0%). Time used: 34.062872648239136s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 1600/3500 (45.7%). Time used: 38.91815114021301s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 1800/3500 (51.4%). Time used: 43.6828179359436s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 2000/3500 (57.1%). Time used: 48.44155287742615s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 2200/3500 (62.9%). Time used: 53.22378134727478s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 2400/3500 (68.6%). Time used: 57.978779792785645s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 2600/3500 (74.3%). Time used: 62.69804573059082s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 2800/3500 (80.0%). Time used: 67.58461952209473s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 3000/3500 (85.7%). Time used: 72.54543399810791s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 3200/3500 (91.4%). Time used: 77.54308223724365s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 3400/3500 (97.1%). Time used: 82.38996195793152s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you are loading depends on it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n",
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.tar.gz HTTP/1.1\" 200 0\n",
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/default_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n",
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n",
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, memory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_invalid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1, dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_validation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameters=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_epochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate=0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_prob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attention_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_embed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoint_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_source_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi': False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data name: gpt_task1_prompt4, data num: 50000\n",
      "gpt_task1_prompt4. Complete 0/3500 (0.0%). Time used: 0.39879441261291504s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 200/3500 (5.7%). Time used: 5.521645784378052s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 400/3500 (11.4%). Time used: 10.982731580734253s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 600/3500 (17.1%). Time used: 16.29967164993286s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 800/3500 (22.9%). Time used: 21.48034691810608s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 1000/3500 (28.6%). Time used: 26.788042545318604s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 1200/3500 (34.3%). Time used: 32.033281326293945s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 1400/3500 (40.0%). Time used: 37.24610209465027s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 1600/3500 (45.7%). Time used: 42.6558096408844s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 1800/3500 (51.4%). Time used: 47.73034858703613s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 2000/3500 (57.1%). Time used: 52.872199296951294s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 2200/3500 (62.9%). Time used: 58.24879455566406s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 2400/3500 (68.6%). Time used: 63.7301139831543s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 2600/3500 (74.3%). Time used: 69.08052062988281s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 2800/3500 (80.0%). Time used: 74.34127306938171s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 3000/3500 (85.7%). Time used: 79.56382417678833s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 3200/3500 (91.4%). Time used: 84.60873079299927s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 3400/3500 (97.1%). Time used: 89.7658178806305s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you are loading depends on it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n",
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.tar.gz HTTP/1.1\" 200 0\n",
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/default_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n",
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n",
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, memory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_invalid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1, dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_validation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameters=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_epochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate=0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_prob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attention_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_embed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoint_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_source_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi': False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data name: gpt_task2_prompt1, data num: 50000\n",
      "gpt_task2_prompt1. Complete 0/3500 (0.0%). Time used: 0.3955256938934326s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 200/3500 (5.7%). Time used: 5.026633024215698s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 400/3500 (11.4%). Time used: 9.278866052627563s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 600/3500 (17.1%). Time used: 13.440539360046387s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 800/3500 (22.9%). Time used: 17.926251888275146s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 1000/3500 (28.6%). Time used: 22.324015378952026s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 1200/3500 (34.3%). Time used: 27.025837182998657s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 1400/3500 (40.0%). Time used: 31.4577579498291s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 1600/3500 (45.7%). Time used: 36.01604509353638s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 1800/3500 (51.4%). Time used: 40.56517958641052s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 2000/3500 (57.1%). Time used: 45.064746141433716s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 2200/3500 (62.9%). Time used: 49.341859340667725s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 2400/3500 (68.6%). Time used: 53.48496890068054s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 2600/3500 (74.3%). Time used: 57.64803695678711s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 2800/3500 (80.0%). Time used: 62.19736409187317s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 3000/3500 (85.7%). Time used: 66.88759112358093s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 3200/3500 (91.4%). Time used: 71.36382675170898s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 3400/3500 (97.1%). Time used: 75.99751567840576s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you are loading depends on it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n",
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.tar.gz HTTP/1.1\" 200 0\n",
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/default_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n",
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n",
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, memory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_invalid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1, dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_validation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameters=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_epochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate=0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_prob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attention_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_embed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoint_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_source_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi': False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data name: gpt_task2_prompt2, data num: 50000\n",
      "gpt_task2_prompt2. Complete 0/3500 (0.0%). Time used: 0.3872864246368408s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 200/3500 (5.7%). Time used: 5.036123514175415s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 400/3500 (11.4%). Time used: 9.28717565536499s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 600/3500 (17.1%). Time used: 13.481512069702148s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 800/3500 (22.9%). Time used: 17.94196081161499s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 1000/3500 (28.6%). Time used: 22.358992099761963s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 1200/3500 (34.3%). Time used: 27.14979076385498s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 1400/3500 (40.0%). Time used: 31.594510555267334s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 1600/3500 (45.7%). Time used: 36.14778208732605s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 1800/3500 (51.4%). Time used: 40.74284863471985s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 2000/3500 (57.1%). Time used: 45.26711678504944s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 2200/3500 (62.9%). Time used: 49.56265211105347s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 2400/3500 (68.6%). Time used: 53.71996760368347s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 2600/3500 (74.3%). Time used: 57.94693636894226s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 2800/3500 (80.0%). Time used: 62.5240044593811s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 3000/3500 (85.7%). Time used: 67.26553511619568s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 3200/3500 (91.4%). Time used: 71.7888388633728s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 3400/3500 (97.1%). Time used: 76.46572375297546s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you are loading depends on it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n",
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.tar.gz HTTP/1.1\" 200 0\n",
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/default_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n",
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n",
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, memory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_invalid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1, dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_validation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameters=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_epochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate=0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_prob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attention_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_embed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoint_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_source_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi': False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data name: gpt_task2_prompt3, data num: 50000\n",
      "gpt_task2_prompt3. Complete 0/3500 (0.0%). Time used: 0.39501214027404785s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 200/3500 (5.7%). Time used: 4.999492406845093s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 400/3500 (11.4%). Time used: 9.188525676727295s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 600/3500 (17.1%). Time used: 13.335038661956787s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 800/3500 (22.9%). Time used: 17.721400022506714s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 1000/3500 (28.6%). Time used: 22.114532709121704s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 1200/3500 (34.3%). Time used: 26.737967014312744s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 1400/3500 (40.0%). Time used: 31.14098334312439s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 1600/3500 (45.7%). Time used: 35.67531204223633s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 1800/3500 (51.4%). Time used: 40.11366128921509s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 2000/3500 (57.1%). Time used: 44.515079736709595s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 2200/3500 (62.9%). Time used: 48.733184814453125s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 2400/3500 (68.6%). Time used: 52.850711822509766s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 2600/3500 (74.3%). Time used: 57.01445651054382s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 2800/3500 (80.0%). Time used: 61.576425075531006s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 3000/3500 (85.7%). Time used: 66.22208285331726s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 3200/3500 (91.4%). Time used: 70.70190834999084s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 3400/3500 (97.1%). Time used: 75.23976731300354s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you are loading depends on it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n",
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.tar.gz HTTP/1.1\" 200 0\n",
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/default_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n",
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n",
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, memory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_invalid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1, dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_validation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameters=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_epochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate=0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_prob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attention_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_embed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoint_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_source_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi': False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data name: gpt_task2_prompt4, data num: 50000\n",
      "gpt_task2_prompt4. Complete 0/3500 (0.0%). Time used: 0.3973052501678467s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 200/3500 (5.7%). Time used: 4.919877767562866s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 400/3500 (11.4%). Time used: 9.175220012664795s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 600/3500 (17.1%). Time used: 13.347159147262573s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 800/3500 (22.9%). Time used: 17.80426001548767s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 1000/3500 (28.6%). Time used: 22.172008991241455s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 1200/3500 (34.3%). Time used: 26.812111616134644s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 1400/3500 (40.0%). Time used: 31.196770429611206s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 1600/3500 (45.7%). Time used: 35.7298698425293s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 1800/3500 (51.4%). Time used: 40.18776893615723s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 2000/3500 (57.1%). Time used: 44.64206075668335s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 2200/3500 (62.9%). Time used: 48.94999718666077s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 2400/3500 (68.6%). Time used: 53.103516817092896s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 2600/3500 (74.3%). Time used: 57.270294189453125s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 2800/3500 (80.0%). Time used: 61.88072109222412s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 3000/3500 (85.7%). Time used: 66.49556541442871s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 3200/3500 (91.4%). Time used: 70.93196749687195s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 3400/3500 (97.1%). Time used: 75.56524562835693s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you are loading depends on it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n",
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.tar.gz HTTP/1.1\" 200 0\n",
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/default_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n",
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n",
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, memory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_invalid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1, dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_validation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameters=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_epochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate=0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_prob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attention_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_embed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoint_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_source_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi': False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data name: gpt_task3_prompt1, data num: 50000\n",
      "gpt_task3_prompt1. Complete 0/3500 (0.0%). Time used: 0.4705817699432373s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 200/3500 (5.7%). Time used: 5.2306225299835205s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 400/3500 (11.4%). Time used: 9.686450004577637s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 600/3500 (17.1%). Time used: 14.089624881744385s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 800/3500 (22.9%). Time used: 18.74783992767334s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 1000/3500 (28.6%). Time used: 23.41583490371704s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 1200/3500 (34.3%). Time used: 28.44668436050415s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 1400/3500 (40.0%). Time used: 33.15793204307556s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 1600/3500 (45.7%). Time used: 37.90179944038391s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 1800/3500 (51.4%). Time used: 42.65242886543274s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 2000/3500 (57.1%). Time used: 47.39825129508972s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 2200/3500 (62.9%). Time used: 51.90737247467041s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 2400/3500 (68.6%). Time used: 56.275578022003174s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 2600/3500 (74.3%). Time used: 60.72172808647156s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 2800/3500 (80.0%). Time used: 65.54099750518799s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 3000/3500 (85.7%). Time used: 70.4821400642395s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 3200/3500 (91.4%). Time used: 75.25030541419983s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 3400/3500 (97.1%). Time used: 80.05097103118896s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you are loading depends on it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n",
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.tar.gz HTTP/1.1\" 200 0\n",
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/default_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n",
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n",
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, memory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_invalid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1, dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_validation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameters=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_epochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate=0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_prob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attention_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_embed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoint_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_source_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi': False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data name: gpt_task3_prompt2, data num: 50000\n",
      "gpt_task3_prompt2. Complete 0/3500 (0.0%). Time used: 0.3947935104370117s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 200/3500 (5.7%). Time used: 5.314051628112793s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 400/3500 (11.4%). Time used: 9.874859809875488s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 600/3500 (17.1%). Time used: 14.387524604797363s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 800/3500 (22.9%). Time used: 19.19030213356018s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 1000/3500 (28.6%). Time used: 24.017327308654785s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 1200/3500 (34.3%). Time used: 29.234748601913452s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 1400/3500 (40.0%). Time used: 34.089704513549805s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 1600/3500 (45.7%). Time used: 39.02683758735657s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 1800/3500 (51.4%). Time used: 43.87672019004822s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 2000/3500 (57.1%). Time used: 48.746744871139526s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 2200/3500 (62.9%). Time used: 53.429893493652344s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 2400/3500 (68.6%). Time used: 57.87675189971924s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 2600/3500 (74.3%). Time used: 62.43215274810791s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 2800/3500 (80.0%). Time used: 67.49566841125488s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 3000/3500 (85.7%). Time used: 72.70694494247437s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 3200/3500 (91.4%). Time used: 77.65970730781555s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 3400/3500 (97.1%). Time used: 82.65191650390625s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you are loading depends on it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n",
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.tar.gz HTTP/1.1\" 200 0\n",
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/default_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n",
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n",
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, memory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_invalid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1, dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_validation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameters=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_epochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate=0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_prob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attention_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_embed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoint_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_source_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi': False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data name: gpt_task3_prompt3, data num: 50000\n",
      "gpt_task3_prompt3. Complete 0/3500 (0.0%). Time used: 0.3973422050476074s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete 200/3500 (5.7%). Time used: 5.323103666305542s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete 400/3500 (11.4%). Time used: 9.877285480499268s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete 600/3500 (17.1%). Time used: 14.384390830993652s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete 800/3500 (22.9%). Time used: 19.17679190635681s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete 1000/3500 (28.6%). Time used: 23.99072575569153s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete 1200/3500 (34.3%). Time used: 29.215209007263184s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete 1400/3500 (40.0%). Time used: 34.03899645805359s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete 1600/3500 (45.7%). Time used: 38.95498991012573s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete 1800/3500 (51.4%). Time used: 43.853899240493774s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete 2000/3500 (57.1%). Time used: 48.71867799758911s. Overlong: 1\n",
      "gpt_task3_prompt3. Complete 2200/3500 (62.9%). Time used: 53.36779808998108s. Overlong: 1\n",
      "gpt_task3_prompt3. Complete 2400/3500 (68.6%). Time used: 57.82828903198242s. Overlong: 1\n",
      "gpt_task3_prompt3. Complete 2600/3500 (74.3%). Time used: 62.39415001869202s. Overlong: 1\n",
      "gpt_task3_prompt3. Complete 2800/3500 (80.0%). Time used: 67.41843366622925s. Overlong: 1\n",
      "gpt_task3_prompt3. Complete 3000/3500 (85.7%). Time used: 72.59616446495056s. Overlong: 1\n",
      "gpt_task3_prompt3. Complete 3200/3500 (91.4%). Time used: 77.60509276390076s. Overlong: 1\n",
      "gpt_task3_prompt3. Complete 3400/3500 (97.1%). Time used: 82.75751185417175s. Overlong: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you are loading depends on it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n",
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.tar.gz HTTP/1.1\" 200 0\n",
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/default_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n",
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n",
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, memory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_invalid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1, dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_validation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameters=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_epochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate=0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_prob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attention_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_embed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoint_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_source_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi': False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data name: gpt_task3_prompt4, data num: 50000\n",
      "gpt_task3_prompt4. Complete 0/3500 (0.0%). Time used: 0.40334272384643555s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 200/3500 (5.7%). Time used: 6.29129958152771s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 400/3500 (11.4%). Time used: 11.870462894439697s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 600/3500 (17.1%). Time used: 17.35236430168152s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 800/3500 (22.9%). Time used: 23.09982132911682s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 1000/3500 (28.6%). Time used: 28.925183057785034s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 1200/3500 (34.3%). Time used: 34.995826721191406s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 1400/3500 (40.0%). Time used: 40.81754398345947s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 1600/3500 (45.7%). Time used: 46.610793113708496s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 1800/3500 (51.4%). Time used: 52.365198373794556s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 2000/3500 (57.1%). Time used: 58.19509220123291s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 2200/3500 (62.9%). Time used: 63.91253328323364s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 2400/3500 (68.6%). Time used: 69.4017105102539s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 2600/3500 (74.3%). Time used: 74.95734643936157s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 2800/3500 (80.0%). Time used: 80.92555022239685s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 3000/3500 (85.7%). Time used: 87.00532698631287s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 3200/3500 (91.4%). Time used: 92.81543684005737s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 3400/3500 (97.1%). Time used: 98.72041440010071s. Overlong: 0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python features.py CS 1 1 --gpt 0 --number 5000\n",
    "python features.py CS 2 1 --gpt 0 --number 5000\n",
    "\n",
    "python features.py CS 1 1 --gpt 1 --number 5000\n",
    "python features.py CS 1 2 --gpt 1 --number 5000\n",
    "python features.py CS 1 3 --gpt 1 --number 5000\n",
    "python features.py CS 1 4 --gpt 1 --number 5000\n",
    "\n",
    "python features.py CS 2 1 --gpt 1 --number 5000\n",
    "python features.py CS 2 2 --gpt 1 --number 5000\n",
    "python features.py CS 2 3 --gpt 1 --number 5000\n",
    "python features.py CS 2 4 --gpt 1 --number 5000\n",
    "\n",
    "python features.py CS 3 1 --gpt 1 --number 5000\n",
    "python features.py CS 3 2 --gpt 1 --number 5000\n",
    "python features.py CS 3 3 --gpt 1 --number 5000\n",
    "python features.py CS 3 4 --gpt 1 --number 5000\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3e9e9a",
   "metadata": {},
   "source": [
    "## CheckGPT Arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541dcf60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(domain='CS_3500', task=1, prompt=1, expid='CS_Train_BatchSize256_Task1_Prompt1', early=100.0, save=1, modelid=0, pretrain=0, saved_model=None, dataamount=100, trans=0, splitr=0.8, ablr=1.0, lr=0.0002, nepochs=100, test=0, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=0, v1=0, adam=1, seed=100, dropout=0.5, batchsize=256)\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Namespace(domain='CS_3500', task=1, prompt=2, expid='CS_Train_BatchSize256_Task1_Prompt2', early=100.0, save=1, modelid=0, pretrain=0, saved_model=None, dataamount=100, trans=0, splitr=0.8, ablr=1.0, lr=0.0002, nepochs=100, test=0, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=0, v1=0, adam=1, seed=100, dropout=0.5, batchsize=256)\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Namespace(domain='CS_3500', task=1, prompt=3, expid='CS_Train_BatchSize256_Task1_Prompt3', early=100.0, save=1, modelid=0, pretrain=0, saved_model=None, dataamount=100, trans=0, splitr=0.8, ablr=1.0, lr=0.0002, nepochs=100, test=0, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=0, v1=0, adam=1, seed=100, dropout=0.5, batchsize=256)\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Namespace(domain='CS_3500', task=1, prompt=4, expid='CS_Train_BatchSize256_Task1_Prompt4', early=100.0, save=1, modelid=0, pretrain=0, saved_model=None, dataamount=100, trans=0, splitr=0.8, ablr=1.0, lr=0.0002, nepochs=100, test=0, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=0, v1=0, adam=1, seed=100, dropout=0.5, batchsize=256)\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Namespace(domain='CS_3500', task=2, prompt=1, expid='CS_Train_BatchSize256_Task2_Prompt1', early=100.0, save=1, modelid=0, pretrain=0, saved_model=None, dataamount=100, trans=0, splitr=0.8, ablr=1.0, lr=0.0002, nepochs=100, test=0, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=0, v1=0, adam=1, seed=100, dropout=0.5, batchsize=256)\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Namespace(domain='CS_3500', task=2, prompt=2, expid='CS_Train_BatchSize256_Task2_Prompt2', early=100.0, save=1, modelid=0, pretrain=0, saved_model=None, dataamount=100, trans=0, splitr=0.8, ablr=1.0, lr=0.0002, nepochs=100, test=0, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=0, v1=0, adam=1, seed=100, dropout=0.5, batchsize=256)\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Namespace(domain='CS_3500', task=2, prompt=3, expid='CS_Train_BatchSize256_Task2_Prompt3', early=100.0, save=1, modelid=0, pretrain=0, saved_model=None, dataamount=100, trans=0, splitr=0.8, ablr=1.0, lr=0.0002, nepochs=100, test=0, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=0, v1=0, adam=1, seed=100, dropout=0.5, batchsize=256)\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Namespace(domain='CS_3500', task=2, prompt=4, expid='CS_Train_BatchSize256_Task2_Prompt4', early=100.0, save=1, modelid=0, pretrain=0, saved_model=None, dataamount=100, trans=0, splitr=0.8, ablr=1.0, lr=0.0002, nepochs=100, test=0, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=0, v1=0, adam=1, seed=100, dropout=0.5, batchsize=256)\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Namespace(domain='CS_3500', task=3, prompt=1, expid='CS_Train_BatchSize256_Task3_Prompt1', early=100.0, save=1, modelid=0, pretrain=0, saved_model=None, dataamount=100, trans=0, splitr=0.8, ablr=1.0, lr=0.0002, nepochs=100, test=0, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=0, v1=0, adam=1, seed=100, dropout=0.5, batchsize=256)\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Namespace(domain='CS_3500', task=3, prompt=2, expid='CS_Train_BatchSize256_Task3_Prompt2', early=100.0, save=1, modelid=0, pretrain=0, saved_model=None, dataamount=100, trans=0, splitr=0.8, ablr=1.0, lr=0.0002, nepochs=100, test=0, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=0, v1=0, adam=1, seed=100, dropout=0.5, batchsize=256)\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Namespace(domain='CS_3500', task=3, prompt=3, expid='CS_Train_BatchSize256_Task3_Prompt3', early=100.0, save=1, modelid=0, pretrain=0, saved_model=None, dataamount=100, trans=0, splitr=0.8, ablr=1.0, lr=0.0002, nepochs=100, test=0, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=0, v1=0, adam=1, seed=100, dropout=0.5, batchsize=256)\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Namespace(domain='CS_3500', task=3, prompt=4, expid='CS_Train_BatchSize256_Task3_Prompt4', early=100.0, save=1, modelid=0, pretrain=0, saved_model=None, dataamount=100, trans=0, splitr=0.8, ablr=1.0, lr=0.0002, nepochs=100, test=0, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=0, v1=0, adam=1, seed=100, dropout=0.5, batchsize=256)\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n",
      "Best model saved.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py CS_5000 1 1 CS_Train_CheckGPTArch_Task1_Prompt1 --batchsize 256\n",
    "python dnn.py CS_5000 1 2 CS_Train_CheckGPTArch_Task1_Prompt2 --batchsize 256\n",
    "python dnn.py CS_5000 1 3 CS_Train_CheckGPTArch_Task1_Prompt3 --batchsize 256\n",
    "python dnn.py CS_5000 1 4 CS_Train_CheckGPTArch_Task1_Prompt4 --batchsize 256\n",
    "\n",
    "python dnn.py CS_5000 2 1 CS_Train_CheckGPTArch_Task2_Prompt1 --batchsize 256\n",
    "python dnn.py CS_5000 2 2 CS_Train_CheckGPTArch_Task2_Prompt2 --batchsize 256\n",
    "python dnn.py CS_5000 2 3 CS_Train_CheckGPTArch_Task2_Prompt3 --batchsize 256\n",
    "python dnn.py CS_5000 2 4 CS_Train_CheckGPTArch_Task2_Prompt4 --batchsize 256\n",
    "\n",
    "python dnn.py CS_5000 3 1 CS_Train_CheckGPTArch_Task3_Prompt1 --batchsize 256\n",
    "python dnn.py CS_5000 3 2 CS_Train_CheckGPTArch_Task3_Prompt2 --batchsize 256\n",
    "python dnn.py CS_5000 3 3 CS_Train_CheckGPTArch_Task3_Prompt3 --batchsize 256\n",
    "python dnn.py CS_5000 3 4 CS_Train_CheckGPTArch_Task3_Prompt4 --batchsize 256\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb0336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(domain='CS_3500', task=1, prompt=1, expid='CS_Test_BatchSize256_Task1_Prompt1', early=100.0, save=1, modelid=0, pretrain=1, saved_model='./exp/CS_Train_BatchSize256_Task1_Prompt1/Best_CS_3500_Task1.pth', dataamount=100, trans=0, splitr=0.8, ablr=1.0, lr=0.0002, nepochs=100, test=1, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=1, v1=0, adam=1, seed=100, dropout=0.5, batchsize=512)\n",
      "Batch: [0/3], Time used: 4.6967s\n",
      "CS_35001 Epoch:0, Test accuracy: 100.0000%, Acc_GPT: 100.0000%, Acc_Human: 100.0000%, F1: 1.0000\n",
      "Namespace(domain='CS_3500', task=1, prompt=2, expid='CS_Test_BatchSize256_Task1_Prompt2', early=100.0, save=1, modelid=0, pretrain=1, saved_model='./exp/CS_Train_BatchSize256_Task1_Prompt2/Best_CS_3500_Task1.pth', dataamount=100, trans=0, splitr=0.8, ablr=1.0, lr=0.0002, nepochs=100, test=1, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=1, v1=0, adam=1, seed=100, dropout=0.5, batchsize=512)\n",
      "Batch: [0/3], Time used: 4.6937s\n",
      "CS_35001 Epoch:0, Test accuracy: 100.0000%, Acc_GPT: 100.0000%, Acc_Human: 100.0000%, F1: 1.0000\n",
      "Namespace(domain='CS_3500', task=1, prompt=3, expid='CS_Test_BatchSize256_Task1_Prompt3', early=100.0, save=1, modelid=0, pretrain=1, saved_model='./exp/CS_Train_BatchSize256_Task1_Prompt3/Best_CS_3500_Task1.pth', dataamount=100, trans=0, splitr=0.8, ablr=1.0, lr=0.0002, nepochs=100, test=1, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=1, v1=0, adam=1, seed=100, dropout=0.5, batchsize=512)\n",
      "Batch: [0/3], Time used: 5.0913s\n",
      "CS_35001 Epoch:0, Test accuracy: 100.0000%, Acc_GPT: 100.0000%, Acc_Human: 100.0000%, F1: 1.0000\n",
      "Namespace(domain='CS_3500', task=1, prompt=4, expid='CS_Test_BatchSize256_Task1_Prompt4', early=100.0, save=1, modelid=0, pretrain=1, saved_model='./exp/CS_Train_BatchSize256_Task1_Prompt4/Best_CS_3500_Task1.pth', dataamount=100, trans=0, splitr=0.8, ablr=1.0, lr=0.0002, nepochs=100, test=1, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=1, v1=0, adam=1, seed=100, dropout=0.5, batchsize=512)\n",
      "Batch: [0/3], Time used: 5.1822s\n",
      "CS_35001 Epoch:0, Test accuracy: 100.0000%, Acc_GPT: 100.0000%, Acc_Human: 100.0000%, F1: 1.0000\n",
      "Namespace(domain='CS_3500', task=2, prompt=1, expid='CS_Test_BatchSize256_Task2_Prompt1', early=100.0, save=1, modelid=0, pretrain=1, saved_model='./exp/CS_Train_BatchSize256_Task2_Prompt1/Best_CS_3500_Task2.pth', dataamount=100, trans=0, splitr=0.8, ablr=1.0, lr=0.0002, nepochs=100, test=1, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=1, v1=0, adam=1, seed=100, dropout=0.5, batchsize=512)\n",
      "Batch: [0/3], Time used: 5.3321s\n",
      "CS_35002 Epoch:0, Test accuracy: 99.6429%, Acc_GPT: 99.5714%, Acc_Human: 99.7143%, F1: 0.9964\n",
      "Namespace(domain='CS_3500', task=2, prompt=2, expid='CS_Test_BatchSize256_Task2_Prompt2', early=100.0, save=1, modelid=0, pretrain=1, saved_model='./exp/CS_Train_BatchSize256_Task2_Prompt2/Best_CS_3500_Task2.pth', dataamount=100, trans=0, splitr=0.8, ablr=1.0, lr=0.0002, nepochs=100, test=1, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=1, v1=0, adam=1, seed=100, dropout=0.5, batchsize=512)\n",
      "Batch: [0/3], Time used: 4.0910s\n",
      "CS_35002 Epoch:0, Test accuracy: 99.1429%, Acc_GPT: 98.8571%, Acc_Human: 99.4286%, F1: 0.9914\n",
      "Namespace(domain='CS_3500', task=2, prompt=3, expid='CS_Test_BatchSize256_Task2_Prompt3', early=100.0, save=1, modelid=0, pretrain=1, saved_model='./exp/CS_Train_BatchSize256_Task2_Prompt3/Best_CS_3500_Task2.pth', dataamount=100, trans=0, splitr=0.8, ablr=1.0, lr=0.0002, nepochs=100, test=1, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=1, v1=0, adam=1, seed=100, dropout=0.5, batchsize=512)\n",
      "Batch: [0/3], Time used: 4.3870s\n",
      "CS_35002 Epoch:0, Test accuracy: 99.4286%, Acc_GPT: 99.2857%, Acc_Human: 99.5714%, F1: 0.9943\n",
      "Namespace(domain='CS_3500', task=2, prompt=4, expid='CS_Test_BatchSize256_Task2_Prompt4', early=100.0, save=1, modelid=0, pretrain=1, saved_model='./exp/CS_Train_BatchSize256_Task2_Prompt4/Best_CS_3500_Task2.pth', dataamount=100, trans=0, splitr=0.8, ablr=1.0, lr=0.0002, nepochs=100, test=1, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=1, v1=0, adam=1, seed=100, dropout=0.5, batchsize=512)\n",
      "Batch: [0/3], Time used: 4.7219s\n",
      "CS_35002 Epoch:0, Test accuracy: 99.4286%, Acc_GPT: 99.4286%, Acc_Human: 99.4286%, F1: 0.9943\n",
      "Namespace(domain='CS_3500', task=3, prompt=1, expid='CS_Test_BatchSize256_Task3_Prompt1', early=100.0, save=1, modelid=0, pretrain=1, saved_model='./exp/CS_Train_BatchSize256_Task3_Prompt1/Best_CS_3500_Task3.pth', dataamount=100, trans=0, splitr=0.8, ablr=1.0, lr=0.0002, nepochs=100, test=1, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=1, v1=0, adam=1, seed=100, dropout=0.5, batchsize=512)\n",
      "Batch: [0/3], Time used: 4.8469s\n",
      "CS_35003 Epoch:0, Test accuracy: 97.3571%, Acc_GPT: 97.4286%, Acc_Human: 97.2857%, F1: 0.9736\n",
      "Namespace(domain='CS_3500', task=3, prompt=2, expid='CS_Test_BatchSize256_Task3_Prompt2', early=100.0, save=1, modelid=0, pretrain=1, saved_model='./exp/CS_Train_BatchSize256_Task3_Prompt2/Best_CS_3500_Task3.pth', dataamount=100, trans=0, splitr=0.8, ablr=1.0, lr=0.0002, nepochs=100, test=1, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=1, v1=0, adam=1, seed=100, dropout=0.5, batchsize=512)\n",
      "Batch: [0/3], Time used: 3.9371s\n",
      "CS_35003 Epoch:0, Test accuracy: 97.4286%, Acc_GPT: 98.2857%, Acc_Human: 96.5714%, F1: 0.9745\n",
      "Namespace(domain='CS_3500', task=3, prompt=3, expid='CS_Test_BatchSize256_Task3_Prompt3', early=100.0, save=1, modelid=0, pretrain=1, saved_model='./exp/CS_Train_BatchSize256_Task3_Prompt3/Best_CS_3500_Task3.pth', dataamount=100, trans=0, splitr=0.8, ablr=1.0, lr=0.0002, nepochs=100, test=1, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=1, v1=0, adam=1, seed=100, dropout=0.5, batchsize=512)\n",
      "Batch: [0/3], Time used: 4.4243s\n",
      "CS_35003 Epoch:0, Test accuracy: 97.8571%, Acc_GPT: 97.1429%, Acc_Human: 98.5714%, F1: 0.9784\n",
      "Namespace(domain='CS_3500', task=3, prompt=4, expid='CS_Test_BatchSize256_Task3_Prompt4', early=100.0, save=1, modelid=0, pretrain=1, saved_model='./exp/CS_Train_BatchSize256_Task3_Prompt4/Best_CS_3500_Task3.pth', dataamount=100, trans=0, splitr=0.8, ablr=1.0, lr=0.0002, nepochs=100, test=1, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=1, v1=0, adam=1, seed=100, dropout=0.5, batchsize=512)\n",
      "Batch: [0/3], Time used: 3.8795s\n",
      "CS_35003 Epoch:0, Test accuracy: 98.8571%, Acc_GPT: 98.8571%, Acc_Human: 98.8571%, F1: 0.9886\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py CS_5000 1 1 CS_Test_CheckGPTArch_Task1_Prompt1 --pretrain 1 --test 1 --saved-model ./exp/CS_Train_CheckGPTArch_Task1_Prompt1/Best_CS_5000_Task1.pth\n",
    "python dnn.py CS_5000 1 2 CS_Test_CheckGPTArch_Task1_Prompt2 --pretrain 1 --test 1 --saved-model ./exp/CS_Train_CheckGPTArch_Task1_Prompt2/Best_CS_5000_Task1.pth\n",
    "python dnn.py CS_5000 1 3 CS_Test_CheckGPTArch_Task1_Prompt3 --pretrain 1 --test 1 --saved-model ./exp/CS_Train_CheckGPTArch_Task1_Prompt3/Best_CS_5000_Task1.pth\n",
    "python dnn.py CS_5000 1 4 CS_Test_CheckGPTArch_Task1_Prompt4 --pretrain 1 --test 1 --saved-model ./exp/CS_Train_CheckGPTArch_Task1_Prompt4/Best_CS_5000_Task1.pth\n",
    "\n",
    "python dnn.py CS_5000 2 1 CS_Test_CheckGPTArch_Task2_Prompt1 --pretrain 1 --test 1 --saved-model ./exp/CS_Train_CheckGPTArch_Task2_Prompt1/Best_CS_5000_Task2.pth\n",
    "python dnn.py CS_5000 2 2 CS_Test_CheckGPTArch_Task2_Prompt2 --pretrain 1 --test 1 --saved-model ./exp/CS_Train_CheckGPTArch_Task2_Prompt2/Best_CS_5000_Task2.pth\n",
    "python dnn.py CS_5000 2 3 CS_Test_CheckGPTArch_Task2_Prompt3 --pretrain 1 --test 1 --saved-model ./exp/CS_Train_CheckGPTArch_Task2_Prompt3/Best_CS_5000_Task2.pth\n",
    "python dnn.py CS_5000 2 4 CS_Test_CheckGPTArch_Task2_Prompt4 --pretrain 1 --test 1 --saved-model ./exp/CS_Train_CheckGPTArch_Task2_Prompt4/Best_CS_5000_Task2.pth\n",
    "\n",
    "python dnn.py CS_5000 3 1 CS_Test_CheckGPTArch_Task3_Prompt1 --pretrain 1 --test 1 --saved-model ./exp/CS_Train_CheckGPTArch_Task3_Prompt1/Best_CS_5000_Task3.pth\n",
    "python dnn.py CS_5000 3 2 CS_Test_CheckGPTArch_Task3_Prompt2 --pretrain 1 --test 1 --saved-model ./exp/CS_Train_CheckGPTArch_Task3_Prompt2/Best_CS_5000_Task3.pth\n",
    "python dnn.py CS_5000 3 3 CS_Test_CheckGPTArch_Task3_Prompt3 --pretrain 1 --test 1 --saved-model ./exp/CS_Train_CheckGPTArch_Task3_Prompt3/Best_CS_5000_Task3.pth\n",
    "python dnn.py CS_5000 3 4 CS_Test_CheckGPTArch_Task3_Prompt4 --pretrain 1 --test 1 --saved-model ./exp/CS_Train_CheckGPTArch_Task3_Prompt4/Best_CS_5000_Task3.pth\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f452aa56",
   "metadata": {},
   "source": [
    "## RCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc73ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py CS_5000 1 1 CS_Train_RCH_Task1_Prompt1 --batchsize 256 --modelid 2\n",
    "python dnn.py CS_5000 1 2 CS_Train_RCH_Task1_Prompt2 --batchsize 256 --modelid 2\n",
    "python dnn.py CS_5000 1 3 CS_Train_RCH_Task1_Prompt3 --batchsize 256 --modelid 2\n",
    "python dnn.py CS_5000 1 4 CS_Train_RCH_Task1_Prompt4 --batchsize 256 --modelid 2\n",
    "\n",
    "python dnn.py CS_5000 2 1 CS_Train_RCH_Task2_Prompt1 --batchsize 256 --modelid 2\n",
    "python dnn.py CS_5000 2 2 CS_Train_RCH_Task2_Prompt2 --batchsize 256 --modelid 2\n",
    "python dnn.py CS_5000 2 3 CS_Train_RCH_Task2_Prompt3 --batchsize 256 --modelid 2\n",
    "python dnn.py CS_5000 2 4 CS_Train_RCH_Task2_Prompt4 --batchsize 256 --modelid 2\n",
    "\n",
    "python dnn.py CS_5000 3 1 CS_Train_RCH_Task3_Prompt1 --batchsize 256 --modelid 2\n",
    "python dnn.py CS_5000 3 2 CS_Train_RCH_Task3_Prompt2 --batchsize 256 --modelid 2\n",
    "python dnn.py CS_5000 3 3 CS_Train_RCH_Task3_Prompt3 --batchsize 256 --modelid 2\n",
    "python dnn.py CS_5000 3 4 CS_Train_RCH_Task3_Prompt4 --batchsize 256 --modelid 2\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98001633",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py CS_5000 1 1 CS_Test_RCH_Task1_Prompt1 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/CS_Train_RCH_Task1_Prompt1/Best_CS_5000_Task1.pth\n",
    "python dnn.py CS_5000 1 2 CS_Test_RCH_Task1_Prompt2 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/CS_Train_RCH_Task1_Prompt2/Best_CS_5000_Task1.pth\n",
    "python dnn.py CS_5000 1 3 CS_Test_RCH_Task1_Prompt3 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/CS_Train_RCH_Task1_Prompt3/Best_CS_5000_Task1.pth\n",
    "python dnn.py CS_5000 1 4 CS_Test_RCH_Task1_Prompt4 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/CS_Train_RCH_Task1_Prompt4/Best_CS_5000_Task1.pth\n",
    "\n",
    "python dnn.py CS_5000 2 1 CS_Test_RCH_Task2_Prompt1 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/CS_Train_RCH_Task2_Prompt1/Best_CS_5000_Task2.pth\n",
    "python dnn.py CS_5000 2 2 CS_Test_RCH_Task2_Prompt2 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/CS_Train_RCH_Task2_Prompt2/Best_CS_5000_Task2.pth\n",
    "python dnn.py CS_5000 2 3 CS_Test_RCH_Task2_Prompt3 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/CS_Train_RCH_Task2_Prompt3/Best_CS_5000_Task2.pth\n",
    "python dnn.py CS_5000 2 4 CS_Test_RCH_Task2_Prompt4 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/CS_Train_RCH_Task2_Prompt4/Best_CS_5000_Task2.pth\n",
    "\n",
    "python dnn.py CS_5000 3 1 CS_Test_RCH_Task3_Prompt1 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/CS_Train_RCH_Task3_Prompt1/Best_CS_5000_Task3.pth\n",
    "python dnn.py CS_5000 3 2 CS_Test_RCH_Task3_Prompt2 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/CS_Train_RCH_Task3_Prompt2/Best_CS_5000_Task3.pth\n",
    "python dnn.py CS_5000 3 3 CS_Test_RCH_Task3_Prompt3 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/CS_Train_RCH_Task3_Prompt3/Best_CS_5000_Task3.pth\n",
    "python dnn.py CS_5000 3 4 CS_Test_RCH_Task3_Prompt4 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/CS_Train_RCH_Task3_Prompt4/Best_CS_5000_Task3.pth\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df8bd50",
   "metadata": {},
   "source": [
    "## MLP-Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea45e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py CS_5000 1 1 CS_Train_MLP_Task1_Prompt1 --batchsize 256 --modelid 3\n",
    "python dnn.py CS_5000 1 2 CS_Train_MLP_Task1_Prompt2 --batchsize 256 --modelid 3\n",
    "python dnn.py CS_5000 1 3 CS_Train_MLP_Task1_Prompt3 --batchsize 256 --modelid 3\n",
    "python dnn.py CS_5000 1 4 CS_Train_MLP_Task1_Prompt4 --batchsize 256 --modelid 3\n",
    "\n",
    "python dnn.py CS_5000 2 1 CS_Train_MLP_Task2_Prompt1 --batchsize 256 --modelid 3\n",
    "python dnn.py CS_5000 2 2 CS_Train_MLP_Task2_Prompt2 --batchsize 256 --modelid 3\n",
    "python dnn.py CS_5000 2 3 CS_Train_MLP_Task2_Prompt3 --batchsize 256 --modelid 3\n",
    "python dnn.py CS_5000 2 4 CS_Train_MLP_Task2_Prompt4 --batchsize 256 --modelid 3\n",
    "\n",
    "python dnn.py CS_5000 3 1 CS_Train_MLP_Task3_Prompt1 --batchsize 256 --modelid 3\n",
    "python dnn.py CS_5000 3 2 CS_Train_MLP_Task3_Prompt2 --batchsize 256 --modelid 3\n",
    "python dnn.py CS_5000 3 3 CS_Train_MLP_Task3_Prompt3 --batchsize 256 --modelid 3\n",
    "python dnn.py CS_5000 3 4 CS_Train_MLP_Task3_Prompt4 --batchsize 256 --modelid 3\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f603966",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "\n",
    "date\n",
    "\n",
    "python dnn.py CS_5000 1 1 CS_Test_MLP_Task1_Prompt1 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/CS_Train_MLP_Task1_Prompt1/Best_CS_5000_Task1.pth\n",
    "python dnn.py CS_5000 1 2 CS_Test_MLP_Task1_Prompt2 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/CS_Train_MLP_Task1_Prompt2/Best_CS_5000_Task1.pth\n",
    "python dnn.py CS_5000 1 3 CS_Test_MLP_Task1_Prompt3 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/CS_Train_MLP_Task1_Prompt3/Best_CS_5000_Task1.pth\n",
    "python dnn.py CS_5000 1 4 CS_Test_MLP_Task1_Prompt4 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/CS_Train_MLP_Task1_Prompt4/Best_CS_5000_Task1.pth\n",
    "\n",
    "python dnn.py CS_5000 2 1 CS_Test_MLP_Task2_Prompt1 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/CS_Train_MLP_Task2_Prompt1/Best_CS_5000_Task2.pth\n",
    "python dnn.py CS_5000 2 2 CS_Test_MLP_Task2_Prompt2 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/CS_Train_MLP_Task2_Prompt2/Best_CS_5000_Task2.pth\n",
    "python dnn.py CS_5000 2 3 CS_Test_MLP_Task2_Prompt3 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/CS_Train_MLP_Task2_Prompt3/Best_CS_5000_Task2.pth\n",
    "python dnn.py CS_5000 2 4 CS_Test_MLP_Task2_Prompt4 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/CS_Train_MLP_Task2_Prompt4/Best_CS_5000_Task2.pth\n",
    "\n",
    "python dnn.py CS_5000 3 1 CS_Test_MLP_Task3_Prompt1 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/CS_Train_MLP_Task3_Prompt1/Best_CS_5000_Task3.pth\n",
    "python dnn.py CS_5000 3 2 CS_Test_MLP_Task3_Prompt2 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/CS_Train_MLP_Task3_Prompt2/Best_CS_5000_Task3.pth\n",
    "python dnn.py CS_5000 3 3 CS_Test_MLP_Task3_Prompt3 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/CS_Train_MLP_Task3_Prompt3/Best_CS_5000_Task3.pth\n",
    "python dnn.py CS_5000 3 4 CS_Test_MLP_Task3_Prompt4 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/CS_Train_MLP_Task3_Prompt4/Best_CS_5000_Task3.pth\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad44b9de",
   "metadata": {},
   "source": [
    "## AlexNet-like CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e8a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py CS_5000 1 1 CS_Train_CNN_Task1_Prompt1 --batchsize 256 --modelid 4\n",
    "python dnn.py CS_5000 1 2 CS_Train_CNN_Task1_Prompt2 --batchsize 256 --modelid 4\n",
    "python dnn.py CS_5000 1 3 CS_Train_CNN_Task1_Prompt3 --batchsize 256 --modelid 4\n",
    "python dnn.py CS_5000 1 4 CS_Train_CNN_Task1_Prompt4 --batchsize 256 --modelid 4\n",
    "\n",
    "python dnn.py CS_5000 2 1 CS_Train_CNN_Task2_Prompt1 --batchsize 256 --modelid 4\n",
    "python dnn.py CS_5000 2 2 CS_Train_CNN_Task2_Prompt2 --batchsize 256 --modelid 4\n",
    "python dnn.py CS_5000 2 3 CS_Train_CNN_Task2_Prompt3 --batchsize 256 --modelid 4\n",
    "python dnn.py CS_5000 2 4 CS_Train_CNN_Task2_Prompt4 --batchsize 256 --modelid 4\n",
    "\n",
    "python dnn.py CS_5000 3 1 CS_Train_CNN_Task3_Prompt1 --batchsize 256 --modelid 4\n",
    "python dnn.py CS_5000 3 2 CS_Train_CNN_Task3_Prompt2 --batchsize 256 --modelid 4\n",
    "python dnn.py CS_5000 3 3 CS_Train_CNN_Task3_Prompt3 --batchsize 256 --modelid 4\n",
    "python dnn.py CS_5000 3 4 CS_Train_CNN_Task3_Prompt4 --batchsize 256 --modelid 4\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea570e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py CS_5000 1 1 CS_Test_CNN_Task1_Prompt1 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/CS_Train_CNN_Task1_Prompt1/Best_CS_5000_Task1.pth\n",
    "python dnn.py CS_5000 1 2 CS_Test_CNN_Task1_Prompt2 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/CS_Train_CNN_Task1_Prompt2/Best_CS_5000_Task1.pth\n",
    "python dnn.py CS_5000 1 3 CS_Test_CNN_Task1_Prompt3 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/CS_Train_CNN_Task1_Prompt3/Best_CS_5000_Task1.pth\n",
    "python dnn.py CS_5000 1 4 CS_Test_CNN_Task1_Prompt4 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/CS_Train_CNN_Task1_Prompt4/Best_CS_5000_Task1.pth\n",
    "\n",
    "python dnn.py CS_5000 2 1 CS_Test_CNN_Task2_Prompt1 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/CS_Train_CNN_Task2_Prompt1/Best_CS_5000_Task2.pth\n",
    "python dnn.py CS_5000 2 2 CS_Test_CNN_Task2_Prompt2 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/CS_Train_CNN_Task2_Prompt2/Best_CS_5000_Task2.pth\n",
    "python dnn.py CS_5000 2 3 CS_Test_CNN_Task2_Prompt3 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/CS_Train_CNN_Task2_Prompt3/Best_CS_5000_Task2.pth\n",
    "python dnn.py CS_5000 2 4 CS_Test_CNN_Task2_Prompt4 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/CS_Train_CNN_Task2_Prompt4/Best_CS_5000_Task2.pth\n",
    "\n",
    "python dnn.py CS_5000 3 1 CS_Test_CNN_Task3_Prompt1 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/CS_Train_CNN_Task3_Prompt1/Best_CS_5000_Task3.pth\n",
    "python dnn.py CS_5000 3 2 CS_Test_CNN_Task3_Prompt2 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/CS_Train_CNN_Task3_Prompt2/Best_CS_5000_Task3.pth\n",
    "python dnn.py CS_5000 3 3 CS_Test_CNN_Task3_Prompt3 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/CS_Train_CNN_Task3_Prompt3/Best_CS_5000_Task3.pth\n",
    "python dnn.py CS_5000 3 4 CS_Test_CNN_Task3_Prompt4 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/CS_Train_CNN_Task3_Prompt4/Best_CS_5000_Task3.pth\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666a4656",
   "metadata": {},
   "source": [
    "## BiLSTM w/o attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40413b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py CS_5000 1 1 CS_Train_BiLSTMwoAttention_Task1_Prompt1 --batchsize 256 --modelid 1\n",
    "python dnn.py CS_5000 1 2 CS_Train_BiLSTMwoAttention_Task1_Prompt2 --batchsize 256 --modelid 1\n",
    "python dnn.py CS_5000 1 3 CS_Train_BiLSTMwoAttention_Task1_Prompt3 --batchsize 256 --modelid 1\n",
    "python dnn.py CS_5000 1 4 CS_Train_BiLSTMwoAttention_Task1_Prompt4 --batchsize 256 --modelid 1\n",
    "\n",
    "python dnn.py CS_5000 2 1 CS_Train_BiLSTMwoAttention_Task2_Prompt1 --batchsize 256 --modelid 1\n",
    "python dnn.py CS_5000 2 2 CS_Train_BiLSTMwoAttention_Task2_Prompt2 --batchsize 256 --modelid 1\n",
    "python dnn.py CS_5000 2 3 CS_Train_BiLSTMwoAttention_Task2_Prompt3 --batchsize 256 --modelid 1\n",
    "python dnn.py CS_5000 2 4 CS_Train_BiLSTMwoAttention_Task2_Prompt4 --batchsize 256 --modelid 1\n",
    "\n",
    "python dnn.py CS_5000 3 1 CS_Train_BiLSTMwoAttention_Task3_Prompt1 --batchsize 256 --modelid 1\n",
    "python dnn.py CS_5000 3 2 CS_Train_BiLSTMwoAttention_Task3_Prompt2 --batchsize 256 --modelid 1\n",
    "python dnn.py CS_5000 3 3 CS_Train_BiLSTMwoAttention_Task3_Prompt3 --batchsize 256 --modelid 1\n",
    "python dnn.py CS_5000 3 4 CS_Train_BiLSTMwoAttention_Task3_Prompt4 --batchsize 256 --modelid 1\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476a0d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py CS_5000 1 1 CS_Test_BiLSTMwoAttention_Task1_Prompt1 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/CS_Train_BiLSTMwoAttention_Task1_Prompt1/Best_CS_5000_Task1.pth\n",
    "python dnn.py CS_5000 1 2 CS_Test_BiLSTMwoAttention_Task1_Prompt2 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/CS_Train_BiLSTMwoAttention_Task1_Prompt2/Best_CS_5000_Task1.pth\n",
    "python dnn.py CS_5000 1 3 CS_Test_BiLSTMwoAttention_Task1_Prompt3 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/CS_Train_BiLSTMwoAttention_Task1_Prompt3/Best_CS_5000_Task1.pth\n",
    "python dnn.py CS_5000 1 4 CS_Test_BiLSTMwoAttention_Task1_Prompt4 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/CS_Train_BiLSTMwoAttention_Task1_Prompt4/Best_CS_5000_Task1.pth\n",
    "\n",
    "python dnn.py CS_5000 2 1 CS_Test_BiLSTMwoAttention_Task2_Prompt1 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/CS_Train_BiLSTMwoAttention_Task2_Prompt1/Best_CS_5000_Task2.pth\n",
    "python dnn.py CS_5000 2 2 CS_Test_BiLSTMwoAttention_Task2_Prompt2 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/CS_Train_BiLSTMwoAttention_Task2_Prompt2/Best_CS_5000_Task2.pth\n",
    "python dnn.py CS_5000 2 3 CS_Test_BiLSTMwoAttention_Task2_Prompt3 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/CS_Train_BiLSTMwoAttention_Task2_Prompt3/Best_CS_5000_Task2.pth\n",
    "python dnn.py CS_5000 2 4 CS_Test_BiLSTMwoAttention_Task2_Prompt4 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/CS_Train_BiLSTMwoAttention_Task2_Prompt4/Best_CS_5000_Task2.pth\n",
    "\n",
    "python dnn.py CS_5000 3 1 CS_Test_BiLSTMwoAttention_Task3_Prompt1 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/CS_Train_BiLSTMwoAttention_Task3_Prompt1/Best_CS_5000_Task3.pth\n",
    "python dnn.py CS_5000 3 2 CS_Test_BiLSTMwoAttention_Task3_Prompt2 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/CS_Train_BiLSTMwoAttention_Task3_Prompt2/Best_CS_5000_Task3.pth\n",
    "python dnn.py CS_5000 3 3 CS_Test_BiLSTMwoAttention_Task3_Prompt3 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/CS_Train_BiLSTMwoAttention_Task3_Prompt3/Best_CS_5000_Task3.pth\n",
    "python dnn.py CS_5000 3 4 CS_Test_BiLSTMwoAttention_Task3_Prompt4 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/CS_Train_BiLSTMwoAttention_Task3_Prompt4/Best_CS_5000_Task3.pth\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ae196c",
   "metadata": {},
   "source": [
    "# PHX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67bf353",
   "metadata": {},
   "source": [
    "Free up disk space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e371bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "shopt -s extglob\n",
    "rm -rf embeddings/!(README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97b30c8",
   "metadata": {},
   "source": [
    "## Representation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1551bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python features.py PHX 1 1 --gpt 0 --number 5000\n",
    "python features.py PHX 2 1 --gpt 0 --number 5000\n",
    "\n",
    "python features.py PHX 1 1 --gpt 1 --number 5000\n",
    "python features.py PHX 1 2 --gpt 1 --number 5000\n",
    "python features.py PHX 1 3 --gpt 1 --number 5000\n",
    "python features.py PHX 1 4 --gpt 1 --number 5000\n",
    "\n",
    "python features.py PHX 2 1 --gpt 1 --number 5000\n",
    "python features.py PHX 2 2 --gpt 1 --number 5000\n",
    "python features.py PHX 2 3 --gpt 1 --number 5000\n",
    "python features.py PHX 2 4 --gpt 1 --number 5000\n",
    "\n",
    "python features.py PHX 3 1 --gpt 1 --number 5000\n",
    "python features.py PHX 3 2 --gpt 1 --number 5000\n",
    "python features.py PHX 3 3 --gpt 1 --number 5000\n",
    "python features.py PHX 3 4 --gpt 1 --number 5000\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1ca9a6",
   "metadata": {},
   "source": [
    "## CheckGPT Arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74fdd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py PHX_5000 1 1 PHX_Train_CheckGPTArch_Task1_Prompt1 --batchsize 256\n",
    "python dnn.py PHX_5000 1 2 PHX_Train_CheckGPTArch_Task1_Prompt2 --batchsize 256\n",
    "python dnn.py PHX_5000 1 3 PHX_Train_CheckGPTArch_Task1_Prompt3 --batchsize 256\n",
    "python dnn.py PHX_5000 1 4 PHX_Train_CheckGPTArch_Task1_Prompt4 --batchsize 256\n",
    "\n",
    "python dnn.py PHX_5000 2 1 PHX_Train_CheckGPTArch_Task2_Prompt1 --batchsize 256\n",
    "python dnn.py PHX_5000 2 2 PHX_Train_CheckGPTArch_Task2_Prompt2 --batchsize 256\n",
    "python dnn.py PHX_5000 2 3 PHX_Train_CheckGPTArch_Task2_Prompt3 --batchsize 256\n",
    "python dnn.py PHX_5000 2 4 PHX_Train_CheckGPTArch_Task2_Prompt4 --batchsize 256\n",
    "\n",
    "python dnn.py PHX_5000 3 1 PHX_Train_CheckGPTArch_Task3_Prompt1 --batchsize 256\n",
    "python dnn.py PHX_5000 3 2 PHX_Train_CheckGPTArch_Task3_Prompt2 --batchsize 256\n",
    "python dnn.py PHX_5000 3 3 PHX_Train_CheckGPTArch_Task3_Prompt3 --batchsize 256\n",
    "python dnn.py PHX_5000 3 4 PHX_Train_CheckGPTArch_Task3_Prompt4 --batchsize 256\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d460b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py PHX_5000 1 1 PHX_Test_CheckGPTArch_Task1_Prompt1 --pretrain 1 --test 1 --saved-model ./exp/PHX_Train_CheckGPTArch_Task1_Prompt1/Best_PHX_5000_Task1.pth\n",
    "python dnn.py PHX_5000 1 2 PHX_Test_CheckGPTArch_Task1_Prompt2 --pretrain 1 --test 1 --saved-model ./exp/PHX_Train_CheckGPTArch_Task1_Prompt2/Best_PHX_5000_Task1.pth\n",
    "python dnn.py PHX_5000 1 3 PHX_Test_CheckGPTArch_Task1_Prompt3 --pretrain 1 --test 1 --saved-model ./exp/PHX_Train_CheckGPTArch_Task1_Prompt3/Best_PHX_5000_Task1.pth\n",
    "python dnn.py PHX_5000 1 4 PHX_Test_CheckGPTArch_Task1_Prompt4 --pretrain 1 --test 1 --saved-model ./exp/PHX_Train_CheckGPTArch_Task1_Prompt4/Best_PHX_5000_Task1.pth\n",
    "\n",
    "python dnn.py PHX_5000 2 1 PHX_Test_CheckGPTArch_Task2_Prompt1 --pretrain 1 --test 1 --saved-model ./exp/PHX_Train_CheckGPTArch_Task2_Prompt1/Best_PHX_5000_Task2.pth\n",
    "python dnn.py PHX_5000 2 2 PHX_Test_CheckGPTArch_Task2_Prompt2 --pretrain 1 --test 1 --saved-model ./exp/PHX_Train_CheckGPTArch_Task2_Prompt2/Best_PHX_5000_Task2.pth\n",
    "python dnn.py PHX_5000 2 3 PHX_Test_CheckGPTArch_Task2_Prompt3 --pretrain 1 --test 1 --saved-model ./exp/PHX_Train_CheckGPTArch_Task2_Prompt3/Best_PHX_5000_Task2.pth\n",
    "python dnn.py PHX_5000 2 4 PHX_Test_CheckGPTArch_Task2_Prompt4 --pretrain 1 --test 1 --saved-model ./exp/PHX_Train_CheckGPTArch_Task2_Prompt4/Best_PHX_5000_Task2.pth\n",
    "\n",
    "python dnn.py PHX_5000 3 1 PHX_Test_CheckGPTArch_Task3_Prompt1 --pretrain 1 --test 1 --saved-model ./exp/PHX_Train_CheckGPTArch_Task3_Prompt1/Best_PHX_5000_Task3.pth\n",
    "python dnn.py PHX_5000 3 2 PHX_Test_CheckGPTArch_Task3_Prompt2 --pretrain 1 --test 1 --saved-model ./exp/PHX_Train_CheckGPTArch_Task3_Prompt2/Best_PHX_5000_Task3.pth\n",
    "python dnn.py PHX_5000 3 3 PHX_Test_CheckGPTArch_Task3_Prompt3 --pretrain 1 --test 1 --saved-model ./exp/PHX_Train_CheckGPTArch_Task3_Prompt3/Best_PHX_5000_Task3.pth\n",
    "python dnn.py PHX_5000 3 4 PHX_Test_CheckGPTArch_Task3_Prompt4 --pretrain 1 --test 1 --saved-model ./exp/PHX_Train_CheckGPTArch_Task3_Prompt4/Best_PHX_5000_Task3.pth\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7bfa0c",
   "metadata": {},
   "source": [
    "## RCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862bdd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py PHX_5000 1 1 PHX_Train_RCH_Task1_Prompt1 --batchsize 256 --modelid 2\n",
    "python dnn.py PHX_5000 1 2 PHX_Train_RCH_Task1_Prompt2 --batchsize 256 --modelid 2\n",
    "python dnn.py PHX_5000 1 3 PHX_Train_RCH_Task1_Prompt3 --batchsize 256 --modelid 2\n",
    "python dnn.py PHX_5000 1 4 PHX_Train_RCH_Task1_Prompt4 --batchsize 256 --modelid 2\n",
    "\n",
    "python dnn.py PHX_5000 2 1 PHX_Train_RCH_Task2_Prompt1 --batchsize 256 --modelid 2\n",
    "python dnn.py PHX_5000 2 2 PHX_Train_RCH_Task2_Prompt2 --batchsize 256 --modelid 2\n",
    "python dnn.py PHX_5000 2 3 PHX_Train_RCH_Task2_Prompt3 --batchsize 256 --modelid 2\n",
    "python dnn.py PHX_5000 2 4 PHX_Train_RCH_Task2_Prompt4 --batchsize 256 --modelid 2\n",
    "\n",
    "python dnn.py PHX_5000 3 1 PHX_Train_RCH_Task3_Prompt1 --batchsize 256 --modelid 2\n",
    "python dnn.py PHX_5000 3 2 PHX_Train_RCH_Task3_Prompt2 --batchsize 256 --modelid 2\n",
    "python dnn.py PHX_5000 3 3 PHX_Train_RCH_Task3_Prompt3 --batchsize 256 --modelid 2\n",
    "python dnn.py PHX_5000 3 4 PHX_Train_RCH_Task3_Prompt4 --batchsize 256 --modelid 2\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbf3ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py PHX_5000 1 1 PHX_Test_RCH_Task1_Prompt1 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/PHX_Train_RCH_Task1_Prompt1/Best_PHX_5000_Task1.pth\n",
    "python dnn.py PHX_5000 1 2 PHX_Test_RCH_Task1_Prompt2 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/PHX_Train_RCH_Task1_Prompt2/Best_PHX_5000_Task1.pth\n",
    "python dnn.py PHX_5000 1 3 PHX_Test_RCH_Task1_Prompt3 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/PHX_Train_RCH_Task1_Prompt3/Best_PHX_5000_Task1.pth\n",
    "python dnn.py PHX_5000 1 4 PHX_Test_RCH_Task1_Prompt4 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/PHX_Train_RCH_Task1_Prompt4/Best_PHX_5000_Task1.pth\n",
    "\n",
    "python dnn.py PHX_5000 2 1 PHX_Test_RCH_Task2_Prompt1 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/PHX_Train_RCH_Task2_Prompt1/Best_PHX_5000_Task2.pth\n",
    "python dnn.py PHX_5000 2 2 PHX_Test_RCH_Task2_Prompt2 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/PHX_Train_RCH_Task2_Prompt2/Best_PHX_5000_Task2.pth\n",
    "python dnn.py PHX_5000 2 3 PHX_Test_RCH_Task2_Prompt3 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/PHX_Train_RCH_Task2_Prompt3/Best_PHX_5000_Task2.pth\n",
    "python dnn.py PHX_5000 2 4 PHX_Test_RCH_Task2_Prompt4 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/PHX_Train_RCH_Task2_Prompt4/Best_PHX_5000_Task2.pth\n",
    "\n",
    "python dnn.py PHX_5000 3 1 PHX_Test_RCH_Task3_Prompt1 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/PHX_Train_RCH_Task3_Prompt1/Best_PHX_5000_Task3.pth\n",
    "python dnn.py PHX_5000 3 2 PHX_Test_RCH_Task3_Prompt2 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/PHX_Train_RCH_Task3_Prompt2/Best_PHX_5000_Task3.pth\n",
    "python dnn.py PHX_5000 3 3 PHX_Test_RCH_Task3_Prompt3 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/PHX_Train_RCH_Task3_Prompt3/Best_PHX_5000_Task3.pth\n",
    "python dnn.py PHX_5000 3 4 PHX_Test_RCH_Task3_Prompt4 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/PHX_Train_RCH_Task3_Prompt4/Best_PHX_5000_Task3.pth\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81cdb12",
   "metadata": {},
   "source": [
    "## MLP-Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62420fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py PHX_5000 1 1 PHX_Train_MLP_Task1_Prompt1 --batchsize 256 --modelid 3\n",
    "python dnn.py PHX_5000 1 2 PHX_Train_MLP_Task1_Prompt2 --batchsize 256 --modelid 3\n",
    "python dnn.py PHX_5000 1 3 PHX_Train_MLP_Task1_Prompt3 --batchsize 256 --modelid 3\n",
    "python dnn.py PHX_5000 1 4 PHX_Train_MLP_Task1_Prompt4 --batchsize 256 --modelid 3\n",
    "\n",
    "python dnn.py PHX_5000 2 1 PHX_Train_MLP_Task2_Prompt1 --batchsize 256 --modelid 3\n",
    "python dnn.py PHX_5000 2 2 PHX_Train_MLP_Task2_Prompt2 --batchsize 256 --modelid 3\n",
    "python dnn.py PHX_5000 2 3 PHX_Train_MLP_Task2_Prompt3 --batchsize 256 --modelid 3\n",
    "python dnn.py PHX_5000 2 4 PHX_Train_MLP_Task2_Prompt4 --batchsize 256 --modelid 3\n",
    "\n",
    "python dnn.py PHX_5000 3 1 PHX_Train_MLP_Task3_Prompt1 --batchsize 256 --modelid 3\n",
    "python dnn.py PHX_5000 3 2 PHX_Train_MLP_Task3_Prompt2 --batchsize 256 --modelid 3\n",
    "python dnn.py PHX_5000 3 3 PHX_Train_MLP_Task3_Prompt3 --batchsize 256 --modelid 3\n",
    "python dnn.py PHX_5000 3 4 PHX_Train_MLP_Task3_Prompt4 --batchsize 256 --modelid 3\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bb6c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py PHX_5000 1 1 PHX_Test_MLP_Task1_Prompt1 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/PHX_Train_MLP_Task1_Prompt1/Best_PHX_5000_Task1.pth\n",
    "python dnn.py PHX_5000 1 2 PHX_Test_MLP_Task1_Prompt2 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/PHX_Train_MLP_Task1_Prompt2/Best_PHX_5000_Task1.pth\n",
    "python dnn.py PHX_5000 1 3 PHX_Test_MLP_Task1_Prompt3 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/PHX_Train_MLP_Task1_Prompt3/Best_PHX_5000_Task1.pth\n",
    "python dnn.py PHX_5000 1 4 PHX_Test_MLP_Task1_Prompt4 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/PHX_Train_MLP_Task1_Prompt4/Best_PHX_5000_Task1.pth\n",
    "\n",
    "python dnn.py PHX_5000 2 1 PHX_Test_MLP_Task2_Prompt1 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/PHX_Train_MLP_Task2_Prompt1/Best_PHX_5000_Task2.pth\n",
    "python dnn.py PHX_5000 2 2 PHX_Test_MLP_Task2_Prompt2 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/PHX_Train_MLP_Task2_Prompt2/Best_PHX_5000_Task2.pth\n",
    "python dnn.py PHX_5000 2 3 PHX_Test_MLP_Task2_Prompt3 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/PHX_Train_MLP_Task2_Prompt3/Best_PHX_5000_Task2.pth\n",
    "python dnn.py PHX_5000 2 4 PHX_Test_MLP_Task2_Prompt4 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/PHX_Train_MLP_Task2_Prompt4/Best_PHX_5000_Task2.pth\n",
    "\n",
    "python dnn.py PHX_5000 3 1 PHX_Test_MLP_Task3_Prompt1 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/PHX_Train_MLP_Task3_Prompt1/Best_PHX_5000_Task3.pth\n",
    "python dnn.py PHX_5000 3 2 PHX_Test_MLP_Task3_Prompt2 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/PHX_Train_MLP_Task3_Prompt2/Best_PHX_5000_Task3.pth\n",
    "python dnn.py PHX_5000 3 3 PHX_Test_MLP_Task3_Prompt3 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/PHX_Train_MLP_Task3_Prompt3/Best_PHX_5000_Task3.pth\n",
    "python dnn.py PHX_5000 3 4 PHX_Test_MLP_Task3_Prompt4 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/PHX_Train_MLP_Task3_Prompt4/Best_PHX_5000_Task3.pth\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1ffd6f",
   "metadata": {},
   "source": [
    "## AlexNet-like CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cab35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py PHX_5000 1 1 PHX_Train_CNN_Task1_Prompt1 --batchsize 256 --modelid 4\n",
    "python dnn.py PHX_5000 1 2 PHX_Train_CNN_Task1_Prompt2 --batchsize 256 --modelid 4\n",
    "python dnn.py PHX_5000 1 3 PHX_Train_CNN_Task1_Prompt3 --batchsize 256 --modelid 4\n",
    "python dnn.py PHX_5000 1 4 PHX_Train_CNN_Task1_Prompt4 --batchsize 256 --modelid 4\n",
    "\n",
    "python dnn.py PHX_5000 2 1 PHX_Train_CNN_Task2_Prompt1 --batchsize 256 --modelid 4\n",
    "python dnn.py PHX_5000 2 2 PHX_Train_CNN_Task2_Prompt2 --batchsize 256 --modelid 4\n",
    "python dnn.py PHX_5000 2 3 PHX_Train_CNN_Task2_Prompt3 --batchsize 256 --modelid 4\n",
    "python dnn.py PHX_5000 2 4 PHX_Train_CNN_Task2_Prompt4 --batchsize 256 --modelid 4\n",
    "\n",
    "python dnn.py PHX_5000 3 1 PHX_Train_CNN_Task3_Prompt1 --batchsize 256 --modelid 4\n",
    "python dnn.py PHX_5000 3 2 PHX_Train_CNN_Task3_Prompt2 --batchsize 256 --modelid 4\n",
    "python dnn.py PHX_5000 3 3 PHX_Train_CNN_Task3_Prompt3 --batchsize 256 --modelid 4\n",
    "python dnn.py PHX_5000 3 4 PHX_Train_CNN_Task3_Prompt4 --batchsize 256 --modelid 4\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d963167",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py PHX_5000 1 1 PHX_Test_CNN_Task1_Prompt1 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/PHX_Train_CNN_Task1_Prompt1/Best_PHX_5000_Task1.pth\n",
    "python dnn.py PHX_5000 1 2 PHX_Test_CNN_Task1_Prompt2 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/PHX_Train_CNN_Task1_Prompt2/Best_PHX_5000_Task1.pth\n",
    "python dnn.py PHX_5000 1 3 PHX_Test_CNN_Task1_Prompt3 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/PHX_Train_CNN_Task1_Prompt3/Best_PHX_5000_Task1.pth\n",
    "python dnn.py PHX_5000 1 4 PHX_Test_CNN_Task1_Prompt4 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/PHX_Train_CNN_Task1_Prompt4/Best_PHX_5000_Task1.pth\n",
    "\n",
    "python dnn.py PHX_5000 2 1 PHX_Test_CNN_Task2_Prompt1 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/PHX_Train_CNN_Task2_Prompt1/Best_PHX_5000_Task2.pth\n",
    "python dnn.py PHX_5000 2 2 PHX_Test_CNN_Task2_Prompt2 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/PHX_Train_CNN_Task2_Prompt2/Best_PHX_5000_Task2.pth\n",
    "python dnn.py PHX_5000 2 3 PHX_Test_CNN_Task2_Prompt3 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/PHX_Train_CNN_Task2_Prompt3/Best_PHX_5000_Task2.pth\n",
    "python dnn.py PHX_5000 2 4 PHX_Test_CNN_Task2_Prompt4 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/PHX_Train_CNN_Task2_Prompt4/Best_PHX_5000_Task2.pth\n",
    "\n",
    "python dnn.py PHX_5000 3 1 PHX_Test_CNN_Task3_Prompt1 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/PHX_Train_CNN_Task3_Prompt1/Best_PHX_5000_Task3.pth\n",
    "python dnn.py PHX_5000 3 2 PHX_Test_CNN_Task3_Prompt2 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/PHX_Train_CNN_Task3_Prompt2/Best_PHX_5000_Task3.pth\n",
    "python dnn.py PHX_5000 3 3 PHX_Test_CNN_Task3_Prompt3 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/PHX_Train_CNN_Task3_Prompt3/Best_PHX_5000_Task3.pth\n",
    "python dnn.py PHX_5000 3 4 PHX_Test_CNN_Task3_Prompt4 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/PHX_Train_CNN_Task3_Prompt4/Best_PHX_5000_Task3.pth\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd98ae4a",
   "metadata": {},
   "source": [
    "## BiLSTM w/o attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014999f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py PHX_5000 1 1 PHX_Train_BiLSTMwoAttention_Task1_Prompt1 --batchsize 256 --modelid 1\n",
    "python dnn.py PHX_5000 1 2 PHX_Train_BiLSTMwoAttention_Task1_Prompt2 --batchsize 256 --modelid 1\n",
    "python dnn.py PHX_5000 1 3 PHX_Train_BiLSTMwoAttention_Task1_Prompt3 --batchsize 256 --modelid 1\n",
    "python dnn.py PHX_5000 1 4 PHX_Train_BiLSTMwoAttention_Task1_Prompt4 --batchsize 256 --modelid 1\n",
    "\n",
    "python dnn.py PHX_5000 2 1 PHX_Train_BiLSTMwoAttention_Task2_Prompt1 --batchsize 256 --modelid 1\n",
    "python dnn.py PHX_5000 2 2 PHX_Train_BiLSTMwoAttention_Task2_Prompt2 --batchsize 256 --modelid 1\n",
    "python dnn.py PHX_5000 2 3 PHX_Train_BiLSTMwoAttention_Task2_Prompt3 --batchsize 256 --modelid 1\n",
    "python dnn.py PHX_5000 2 4 PHX_Train_BiLSTMwoAttention_Task2_Prompt4 --batchsize 256 --modelid 1\n",
    "\n",
    "python dnn.py PHX_5000 3 1 PHX_Train_BiLSTMwoAttention_Task3_Prompt1 --batchsize 256 --modelid 1\n",
    "python dnn.py PHX_5000 3 2 PHX_Train_BiLSTMwoAttention_Task3_Prompt2 --batchsize 256 --modelid 1\n",
    "python dnn.py PHX_5000 3 3 PHX_Train_BiLSTMwoAttention_Task3_Prompt3 --batchsize 256 --modelid 1\n",
    "python dnn.py PHX_5000 3 4 PHX_Train_BiLSTMwoAttention_Task3_Prompt4 --batchsize 256 --modelid 1\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff2ff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py PHX_5000 1 1 PHX_Test_BiLSTMwoAttention_Task1_Prompt1 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/PHX_Train_BiLSTMwoAttention_Task1_Prompt1/Best_PHX_5000_Task1.pth\n",
    "python dnn.py PHX_5000 1 2 PHX_Test_BiLSTMwoAttention_Task1_Prompt2 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/PHX_Train_BiLSTMwoAttention_Task1_Prompt2/Best_PHX_5000_Task1.pth\n",
    "python dnn.py PHX_5000 1 3 PHX_Test_BiLSTMwoAttention_Task1_Prompt3 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/PHX_Train_BiLSTMwoAttention_Task1_Prompt3/Best_PHX_5000_Task1.pth\n",
    "python dnn.py PHX_5000 1 4 PHX_Test_BiLSTMwoAttention_Task1_Prompt4 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/PHX_Train_BiLSTMwoAttention_Task1_Prompt4/Best_PHX_5000_Task1.pth\n",
    "\n",
    "python dnn.py PHX_5000 2 1 PHX_Test_BiLSTMwoAttention_Task2_Prompt1 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/PHX_Train_BiLSTMwoAttention_Task2_Prompt1/Best_PHX_5000_Task2.pth\n",
    "python dnn.py PHX_5000 2 2 PHX_Test_BiLSTMwoAttention_Task2_Prompt2 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/PHX_Train_BiLSTMwoAttention_Task2_Prompt2/Best_PHX_5000_Task2.pth\n",
    "python dnn.py PHX_5000 2 3 PHX_Test_BiLSTMwoAttention_Task2_Prompt3 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/PHX_Train_BiLSTMwoAttention_Task2_Prompt3/Best_PHX_5000_Task2.pth\n",
    "python dnn.py PHX_5000 2 4 PHX_Test_BiLSTMwoAttention_Task2_Prompt4 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/PHX_Train_BiLSTMwoAttention_Task2_Prompt4/Best_PHX_5000_Task2.pth\n",
    "\n",
    "python dnn.py PHX_5000 3 1 PHX_Test_BiLSTMwoAttention_Task3_Prompt1 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/PHX_Train_BiLSTMwoAttention_Task3_Prompt1/Best_PHX_5000_Task3.pth\n",
    "python dnn.py PHX_5000 3 2 PHX_Test_BiLSTMwoAttention_Task3_Prompt2 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/PHX_Train_BiLSTMwoAttention_Task3_Prompt2/Best_PHX_5000_Task3.pth\n",
    "python dnn.py PHX_5000 3 3 PHX_Test_BiLSTMwoAttention_Task3_Prompt3 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/PHX_Train_BiLSTMwoAttention_Task3_Prompt3/Best_PHX_5000_Task3.pth\n",
    "python dnn.py PHX_5000 3 4 PHX_Test_BiLSTMwoAttention_Task3_Prompt4 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/PHX_Train_BiLSTMwoAttention_Task3_Prompt4/Best_PHX_5000_Task3.pth\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e6ba71",
   "metadata": {},
   "source": [
    "# HSS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18999f29",
   "metadata": {},
   "source": [
    "Free up disk space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bcac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "shopt -s extglob\n",
    "rm -rf embeddings/!(README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44031def",
   "metadata": {},
   "source": [
    "## Representation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e124f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python features.py HSS 1 1 --gpt 0 --number 5000\n",
    "python features.py HSS 2 1 --gpt 0 --number 5000\n",
    "\n",
    "python features.py HSS 1 1 --gpt 1 --number 5000\n",
    "python features.py HSS 1 2 --gpt 1 --number 5000\n",
    "python features.py HSS 1 3 --gpt 1 --number 5000\n",
    "python features.py HSS 1 4 --gpt 1 --number 5000\n",
    "\n",
    "python features.py HSS 2 1 --gpt 1 --number 5000\n",
    "python features.py HSS 2 2 --gpt 1 --number 5000\n",
    "python features.py HSS 2 3 --gpt 1 --number 5000\n",
    "python features.py HSS 2 4 --gpt 1 --number 5000\n",
    "\n",
    "python features.py HSS 3 1 --gpt 1 --number 5000\n",
    "python features.py HSS 3 2 --gpt 1 --number 5000\n",
    "python features.py HSS 3 3 --gpt 1 --number 5000\n",
    "python features.py HSS 3 4 --gpt 1 --number 5000\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949d30bd",
   "metadata": {},
   "source": [
    "## CheckGPT Arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31079076",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py HSS_5000 1 1 HSS_Train_CheckGPTArch_Task1_Prompt1 --batchsize 256\n",
    "python dnn.py HSS_5000 1 2 HSS_Train_CheckGPTArch_Task1_Prompt2 --batchsize 256\n",
    "python dnn.py HSS_5000 1 3 HSS_Train_CheckGPTArch_Task1_Prompt3 --batchsize 256\n",
    "python dnn.py HSS_5000 1 4 HSS_Train_CheckGPTArch_Task1_Prompt4 --batchsize 256\n",
    "\n",
    "python dnn.py HSS_5000 2 1 HSS_Train_CheckGPTArch_Task2_Prompt1 --batchsize 256\n",
    "python dnn.py HSS_5000 2 2 HSS_Train_CheckGPTArch_Task2_Prompt2 --batchsize 256\n",
    "python dnn.py HSS_5000 2 3 HSS_Train_CheckGPTArch_Task2_Prompt3 --batchsize 256\n",
    "python dnn.py HSS_5000 2 4 HSS_Train_CheckGPTArch_Task2_Prompt4 --batchsize 256\n",
    "\n",
    "python dnn.py HSS_5000 3 1 HSS_Train_CheckGPTArch_Task3_Prompt1 --batchsize 256\n",
    "python dnn.py HSS_5000 3 2 HSS_Train_CheckGPTArch_Task3_Prompt2 --batchsize 256\n",
    "python dnn.py HSS_5000 3 3 HSS_Train_CheckGPTArch_Task3_Prompt3 --batchsize 256\n",
    "python dnn.py HSS_5000 3 4 HSS_Train_CheckGPTArch_Task3_Prompt4 --batchsize 256\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d86916",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py HSS_5000 1 1 HSS_Test_CheckGPTArch_Task1_Prompt1 --pretrain 1 --test 1 --saved-model ./exp/HSS_Train_CheckGPTArch_Task1_Prompt1/Best_HSS_5000_Task1.pth\n",
    "python dnn.py HSS_5000 1 2 HSS_Test_CheckGPTArch_Task1_Prompt2 --pretrain 1 --test 1 --saved-model ./exp/HSS_Train_CheckGPTArch_Task1_Prompt2/Best_HSS_5000_Task1.pth\n",
    "python dnn.py HSS_5000 1 3 HSS_Test_CheckGPTArch_Task1_Prompt3 --pretrain 1 --test 1 --saved-model ./exp/HSS_Train_CheckGPTArch_Task1_Prompt3/Best_HSS_5000_Task1.pth\n",
    "python dnn.py HSS_5000 1 4 HSS_Test_CheckGPTArch_Task1_Prompt4 --pretrain 1 --test 1 --saved-model ./exp/HSS_Train_CheckGPTArch_Task1_Prompt4/Best_HSS_5000_Task1.pth\n",
    "\n",
    "python dnn.py HSS_5000 2 1 HSS_Test_CheckGPTArch_Task2_Prompt1 --pretrain 1 --test 1 --saved-model ./exp/HSS_Train_CheckGPTArch_Task2_Prompt1/Best_HSS_5000_Task2.pth\n",
    "python dnn.py HSS_5000 2 2 HSS_Test_CheckGPTArch_Task2_Prompt2 --pretrain 1 --test 1 --saved-model ./exp/HSS_Train_CheckGPTArch_Task2_Prompt2/Best_HSS_5000_Task2.pth\n",
    "python dnn.py HSS_5000 2 3 HSS_Test_CheckGPTArch_Task2_Prompt3 --pretrain 1 --test 1 --saved-model ./exp/HSS_Train_CheckGPTArch_Task2_Prompt3/Best_HSS_5000_Task2.pth\n",
    "python dnn.py HSS_5000 2 4 HSS_Test_CheckGPTArch_Task2_Prompt4 --pretrain 1 --test 1 --saved-model ./exp/HSS_Train_CheckGPTArch_Task2_Prompt4/Best_HSS_5000_Task2.pth\n",
    "\n",
    "python dnn.py HSS_5000 3 1 HSS_Test_CheckGPTArch_Task3_Prompt1 --pretrain 1 --test 1 --saved-model ./exp/HSS_Train_CheckGPTArch_Task3_Prompt1/Best_HSS_5000_Task3.pth\n",
    "python dnn.py HSS_5000 3 2 HSS_Test_CheckGPTArch_Task3_Prompt2 --pretrain 1 --test 1 --saved-model ./exp/HSS_Train_CheckGPTArch_Task3_Prompt2/Best_HSS_5000_Task3.pth\n",
    "python dnn.py HSS_5000 3 3 HSS_Test_CheckGPTArch_Task3_Prompt3 --pretrain 1 --test 1 --saved-model ./exp/HSS_Train_CheckGPTArch_Task3_Prompt3/Best_HSS_5000_Task3.pth\n",
    "python dnn.py HSS_5000 3 4 HSS_Test_CheckGPTArch_Task3_Prompt4 --pretrain 1 --test 1 --saved-model ./exp/HSS_Train_CheckGPTArch_Task3_Prompt4/Best_HSS_5000_Task3.pth\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c083ca3",
   "metadata": {},
   "source": [
    "## RCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fc8989",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py HSS_5000 1 1 HSS_Train_RCH_Task1_Prompt1 --batchsize 256 --modelid 2\n",
    "python dnn.py HSS_5000 1 2 HSS_Train_RCH_Task1_Prompt2 --batchsize 256 --modelid 2\n",
    "python dnn.py HSS_5000 1 3 HSS_Train_RCH_Task1_Prompt3 --batchsize 256 --modelid 2\n",
    "python dnn.py HSS_5000 1 4 HSS_Train_RCH_Task1_Prompt4 --batchsize 256 --modelid 2\n",
    "\n",
    "python dnn.py HSS_5000 2 1 HSS_Train_RCH_Task2_Prompt1 --batchsize 256 --modelid 2\n",
    "python dnn.py HSS_5000 2 2 HSS_Train_RCH_Task2_Prompt2 --batchsize 256 --modelid 2\n",
    "python dnn.py HSS_5000 2 3 HSS_Train_RCH_Task2_Prompt3 --batchsize 256 --modelid 2\n",
    "python dnn.py HSS_5000 2 4 HSS_Train_RCH_Task2_Prompt4 --batchsize 256 --modelid 2\n",
    "\n",
    "python dnn.py HSS_5000 3 1 HSS_Train_RCH_Task3_Prompt1 --batchsize 256 --modelid 2\n",
    "python dnn.py HSS_5000 3 2 HSS_Train_RCH_Task3_Prompt2 --batchsize 256 --modelid 2\n",
    "python dnn.py HSS_5000 3 3 HSS_Train_RCH_Task3_Prompt3 --batchsize 256 --modelid 2\n",
    "python dnn.py HSS_5000 3 4 HSS_Train_RCH_Task3_Prompt4 --batchsize 256 --modelid 2\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561ae8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py HSS_5000 1 1 HSS_Test_RCH_Task1_Prompt1 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/HSS_Train_RCH_Task1_Prompt1/Best_HSS_5000_Task1.pth\n",
    "python dnn.py HSS_5000 1 2 HSS_Test_RCH_Task1_Prompt2 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/HSS_Train_RCH_Task1_Prompt2/Best_HSS_5000_Task1.pth\n",
    "python dnn.py HSS_5000 1 3 HSS_Test_RCH_Task1_Prompt3 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/HSS_Train_RCH_Task1_Prompt3/Best_HSS_5000_Task1.pth\n",
    "python dnn.py HSS_5000 1 4 HSS_Test_RCH_Task1_Prompt4 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/HSS_Train_RCH_Task1_Prompt4/Best_HSS_5000_Task1.pth\n",
    "\n",
    "python dnn.py HSS_5000 2 1 HSS_Test_RCH_Task2_Prompt1 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/HSS_Train_RCH_Task2_Prompt1/Best_HSS_5000_Task2.pth\n",
    "python dnn.py HSS_5000 2 2 HSS_Test_RCH_Task2_Prompt2 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/HSS_Train_RCH_Task2_Prompt2/Best_HSS_5000_Task2.pth\n",
    "python dnn.py HSS_5000 2 3 HSS_Test_RCH_Task2_Prompt3 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/HSS_Train_RCH_Task2_Prompt3/Best_HSS_5000_Task2.pth\n",
    "python dnn.py HSS_5000 2 4 HSS_Test_RCH_Task2_Prompt4 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/HSS_Train_RCH_Task2_Prompt4/Best_HSS_5000_Task2.pth\n",
    "\n",
    "python dnn.py HSS_5000 3 1 HSS_Test_RCH_Task3_Prompt1 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/HSS_Train_RCH_Task3_Prompt1/Best_HSS_5000_Task3.pth\n",
    "python dnn.py HSS_5000 3 2 HSS_Test_RCH_Task3_Prompt2 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/HSS_Train_RCH_Task3_Prompt2/Best_HSS_5000_Task3.pth\n",
    "python dnn.py HSS_5000 3 3 HSS_Test_RCH_Task3_Prompt3 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/HSS_Train_RCH_Task3_Prompt3/Best_HSS_5000_Task3.pth\n",
    "python dnn.py HSS_5000 3 4 HSS_Test_RCH_Task3_Prompt4 --pretrain 1 --test 1 --modelid 2 --saved-model ./exp/HSS_Train_RCH_Task3_Prompt4/Best_HSS_5000_Task3.pth\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aff3613",
   "metadata": {},
   "source": [
    "## MLP-Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33acfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py HSS_5000 1 1 HSS_Train_MLP_Task1_Prompt1 --batchsize 256 --modelid 3\n",
    "python dnn.py HSS_5000 1 2 HSS_Train_MLP_Task1_Prompt2 --batchsize 256 --modelid 3\n",
    "python dnn.py HSS_5000 1 3 HSS_Train_MLP_Task1_Prompt3 --batchsize 256 --modelid 3\n",
    "python dnn.py HSS_5000 1 4 HSS_Train_MLP_Task1_Prompt4 --batchsize 256 --modelid 3\n",
    "\n",
    "python dnn.py HSS_5000 2 1 HSS_Train_MLP_Task2_Prompt1 --batchsize 256 --modelid 3\n",
    "python dnn.py HSS_5000 2 2 HSS_Train_MLP_Task2_Prompt2 --batchsize 256 --modelid 3\n",
    "python dnn.py HSS_5000 2 3 HSS_Train_MLP_Task2_Prompt3 --batchsize 256 --modelid 3\n",
    "python dnn.py HSS_5000 2 4 HSS_Train_MLP_Task2_Prompt4 --batchsize 256 --modelid 3\n",
    "\n",
    "python dnn.py HSS_5000 3 1 HSS_Train_MLP_Task3_Prompt1 --batchsize 256 --modelid 3\n",
    "python dnn.py HSS_5000 3 2 HSS_Train_MLP_Task3_Prompt2 --batchsize 256 --modelid 3\n",
    "python dnn.py HSS_5000 3 3 HSS_Train_MLP_Task3_Prompt3 --batchsize 256 --modelid 3\n",
    "python dnn.py HSS_5000 3 4 HSS_Train_MLP_Task3_Prompt4 --batchsize 256 --modelid 3\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4137c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py HSS_5000 1 1 HSS_Test_MLP_Task1_Prompt1 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/HSS_Train_MLP_Task1_Prompt1/Best_HSS_5000_Task1.pth\n",
    "python dnn.py HSS_5000 1 2 HSS_Test_MLP_Task1_Prompt2 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/HSS_Train_MLP_Task1_Prompt2/Best_HSS_5000_Task1.pth\n",
    "python dnn.py HSS_5000 1 3 HSS_Test_MLP_Task1_Prompt3 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/HSS_Train_MLP_Task1_Prompt3/Best_HSS_5000_Task1.pth\n",
    "python dnn.py HSS_5000 1 4 HSS_Test_MLP_Task1_Prompt4 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/HSS_Train_MLP_Task1_Prompt4/Best_HSS_5000_Task1.pth\n",
    "\n",
    "python dnn.py HSS_5000 2 1 HSS_Test_MLP_Task2_Prompt1 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/HSS_Train_MLP_Task2_Prompt1/Best_HSS_5000_Task2.pth\n",
    "python dnn.py HSS_5000 2 2 HSS_Test_MLP_Task2_Prompt2 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/HSS_Train_MLP_Task2_Prompt2/Best_HSS_5000_Task2.pth\n",
    "python dnn.py HSS_5000 2 3 HSS_Test_MLP_Task2_Prompt3 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/HSS_Train_MLP_Task2_Prompt3/Best_HSS_5000_Task2.pth\n",
    "python dnn.py HSS_5000 2 4 HSS_Test_MLP_Task2_Prompt4 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/HSS_Train_MLP_Task2_Prompt4/Best_HSS_5000_Task2.pth\n",
    "\n",
    "python dnn.py HSS_5000 3 1 HSS_Test_MLP_Task3_Prompt1 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/HSS_Train_MLP_Task3_Prompt1/Best_HSS_5000_Task3.pth\n",
    "python dnn.py HSS_5000 3 2 HSS_Test_MLP_Task3_Prompt2 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/HSS_Train_MLP_Task3_Prompt2/Best_HSS_5000_Task3.pth\n",
    "python dnn.py HSS_5000 3 3 HSS_Test_MLP_Task3_Prompt3 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/HSS_Train_MLP_Task3_Prompt3/Best_HSS_5000_Task3.pth\n",
    "python dnn.py HSS_5000 3 4 HSS_Test_MLP_Task3_Prompt4 --pretrain 1 --test 1 --modelid 3 --saved-model ./exp/HSS_Train_MLP_Task3_Prompt4/Best_HSS_5000_Task3.pth\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a013a0d",
   "metadata": {},
   "source": [
    "## AlexNet-like CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0dfcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py HSS_5000 1 1 HSS_Train_CNN_Task1_Prompt1 --batchsize 256 --modelid 4\n",
    "python dnn.py HSS_5000 1 2 HSS_Train_CNN_Task1_Prompt2 --batchsize 256 --modelid 4\n",
    "python dnn.py HSS_5000 1 3 HSS_Train_CNN_Task1_Prompt3 --batchsize 256 --modelid 4\n",
    "python dnn.py HSS_5000 1 4 HSS_Train_CNN_Task1_Prompt4 --batchsize 256 --modelid 4\n",
    "\n",
    "python dnn.py HSS_5000 2 1 HSS_Train_CNN_Task2_Prompt1 --batchsize 256 --modelid 4\n",
    "python dnn.py HSS_5000 2 2 HSS_Train_CNN_Task2_Prompt2 --batchsize 256 --modelid 4\n",
    "python dnn.py HSS_5000 2 3 HSS_Train_CNN_Task2_Prompt3 --batchsize 256 --modelid 4\n",
    "python dnn.py HSS_5000 2 4 HSS_Train_CNN_Task2_Prompt4 --batchsize 256 --modelid 4\n",
    "\n",
    "python dnn.py HSS_5000 3 1 HSS_Train_CNN_Task3_Prompt1 --batchsize 256 --modelid 4\n",
    "python dnn.py HSS_5000 3 2 HSS_Train_CNN_Task3_Prompt2 --batchsize 256 --modelid 4\n",
    "python dnn.py HSS_5000 3 3 HSS_Train_CNN_Task3_Prompt3 --batchsize 256 --modelid 4\n",
    "python dnn.py HSS_5000 3 4 HSS_Train_CNN_Task3_Prompt4 --batchsize 256 --modelid 4\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7114c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py HSS_5000 1 1 HSS_Test_CNN_Task1_Prompt1 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/HSS_Train_CNN_Task1_Prompt1/Best_HSS_5000_Task1.pth\n",
    "python dnn.py HSS_5000 1 2 HSS_Test_CNN_Task1_Prompt2 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/HSS_Train_CNN_Task1_Prompt2/Best_HSS_5000_Task1.pth\n",
    "python dnn.py HSS_5000 1 3 HSS_Test_CNN_Task1_Prompt3 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/HSS_Train_CNN_Task1_Prompt3/Best_HSS_5000_Task1.pth\n",
    "python dnn.py HSS_5000 1 4 HSS_Test_CNN_Task1_Prompt4 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/HSS_Train_CNN_Task1_Prompt4/Best_HSS_5000_Task1.pth\n",
    "\n",
    "python dnn.py HSS_5000 2 1 HSS_Test_CNN_Task2_Prompt1 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/HSS_Train_CNN_Task2_Prompt1/Best_HSS_5000_Task2.pth\n",
    "python dnn.py HSS_5000 2 2 HSS_Test_CNN_Task2_Prompt2 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/HSS_Train_CNN_Task2_Prompt2/Best_HSS_5000_Task2.pth\n",
    "python dnn.py HSS_5000 2 3 HSS_Test_CNN_Task2_Prompt3 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/HSS_Train_CNN_Task2_Prompt3/Best_HSS_5000_Task2.pth\n",
    "python dnn.py HSS_5000 2 4 HSS_Test_CNN_Task2_Prompt4 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/HSS_Train_CNN_Task2_Prompt4/Best_HSS_5000_Task2.pth\n",
    "\n",
    "python dnn.py HSS_5000 3 1 HSS_Test_CNN_Task3_Prompt1 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/HSS_Train_CNN_Task3_Prompt1/Best_HSS_5000_Task3.pth\n",
    "python dnn.py HSS_5000 3 2 HSS_Test_CNN_Task3_Prompt2 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/HSS_Train_CNN_Task3_Prompt2/Best_HSS_5000_Task3.pth\n",
    "python dnn.py HSS_5000 3 3 HSS_Test_CNN_Task3_Prompt3 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/HSS_Train_CNN_Task3_Prompt3/Best_HSS_5000_Task3.pth\n",
    "python dnn.py HSS_5000 3 4 HSS_Test_CNN_Task3_Prompt4 --pretrain 1 --test 1 --modelid 4 --saved-model ./exp/HSS_Train_CNN_Task3_Prompt4/Best_HSS_5000_Task3.pth\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320472a7",
   "metadata": {},
   "source": [
    "## BiLSTM w/o attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db394b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py HSS_5000 1 1 HSS_Train_BiLSTMwoAttention_Task1_Prompt1 --batchsize 256 --modelid 1\n",
    "python dnn.py HSS_5000 1 2 HSS_Train_BiLSTMwoAttention_Task1_Prompt2 --batchsize 256 --modelid 1\n",
    "python dnn.py HSS_5000 1 3 HSS_Train_BiLSTMwoAttention_Task1_Prompt3 --batchsize 256 --modelid 1\n",
    "python dnn.py HSS_5000 1 4 HSS_Train_BiLSTMwoAttention_Task1_Prompt4 --batchsize 256 --modelid 1\n",
    "\n",
    "python dnn.py HSS_5000 2 1 HSS_Train_BiLSTMwoAttention_Task2_Prompt1 --batchsize 256 --modelid 1\n",
    "python dnn.py HSS_5000 2 2 HSS_Train_BiLSTMwoAttention_Task2_Prompt2 --batchsize 256 --modelid 1\n",
    "python dnn.py HSS_5000 2 3 HSS_Train_BiLSTMwoAttention_Task2_Prompt3 --batchsize 256 --modelid 1\n",
    "python dnn.py HSS_5000 2 4 HSS_Train_BiLSTMwoAttention_Task2_Prompt4 --batchsize 256 --modelid 1\n",
    "\n",
    "python dnn.py HSS_5000 3 1 HSS_Train_BiLSTMwoAttention_Task3_Prompt1 --batchsize 256 --modelid 1\n",
    "python dnn.py HSS_5000 3 2 HSS_Train_BiLSTMwoAttention_Task3_Prompt2 --batchsize 256 --modelid 1\n",
    "python dnn.py HSS_5000 3 3 HSS_Train_BiLSTMwoAttention_Task3_Prompt3 --batchsize 256 --modelid 1\n",
    "python dnn.py HSS_5000 3 4 HSS_Train_BiLSTMwoAttention_Task3_Prompt4 --batchsize 256 --modelid 1\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c58c3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "date\n",
    "\n",
    "python dnn.py HSS_5000 1 1 HSS_Test_BiLSTMwoAttention_Task1_Prompt1 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/HSS_Train_BiLSTMwoAttention_Task1_Prompt1/Best_HSS_5000_Task1.pth\n",
    "python dnn.py HSS_5000 1 2 HSS_Test_BiLSTMwoAttention_Task1_Prompt2 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/HSS_Train_BiLSTMwoAttention_Task1_Prompt2/Best_HSS_5000_Task1.pth\n",
    "python dnn.py HSS_5000 1 3 HSS_Test_BiLSTMwoAttention_Task1_Prompt3 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/HSS_Train_BiLSTMwoAttention_Task1_Prompt3/Best_HSS_5000_Task1.pth\n",
    "python dnn.py HSS_5000 1 4 HSS_Test_BiLSTMwoAttention_Task1_Prompt4 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/HSS_Train_BiLSTMwoAttention_Task1_Prompt4/Best_HSS_5000_Task1.pth\n",
    "\n",
    "python dnn.py HSS_5000 2 1 HSS_Test_BiLSTMwoAttention_Task2_Prompt1 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/HSS_Train_BiLSTMwoAttention_Task2_Prompt1/Best_HSS_5000_Task2.pth\n",
    "python dnn.py HSS_5000 2 2 HSS_Test_BiLSTMwoAttention_Task2_Prompt2 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/HSS_Train_BiLSTMwoAttention_Task2_Prompt2/Best_HSS_5000_Task2.pth\n",
    "python dnn.py HSS_5000 2 3 HSS_Test_BiLSTMwoAttention_Task2_Prompt3 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/HSS_Train_BiLSTMwoAttention_Task2_Prompt3/Best_HSS_5000_Task2.pth\n",
    "python dnn.py HSS_5000 2 4 HSS_Test_BiLSTMwoAttention_Task2_Prompt4 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/HSS_Train_BiLSTMwoAttention_Task2_Prompt4/Best_HSS_5000_Task2.pth\n",
    "\n",
    "python dnn.py HSS_5000 3 1 HSS_Test_BiLSTMwoAttention_Task3_Prompt1 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/HSS_Train_BiLSTMwoAttention_Task3_Prompt1/Best_HSS_5000_Task3.pth\n",
    "python dnn.py HSS_5000 3 2 HSS_Test_BiLSTMwoAttention_Task3_Prompt2 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/HSS_Train_BiLSTMwoAttention_Task3_Prompt2/Best_HSS_5000_Task3.pth\n",
    "python dnn.py HSS_5000 3 3 HSS_Test_BiLSTMwoAttention_Task3_Prompt3 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/HSS_Train_BiLSTMwoAttention_Task3_Prompt3/Best_HSS_5000_Task3.pth\n",
    "python dnn.py HSS_5000 3 4 HSS_Test_BiLSTMwoAttention_Task3_Prompt4 --pretrain 1 --test 1 --modelid 1 --saved-model ./exp/HSS_Train_BiLSTMwoAttention_Task3_Prompt4/Best_HSS_5000_Task3.pth\n",
    "\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd67b365",
   "metadata": {},
   "source": [
    "Free up disk space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1ea42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "shopt -s extglob\n",
    "rm -rf embeddings/!(README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
