{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcb1b728",
   "metadata": {},
   "source": [
    "Contains pipeline to reproduce section 5.3 unified classifiers.\n",
    "\n",
    "Due to high disk space usage, this pipeline will not train the model from scratch.\n",
    "\n",
    "Instead, it will use the pre-trained models that is provided from the publication's artifact.\n",
    "\n",
    "Reproduction's working directory will always be `~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/` unless specified otherwise through comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf492f6",
   "metadata": {},
   "source": [
    "# Pre-experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20759c52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T05:13:25.442587Z",
     "iopub.status.busy": "2025-12-31T05:13:25.442019Z",
     "iopub.status.idle": "2025-12-31T05:13:25.654382Z",
     "shell.execute_reply": "2025-12-31T05:13:25.652638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 31 05:13:25 UTC 2025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13G\t.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "date\n",
    "du -hs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78455b98",
   "metadata": {},
   "source": [
    "Adjustment to `dnn.py` for working with a specified sample size on unified classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de99a839",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T05:13:25.660487Z",
     "iopub.status.busy": "2025-12-31T05:13:25.659936Z",
     "iopub.status.idle": "2025-12-31T05:13:25.680076Z",
     "shell.execute_reply": "2025-12-31T05:13:25.678256Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "\n",
    "sed -i 's/domain_list = \\[\"CS\", \"PHY\", \"LIT\"\\]/domain_list = [\"CS\", \"PHX\", \"HSS\"]/' dnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "251aaa3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T05:13:25.685558Z",
     "iopub.status.busy": "2025-12-31T05:13:25.685010Z",
     "iopub.status.idle": "2025-12-31T05:13:25.704272Z",
     "shell.execute_reply": "2025-12-31T05:13:25.702383Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "\n",
    "sed -i \"/parser.add_argument('expid', type=str)/a \\\n",
    "parser.add_argument('--sample', type=int, default=50000)\" dnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "573b06c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T05:13:25.710275Z",
     "iopub.status.busy": "2025-12-31T05:13:25.709724Z",
     "iopub.status.idle": "2025-12-31T05:13:25.728926Z",
     "shell.execute_reply": "2025-12-31T05:13:25.726979Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "\n",
    "sed -i \"/ID = args.expid/a \\\n",
    "SAMPLE_SIZE = args.sample\" dnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "598419d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T05:13:25.734489Z",
     "iopub.status.busy": "2025-12-31T05:13:25.733920Z",
     "iopub.status.idle": "2025-12-31T05:13:25.753886Z",
     "shell.execute_reply": "2025-12-31T05:13:25.751901Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "\n",
    "sed -i 's/domain_list = \\[\"CS\", \"PHX\", \"HSS\"\\]/domain_list = [\"CS\", \"PHX\", \"HSS\"]\\n\\t\\tif args.domain == f\"ALL_{SAMPLE_SIZE}\":\\n\\t\\t\\tdomain_list = \\[f\"CS_{SAMPLE_SIZE}\", f\"PHX_{SAMPLE_SIZE}\", f\"HSS_{SAMPLE_SIZE}\"\\]/' dnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f978408",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T05:13:25.759521Z",
     "iopub.status.busy": "2025-12-31T05:13:25.758941Z",
     "iopub.status.idle": "2025-12-31T05:13:25.778374Z",
     "shell.execute_reply": "2025-12-31T05:13:25.776405Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "\n",
    "sed -i $'s/\\t/    /g' dnn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55e8abc",
   "metadata": {},
   "source": [
    "# Cross-prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81586e21",
   "metadata": {},
   "source": [
    "## CS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b45d53ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T05:13:25.783982Z",
     "iopub.status.busy": "2025-12-31T05:13:25.783397Z",
     "iopub.status.idle": "2025-12-31T05:13:26.192569Z",
     "shell.execute_reply": "2025-12-31T05:13:26.190158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 31 05:13:25 UTC 2025\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13G\t.\r\n"
     ]
    }
   ],
   "source": [
    "!date\n",
    "!du -hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c48674fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T05:13:26.199667Z",
     "iopub.status.busy": "2025-12-31T05:13:26.198718Z",
     "iopub.status.idle": "2025-12-31T05:45:05.883799Z",
     "shell.execute_reply": "2025-12-31T05:45:05.880777Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: ground, data num: 50000\n",
      "ground. Complete 0/5000 (0.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 0.401348352432251s. Overlong: 0\n",
      "ground. Complete 200/5000 (4.0%). Time used: 5.379410982131958s."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Overlong: 0\n",
      "ground. Complete 400/5000 (8.0%). Time used: 9.841259479522705s. Overlong: 0\n",
      "ground. Co"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mplete 600/5000 (12.0%). Time used: 14.28398084640503s. Overlong: 0\n",
      "ground. Complete 800/5000 (16.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "). Time used: 19.06571912765503s. Overlong: 0\n",
      "ground. Complete 1000/5000 (20.0%). Time used: 23.9023"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5185623169s. Overlong: 1\n",
      "ground. Complete 1200/5000 (24.0%). Time used: 29.10672354698181s. Overlong"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 3\n",
      "ground. Complete 1400/5000 (28.0%). Time used: 33.920939922332764s. Overlong: 3\n",
      "ground. Complete"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1600/5000 (32.0%). Time used: 38.86302351951599s. Overlong: 3\n",
      "ground. Complete 1800/5000 (36.0%). T"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ime used: 43.79897618293762s. Overlong: 3\n",
      "ground. Complete 2000/5000 (40.0%). Time used: 48.72053122"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520447s. Overlong: 4\n",
      "ground. Complete 2200/5000 (44.0%). Time used: 53.28650784492493s. Overlong: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground. Complete 2400/5000 (48.0%). Time used: 57.6888689994812s. Overlong: 4\n",
      "ground. Complete 2600/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 (52.0%). Time used: 62.17270016670227s. Overlong: 4\n",
      "ground. Complete 2800/5000 (56.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 67.17269825935364s. Overlong: 5\n",
      "ground. Complete 3000/5000 (60.0%). Time used: 72.35100173950195"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s. Overlong: 6\n",
      "ground. Complete 3200/5000 (64.0%). Time used: 77.32999920845032s. Overlong: 6\n",
      "ground"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Complete 3400/5000 (68.0%). Time used: 82.4670557975769s. Overlong: 6\n",
      "ground. Complete 3600/5000 ("
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.0%). Time used: 87.55008578300476s. Overlong: 6\n",
      "ground. Complete 3800/5000 (76.0%). Time used: 91"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".95201826095581s. Overlong: 6\n",
      "ground. Complete 4000/5000 (80.0%). Time used: 96.39378714561462s. Ove"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rlong: 6\n",
      "ground. Complete 4200/5000 (84.0%). Time used: 101.15359115600586s. Overlong: 6\n",
      "ground. Com"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plete 4400/5000 (88.0%). Time used: 105.8044285774231s. Overlong: 6\n",
      "ground. Complete 4600/5000 (92.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 110.57369136810303s. Overlong: 6\n",
      "ground. Complete 4800/5000 (96.0%). Time used: 115.5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9131336212158s. Overlong: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: ground_task2, data num: 50000\n",
      "ground_task2. Complete 0/5000 (0."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 0.3969719409942627s. Overlong: 0\n",
      "ground_task2. Complete 200/5000 (4.0%). Time used: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.59612512588501s. Overlong: 0\n",
      "ground_task2. Complete 400/5000 (8.0%). Time used: 8.574008703231812s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Overlong: 0\n",
      "ground_task2. Complete 600/5000 (12.0%). Time used: 12.552413940429688s. Overlong: 0\n",
      "g"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round_task2. Complete 800/5000 (16.0%). Time used: 16.646891832351685s. Overlong: 0\n",
      "ground_task2. Co"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mplete 1000/5000 (20.0%). Time used: 20.696979999542236s. Overlong: 0\n",
      "ground_task2. Complete 1200/50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 (24.0%). Time used: 24.87743878364563s. Overlong: 0\n",
      "ground_task2. Complete 1400/5000 (28.0%). Tim"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e used: 28.939610242843628s. Overlong: 0\n",
      "ground_task2. Complete 1600/5000 (32.0%). Time used: 33.053"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552865982056s. Overlong: 0\n",
      "ground_task2. Complete 1800/5000 (36.0%). Time used: 37.17690658569336s. "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlong: 0\n",
      "ground_task2. Complete 2000/5000 (40.0%). Time used: 41.28815960884094s. Overlong: 0\n",
      "gro"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "und_task2. Complete 2200/5000 (44.0%). Time used: 45.25305700302124s. Overlong: 0\n",
      "ground_task2. Comp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lete 2400/5000 (48.0%). Time used: 49.18588399887085s. Overlong: 0\n",
      "ground_task2. Complete 2600/5000 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52.0%). Time used: 53.13707685470581s. Overlong: 0\n",
      "ground_task2. Complete 2800/5000 (56.0%). Time u"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sed: 57.264315605163574s. Overlong: 0\n",
      "ground_task2. Complete 3000/5000 (60.0%). Time used: 61.440057"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "039260864s. Overlong: 0\n",
      "ground_task2. Complete 3200/5000 (64.0%). Time used: 65.49863791465759s. Ove"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rlong: 0\n",
      "ground_task2. Complete 3400/5000 (68.0%). Time used: 69.62145686149597s. Overlong: 0\n",
      "ground"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_task2. Complete 3600/5000 (72.0%). Time used: 73.79894828796387s. Overlong: 0\n",
      "ground_task2. Complet"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 3800/5000 (76.0%). Time used: 77.7597005367279s. Overlong: 0\n",
      "ground_task2. Complete 4000/5000 (80."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 81.68884825706482s. Overlong: 0\n",
      "ground_task2. Complete 4200/5000 (84.0%). Time used:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 85.69826889038086s. Overlong: 0\n",
      "ground_task2. Complete 4400/5000 (88.0%). Time used: 89.66013240814"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209s. Overlong: 0\n",
      "ground_task2. Complete 4600/5000 (92.0%). Time used: 93.69986605644226s. Overlong:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0\n",
      "ground_task2. Complete 4800/5000 (96.0%). Time used: 97.81927967071533s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task1_prompt1, data num: 50000\n",
      "gpt_task1_prompt1. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.4089217185974121s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 200/5000 (4.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 5.33371639251709s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 400/5000 (8.0%). Time used"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 10.26017951965332s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 600/5000 (12.0%). Time used: 15.106311"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321258545s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 800/5000 (16.0%). Time used: 20.034863233566284s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Overlong: 0\n",
      "gpt_task1_prompt1. Complete 1000/5000 (20.0%). Time used: 24.79729175567627s. Overlong"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 0\n",
      "gpt_task1_prompt1. Complete 1200/5000 (24.0%). Time used: 29.579787969589233s. Overlong: 0\n",
      "gpt_t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ask1_prompt1. Complete 1400/5000 (28.0%). Time used: 34.45958876609802s. Overlong: 0\n",
      "gpt_task1_promp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1. Complete 1600/5000 (32.0%). Time used: 39.24574136734009s. Overlong: 0\n",
      "gpt_task1_prompt1. Comple"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "te 1800/5000 (36.0%). Time used: 44.131991147994995s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 2000/5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000 (40.0%). Time used: 49.317586183547974s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 2200/5000 (44.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 54.25378489494324s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 2400/5000 (48.0%). Time u"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sed: 59.21799612045288s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 2600/5000 (52.0%). Time used: 64.18"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432378768921s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 2800/5000 (56.0%). Time used: 69.132954359054"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 3000/5000 (60.0%). Time used: 74.14431166648865s. Overl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ong: 0\n",
      "gpt_task1_prompt1. Complete 3200/5000 (64.0%). Time used: 78.9554877281189s. Overlong: 0\n",
      "gpt_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task1_prompt1. Complete 3400/5000 (68.0%). Time used: 83.90589332580566s. Overlong: 0\n",
      "gpt_task1_prom"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt1. Complete 3600/5000 (72.0%). Time used: 88.83134126663208s. Overlong: 0\n",
      "gpt_task1_prompt1. Compl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ete 3800/5000 (76.0%). Time used: 93.77057099342346s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 4000/5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000 (80.0%). Time used: 98.81674408912659s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 4200/5000 (84.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "). Time used: 103.62033271789551s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 4400/5000 (88.0%). Time u"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sed: 108.47051811218262s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 4600/5000 (92.0%). Time used: 113."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41125297546387s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 4800/5000 (96.0%). Time used: 118.332879781"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72302s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task1_prompt2, data num: 50000\n",
      "gpt_task1_prompt2. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.41985034942626953s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 200/5000 (4."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 5.932226181030273s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 400/5000 (8.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 11.152758598327637s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 600/5000 (12.0%). Time used: 16.435"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880422592163s. Overlong: 1\n",
      "gpt_task1_prompt2. Complete 800/5000 (16.0%). Time used: 21.7675964832305"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9s. Overlong: 1\n",
      "gpt_task1_prompt2. Complete 1000/5000 (20.0%). Time used: 26.91913628578186s. Overlo"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng: 1\n",
      "gpt_task1_prompt2. Complete 1200/5000 (24.0%). Time used: 32.15497398376465s. Overlong: 1\n",
      "gpt_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task1_prompt2. Complete 1400/5000 (28.0%). Time used: 37.55443716049194s. Overlong: 1\n",
      "gpt_task1_prom"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt2. Complete 1600/5000 (32.0%). Time used: 42.63511776924133s. Overlong: 1\n",
      "gpt_task1_prompt2. Compl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ete 1800/5000 (36.0%). Time used: 47.88334918022156s. Overlong: 2\n",
      "gpt_task1_prompt2. Complete 2000/5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000 (40.0%). Time used: 53.18862771987915s. Overlong: 2\n",
      "gpt_task1_prompt2. Complete 2200/5000 (44.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "). Time used: 58.56661796569824s. Overlong: 2\n",
      "gpt_task1_prompt2. Complete 2400/5000 (48.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 63.68417954444885s. Overlong: 2\n",
      "gpt_task1_prompt2. Complete 2600/5000 (52.0%). Time used: 68.825"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67071914673s. Overlong: 2\n",
      "gpt_task1_prompt2. Complete 2800/5000 (56.0%). Time used: 74.2674572467804"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s. Overlong: 2\n",
      "gpt_task1_prompt2. Complete 3000/5000 (60.0%). Time used: 79.4647912979126s. Overlong"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 2\n",
      "gpt_task1_prompt2. Complete 3200/5000 (64.0%). Time used: 84.66883516311646s. Overlong: 2\n",
      "gpt_ta"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk1_prompt2. Complete 3400/5000 (68.0%). Time used: 90.0899829864502s. Overlong: 3\n",
      "gpt_task1_prompt2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Complete 3600/5000 (72.0%). Time used: 95.36151361465454s. Overlong: 3\n",
      "gpt_task1_prompt2. Complete"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3800/5000 (76.0%). Time used: 100.63253951072693s. Overlong: 3\n",
      "gpt_task1_prompt2. Complete 4000/500"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (80.0%). Time used: 105.73413014411926s. Overlong: 4\n",
      "gpt_task1_prompt2. Complete 4200/5000 (84.0%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Time used: 111.05505013465881s. Overlong: 4\n",
      "gpt_task1_prompt2. Complete 4400/5000 (88.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 116.3715558052063s. Overlong: 4\n",
      "gpt_task1_prompt2. Complete 4600/5000 (92.0%). Time used: 121.68"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604779243469s. Overlong: 4\n",
      "gpt_task1_prompt2. Complete 4800/5000 (96.0%). Time used: 126.98797225952"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148s. Overlong: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task1_prompt3, data num: 50000\n",
      "gpt_task1_prompt3. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.4150390625s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 200/5000 (4.0%). Ti"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me used: 6.024489402770996s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 400/5000 (8.0%). Time used: 11."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446949481964111s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 600/5000 (12.0%). Time used: 16.7566955089"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5691s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 800/5000 (16.0%). Time used: 22.084761142730713s. Ove"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rlong: 0\n",
      "gpt_task1_prompt3. Complete 1000/5000 (20.0%). Time used: 27.4204204082489s. Overlong: 0\n",
      "gp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_task1_prompt3. Complete 1200/5000 (24.0%). Time used: 32.84346318244934s. Overlong: 0\n",
      "gpt_task1_pr"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ompt3. Complete 1400/5000 (28.0%). Time used: 38.218576192855835s. Overlong: 0\n",
      "gpt_task1_prompt3. Co"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mplete 1600/5000 (32.0%). Time used: 43.588905572891235s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 18"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00/5000 (36.0%). Time used: 48.92342948913574s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 2000/5000 (4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0%). Time used: 54.24862194061279s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 2200/5000 (44.0%). Tim"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e used: 59.6052360534668s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 2400/5000 (48.0%). Time used: 64."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9255211353302s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 2600/5000 (52.0%). Time used: 70.21527171134"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "949s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 2800/5000 (56.0%). Time used: 75.65178179740906s. Over"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long: 0\n",
      "gpt_task1_prompt3. Complete 3000/5000 (60.0%). Time used: 81.15139293670654s. Overlong: 0\n",
      "gp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_task1_prompt3. Complete 3200/5000 (64.0%). Time used: 86.60849738121033s. Overlong: 0\n",
      "gpt_task1_pr"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ompt3. Complete 3400/5000 (68.0%). Time used: 91.98265528678894s. Overlong: 0\n",
      "gpt_task1_prompt3. Com"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plete 3600/5000 (72.0%). Time used: 97.381756067276s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 3800/5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000 (76.0%). Time used: 102.66084456443787s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 4000/5000 (80.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 107.99965500831604s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 4200/5000 (84.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 113.44134140014648s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 4400/5000 (88.0%). Time used: 118"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".85096001625061s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 4600/5000 (92.0%). Time used: 124.18767118"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45398s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 4800/5000 (96.0%). Time used: 129.69978308677673s. O"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task1_prompt4, data num: 50000\n",
      "gpt_task1_prompt4. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.4162936210632324s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 200/5000 (4.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 5.478255987167358s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 400/5000 (8.0%). Time use"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 10.815146684646606s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 600/5000 (12.0%). Time used: 16.0473"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4492301941s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 800/5000 (16.0%). Time used: 21.1369686126709s."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Overlong: 0\n",
      "gpt_task1_prompt4. Complete 1000/5000 (20.0%). Time used: 26.356138467788696s. Overlong"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 0\n",
      "gpt_task1_prompt4. Complete 1200/5000 (24.0%). Time used: 31.498889207839966s. Overlong: 0\n",
      "gpt_t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ask1_prompt4. Complete 1400/5000 (28.0%). Time used: 36.65447187423706s. Overlong: 0\n",
      "gpt_task1_promp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t4. Complete 1600/5000 (32.0%). Time used: 41.95493507385254s. Overlong: 0\n",
      "gpt_task1_prompt4. Comple"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "te 1800/5000 (36.0%). Time used: 46.95185351371765s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 2000/50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 (40.0%). Time used: 52.00361371040344s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 2200/5000 (44.0%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Time used: 57.29631972312927s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 2400/5000 (48.0%). Time use"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 62.67329263687134s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 2600/5000 (52.0%). Time used: 67.9120"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9387779236s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 2800/5000 (56.0%). Time used: 73.09160923957825"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 3000/5000 (60.0%). Time used: 78.2052264213562s. Overlong"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 0\n",
      "gpt_task1_prompt4. Complete 3200/5000 (64.0%). Time used: 83.16001057624817s. Overlong: 0\n",
      "gpt_ta"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk1_prompt4. Complete 3400/5000 (68.0%). Time used: 88.21906471252441s. Overlong: 0\n",
      "gpt_task1_prompt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Complete 3600/5000 (72.0%). Time used: 93.24882102012634s. Overlong: 0\n",
      "gpt_task1_prompt4. Complet"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 3800/5000 (76.0%). Time used: 98.51690649986267s. Overlong: 1\n",
      "gpt_task1_prompt4. Complete 4000/500"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (80.0%). Time used: 103.81663823127747s. Overlong: 1\n",
      "gpt_task1_prompt4. Complete 4200/5000 (84.0%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Time used: 109.09234762191772s. Overlong: 1\n",
      "gpt_task1_prompt4. Complete 4400/5000 (88.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 114.05291962623596s. Overlong: 1\n",
      "gpt_task1_prompt4. Complete 4600/5000 (92.0%). Time used: 119.1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234221458435s. Overlong: 1\n",
      "gpt_task1_prompt4. Complete 4800/5000 (96.0%). Time used: 124.14394354820"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251s. Overlong: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task2_prompt1, data num: 50000\n",
      "gpt_task2_prompt1. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.3928515911102295s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 200/5000 (4.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 4.9933600425720215s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 400/5000 (8.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 9.244739294052124s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 600/5000 (12.0%). Time used: 13.3996"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20294570923s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 800/5000 (16.0%). Time used: 17.85059785842895"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 1000/5000 (20.0%). Time used: 22.21388339996338s. Overlo"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng: 0\n",
      "gpt_task2_prompt1. Complete 1200/5000 (24.0%). Time used: 26.859475135803223s. Overlong: 0\n",
      "gpt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_task2_prompt1. Complete 1400/5000 (28.0%). Time used: 31.252821445465088s. Overlong: 0\n",
      "gpt_task2_pr"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ompt1. Complete 1600/5000 (32.0%). Time used: 35.79566717147827s. Overlong: 0\n",
      "gpt_task2_prompt1. Com"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plete 1800/5000 (36.0%). Time used: 40.30939269065857s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 2000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/5000 (40.0%). Time used: 44.76002740859985s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 2200/5000 (44."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 49.01521611213684s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 2400/5000 (48.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 53.1519193649292s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 2600/5000 (52.0%). Time used: 57.30"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "718445777893s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 2800/5000 (56.0%). Time used: 61.816584348678"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 3000/5000 (60.0%). Time used: 66.45843076705933s. Overl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ong: 0\n",
      "gpt_task2_prompt1. Complete 3200/5000 (64.0%). Time used: 70.88932752609253s. Overlong: 0\n",
      "gpt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_task2_prompt1. Complete 3400/5000 (68.0%). Time used: 75.52073645591736s. Overlong: 0\n",
      "gpt_task2_pro"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpt1. Complete 3600/5000 (72.0%). Time used: 80.11936616897583s. Overlong: 0\n",
      "gpt_task2_prompt1. Comp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lete 3800/5000 (76.0%). Time used: 84.266428232193s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 4000/50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 (80.0%). Time used: 88.44101428985596s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 4200/5000 (84.0%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Time used: 92.81074643135071s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 4400/5000 (88.0%). Time use"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 97.07277464866638s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 4600/5000 (92.0%). Time used: 101.436"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52653694153s. Overlong: 1\n",
      "gpt_task2_prompt1. Complete 4800/5000 (96.0%). Time used: 105.820310831069"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95s. Overlong: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task2_prompt2, data num: 50000\n",
      "gpt_task2_prompt2. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.4046051502227783s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 200/5000 (4.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 5.0434911251068115s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 400/5000 (8.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 9.299936294555664s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 600/5000 (12.0%). Time used: 13.4893"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53895187378s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 800/5000 (16.0%). Time used: 17.92728638648986"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 1000/5000 (20.0%). Time used: 22.317735195159912s. Overl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ong: 0\n",
      "gpt_task2_prompt2. Complete 1200/5000 (24.0%). Time used: 27.059558629989624s. Overlong: 0\n",
      "gp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_task2_prompt2. Complete 1400/5000 (28.0%). Time used: 31.48190927505493s. Overlong: 0\n",
      "gpt_task2_pr"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ompt2. Complete 1600/5000 (32.0%). Time used: 36.03085923194885s. Overlong: 0\n",
      "gpt_task2_prompt2. Com"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plete 1800/5000 (36.0%). Time used: 40.56608319282532s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 2000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/5000 (40.0%). Time used: 45.05104970932007s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 2200/5000 (44."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 49.333375453948975s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 2400/5000 (48.0%). Time"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " used: 53.47737455368042s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 2600/5000 (52.0%). Time used: 57."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68851709365845s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 2800/5000 (56.0%). Time used: 62.2642016410"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82764s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 3000/5000 (60.0%). Time used: 66.96860647201538s. Ov"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erlong: 0\n",
      "gpt_task2_prompt2. Complete 3200/5000 (64.0%). Time used: 71.46536326408386s. Overlong: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_task2_prompt2. Complete 3400/5000 (68.0%). Time used: 76.025639295578s. Overlong: 0\n",
      "gpt_task2_pr"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ompt2. Complete 3600/5000 (72.0%). Time used: 80.61898756027222s. Overlong: 0\n",
      "gpt_task2_prompt2. Com"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plete 3800/5000 (76.0%). Time used: 84.7852053642273s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 4000/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 (80.0%). Time used: 88.93300533294678s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 4200/5000 (84.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 93.33199548721313s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 4400/5000 (88.0%). Time u"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sed: 97.64912867546082s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 4600/5000 (92.0%). Time used: 102.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3566193580627s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 4800/5000 (96.0%). Time used: 106.5865442752"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8381s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task2_prompt3, data num: 50000\n",
      "gpt_task2_prompt3. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.3937723636627197s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 200/5000 (4.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 4.947321891784668s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 400/5000 (8.0%). Time use"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 9.125843048095703s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 600/5000 (12.0%). Time used: 13.26470"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6134796143s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 800/5000 (16.0%). Time used: 17.61235213279724s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Overlong: 0\n",
      "gpt_task2_prompt3. Complete 1000/5000 (20.0%). Time used: 21.966615676879883s. Overlon"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g: 0\n",
      "gpt_task2_prompt3. Complete 1200/5000 (24.0%). Time used: 26.527431964874268s. Overlong: 0\n",
      "gpt_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task2_prompt3. Complete 1400/5000 (28.0%). Time used: 30.847096920013428s. Overlong: 0\n",
      "gpt_task2_pro"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpt3. Complete 1600/5000 (32.0%). Time used: 35.28511095046997s. Overlong: 0\n",
      "gpt_task2_prompt3. Comp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lete 1800/5000 (36.0%). Time used: 39.675843954086304s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 2000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/5000 (40.0%). Time used: 44.0541934967041s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 2200/5000 (44.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 48.31320881843567s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 2400/5000 (48.0%). Time u"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sed: 52.42509746551514s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 2600/5000 (52.0%). Time used: 56.58"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273243904114s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 2800/5000 (56.0%). Time used: 61.097818374633"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 3000/5000 (60.0%). Time used: 65.6745834350586s. Overlo"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng: 0\n",
      "gpt_task2_prompt3. Complete 3200/5000 (64.0%). Time used: 70.0655152797699s. Overlong: 0\n",
      "gpt_t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ask2_prompt3. Complete 3400/5000 (68.0%). Time used: 74.54368543624878s. Overlong: 0\n",
      "gpt_task2_promp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3. Complete 3600/5000 (72.0%). Time used: 79.0771074295044s. Overlong: 0\n",
      "gpt_task2_prompt3. Complet"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 3800/5000 (76.0%). Time used: 83.16513204574585s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 4000/500"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (80.0%). Time used: 87.25711870193481s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 4200/5000 (84.0%)."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Time used: 91.5949637889862s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 4400/5000 (88.0%). Time used:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 95.82640290260315s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 4600/5000 (92.0%). Time used: 100.17100"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "477218628s. Overlong: 1\n",
      "gpt_task2_prompt3. Complete 4800/5000 (96.0%). Time used: 104.56145906448364"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s. Overlong: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task2_prompt4, data num: 50000\n",
      "gpt_task2_prompt4. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.4006192684173584s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 200/5000 (4.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 4.900307893753052s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 400/5000 (8.0%). Time use"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 9.151536703109741s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 600/5000 (12.0%). Time used: 13.32122"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7073669434s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 800/5000 (16.0%). Time used: 17.756274700164795"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 1000/5000 (20.0%). Time used: 22.103421449661255s. Overlo"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng: 0\n",
      "gpt_task2_prompt4. Complete 1200/5000 (24.0%). Time used: 26.70344591140747s. Overlong: 0\n",
      "gpt_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task2_prompt4. Complete 1400/5000 (28.0%). Time used: 31.06082820892334s. Overlong: 0\n",
      "gpt_task2_prom"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt4. Complete 1600/5000 (32.0%). Time used: 35.530667304992676s. Overlong: 0\n",
      "gpt_task2_prompt4. Comp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lete 1800/5000 (36.0%). Time used: 39.96936821937561s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 2000/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 (40.0%). Time used: 44.43041229248047s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 2200/5000 (44.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 48.735107421875s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 2400/5000 (48.0%). Time use"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 52.889055252075195s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 2600/5000 (52.0%). Time used: 57.056"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16903305054s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 2800/5000 (56.0%). Time used: 61.6313745975494"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 3000/5000 (60.0%). Time used: 66.21162271499634s. Overlo"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng: 0\n",
      "gpt_task2_prompt4. Complete 3200/5000 (64.0%). Time used: 70.61854100227356s. Overlong: 0\n",
      "gpt_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task2_prompt4. Complete 3400/5000 (68.0%). Time used: 75.14746737480164s. Overlong: 0\n",
      "gpt_task2_prom"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt4. Complete 3600/5000 (72.0%). Time used: 79.75010800361633s. Overlong: 0\n",
      "gpt_task2_prompt4. Compl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ete 3800/5000 (76.0%). Time used: 83.94466352462769s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 4000/5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000 (80.0%). Time used: 88.1827802658081s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 4200/5000 (84.0%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Time used: 92.50445604324341s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 4400/5000 (88.0%). Time use"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 96.75715136528015s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 4600/5000 (92.0%). Time used: 101.186"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75684928894s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 4800/5000 (96.0%). Time used: 105.714821100234"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task3_prompt1, data num: 50000\n",
      "gpt_task3_prompt1. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.40335869789123535s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 200/5000 (4."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 5.1363348960876465s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 400/5000 (8.0%). Time u"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sed: 9.643014430999756s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 600/5000 (12.0%). Time used: 14.084"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891080856323s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 800/5000 (16.0%). Time used: 18.7719166278839"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 1000/5000 (20.0%). Time used: 23.457144021987915s. Overl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ong: 0\n",
      "gpt_task3_prompt1. Complete 1200/5000 (24.0%). Time used: 28.450868129730225s. Overlong: 0\n",
      "gp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_task3_prompt1. Complete 1400/5000 (28.0%). Time used: 33.139238357543945s. Overlong: 0\n",
      "gpt_task3_p"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rompt1. Complete 1600/5000 (32.0%). Time used: 37.88016152381897s. Overlong: 0\n",
      "gpt_task3_prompt1. Co"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mplete 1800/5000 (36.0%). Time used: 42.6826388835907s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 2000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/5000 (40.0%). Time used: 47.429779052734375s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 2200/5000 (44"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 51.972992181777954s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 2400/5000 (48.0%). Tim"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e used: 56.38626742362976s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 2600/5000 (52.0%). Time used: 60"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".86666750907898s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 2800/5000 (56.0%). Time used: 65.675777435"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30273s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 3000/5000 (60.0%). Time used: 70.56745386123657s. Ov"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erlong: 0\n",
      "gpt_task3_prompt1. Complete 3200/5000 (64.0%). Time used: 75.26073884963989s. Overlong: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_task3_prompt1. Complete 3400/5000 (68.0%). Time used: 80.05796241760254s. Overlong: 0\n",
      "gpt_task3_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt1. Complete 3600/5000 (72.0%). Time used: 84.86559247970581s. Overlong: 0\n",
      "gpt_task3_prompt1. C"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omplete 3800/5000 (76.0%). Time used: 89.32893872261047s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 40"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00/5000 (80.0%). Time used: 93.76818704605103s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 4200/5000 (8"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0%). Time used: 98.38273453712463s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 4400/5000 (88.0%). Tim"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e used: 102.93765187263489s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 4600/5000 (92.0%). Time used: 1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07.54124784469604s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 4800/5000 (96.0%). Time used: 112.284487"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48588562s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task3_prompt2, data num: 50000\n",
      "gpt_task3_prompt2. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.407421350479126s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 200/5000 (4.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "). Time used: 5.952247619628906s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 400/5000 (8.0%). Time used"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 11.22570276260376s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 600/5000 (12.0%). Time used: 16.482147"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "693634033s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 800/5000 (16.0%). Time used: 21.936361074447632s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Overlong: 0\n",
      "gpt_task3_prompt2. Complete 1000/5000 (20.0%). Time used: 27.408865690231323s. Overlon"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g: 0\n",
      "gpt_task3_prompt2. Complete 1200/5000 (24.0%). Time used: 33.151565074920654s. Overlong: 0\n",
      "gpt_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task3_prompt2. Complete 1400/5000 (28.0%). Time used: 38.636595487594604s. Overlong: 0\n",
      "gpt_task3_pro"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpt2. Complete 1600/5000 (32.0%). Time used: 44.126118421554565s. Overlong: 0\n",
      "gpt_task3_prompt2. Com"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plete 1800/5000 (36.0%). Time used: 49.608927965164185s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 200"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (40.0%). Time used: 55.091384410858154s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 2200/5000 (4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0%). Time used: 60.45574641227722s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 2400/5000 (48.0%). Tim"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e used: 65.65660548210144s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 2600/5000 (52.0%). Time used: 70"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".9538369178772s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 2800/5000 (56.0%). Time used: 76.6221878528"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 3000/5000 (60.0%). Time used: 82.31854486465454s. Over"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long: 0\n",
      "gpt_task3_prompt2. Complete 3200/5000 (64.0%). Time used: 87.85148978233337s. Overlong: 0\n",
      "gp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_task3_prompt2. Complete 3400/5000 (68.0%). Time used: 93.4190948009491s. Overlong: 0\n",
      "gpt_task3_pro"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpt2. Complete 3600/5000 (72.0%). Time used: 98.95420002937317s. Overlong: 0\n",
      "gpt_task3_prompt2. Comp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lete 3800/5000 (76.0%). Time used: 104.21032738685608s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 4000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/5000 (80.0%). Time used: 109.54167914390564s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 4200/5000 (84"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 114.92031264305115s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 4400/5000 (88.0%). Tim"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e used: 120.24354481697083s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 4600/5000 (92.0%). Time used: 1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.64732909202576s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 4800/5000 (96.0%). Time used: 131.164987"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08724976s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task3_prompt3, data num: 50000\n",
      "gpt_task3_prompt3. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.48902153968811035s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete 200/5000 (4."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 5.456380844116211s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete 400/5000 (8.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 10.05815076828003s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete 600/5000 (12.0%). Time used: 14.6650"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71964263916s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete 800/5000 (16.0%). Time used: 19.4856960773468s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Overlong: 0\n",
      "gpt_task3_prompt3. Complete 1000/5000 (20.0%). Time used: 24.4419162273407s. Overlong:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0\n",
      "gpt_task3_prompt3. Complete 1200/5000 (24.0%). Time used: 29.906148433685303s. Overlong: 0\n",
      "gpt_ta"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk3_prompt3. Complete 1400/5000 (28.0%). Time used: 35.0100302696228s. Overlong: 0\n",
      "gpt_task3_prompt3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Complete 1600/5000 (32.0%). Time used: 39.94713377952576s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1800/5000 (36.0%). Time used: 44.84496593475342s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete 2000/5000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (40.0%). Time used: 49.711628675460815s. Overlong: 1\n",
      "gpt_task3_prompt3. Complete 2200/5000 (44.0%)."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Time used: 54.420066118240356s. Overlong: 1\n",
      "gpt_task3_prompt3. Complete 2400/5000 (48.0%). Time use"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 59.009721755981445s. Overlong: 1\n",
      "gpt_task3_prompt3. Complete 2600/5000 (52.0%). Time used: 63.633"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63575935364s. Overlong: 1\n",
      "gpt_task3_prompt3. Complete 2800/5000 (56.0%). Time used: 68.6818060874939"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s. Overlong: 1\n",
      "gpt_task3_prompt3. Complete 3000/5000 (60.0%). Time used: 73.83168864250183s. Overlon"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g: 1\n",
      "gpt_task3_prompt3. Complete 3200/5000 (64.0%). Time used: 78.70316815376282s. Overlong: 1\n",
      "gpt_t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ask3_prompt3. Complete 3400/5000 (68.0%). Time used: 83.75334453582764s. Overlong: 1\n",
      "gpt_task3_promp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3. Complete 3600/5000 (72.0%). Time used: 88.652174949646s. Overlong: 1\n",
      "gpt_task3_prompt3. Complete"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3800/5000 (76.0%). Time used: 93.18497323989868s. Overlong: 1\n",
      "gpt_task3_prompt3. Complete 4000/5000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (80.0%). Time used: 97.74405479431152s. Overlong: 1\n",
      "gpt_task3_prompt3. Complete 4200/5000 (84.0%). "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used: 102.48299837112427s. Overlong: 1\n",
      "gpt_task3_prompt3. Complete 4400/5000 (88.0%). Time used"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 107.10099673271179s. Overlong: 1\n",
      "gpt_task3_prompt3. Complete 4600/5000 (92.0%). Time used: 111.834"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97285842896s. Overlong: 1\n",
      "gpt_task3_prompt3. Complete 4800/5000 (96.0%). Time used: 116.705697774887"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08s. Overlong: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task3_prompt4, data num: 50000\n",
      "gpt_task3_prompt4. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.4358510971069336s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 200/5000 (4.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 5.766088008880615s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 400/5000 (8.0%). Time use"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 10.65652322769165s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 600/5000 (12.0%). Time used: 15.44833"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9700698853s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 800/5000 (16.0%). Time used: 20.56865620613098s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Overlong: 0\n",
      "gpt_task3_prompt4. Complete 1000/5000 (20.0%). Time used: 25.767109394073486s. Overlon"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g: 0\n",
      "gpt_task3_prompt4. Complete 1200/5000 (24.0%). Time used: 31.196977853775024s. Overlong: 0\n",
      "gpt_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task3_prompt4. Complete 1400/5000 (28.0%). Time used: 36.44343876838684s. Overlong: 0\n",
      "gpt_task3_prom"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt4. Complete 1600/5000 (32.0%). Time used: 41.631272315979004s. Overlong: 0\n",
      "gpt_task3_prompt4. Comp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lete 1800/5000 (36.0%). Time used: 46.76429009437561s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 2000/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 (40.0%). Time used: 51.99819779396057s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 2200/5000 (44.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 57.086204051971436s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 2400/5000 (48.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 61.85759353637695s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 2600/5000 (52.0%). Time used: 66.7"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3735737800598s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 2800/5000 (56.0%). Time used: 72.10964560508"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "728s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 3000/5000 (60.0%). Time used: 77.59263038635254s. Over"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long: 0\n",
      "gpt_task3_prompt4. Complete 3200/5000 (64.0%). Time used: 82.78881883621216s. Overlong: 0\n",
      "gp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_task3_prompt4. Complete 3400/5000 (68.0%). Time used: 88.07905554771423s. Overlong: 0\n",
      "gpt_task3_pr"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ompt4. Complete 3600/5000 (72.0%). Time used: 93.24244213104248s. Overlong: 0\n",
      "gpt_task3_prompt4. Com"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plete 3800/5000 (76.0%). Time used: 98.100745677948s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 4000/5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000 (80.0%). Time used: 102.93492698669434s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 4200/5000 (84.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 108.07545256614685s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 4400/5000 (88.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 113.03415966033936s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 4600/5000 (92.0%). Time used: 118"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".13767790794373s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 4800/5000 (96.0%). Time used: 123.28552269"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "935608s. Overlong: 0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "\n",
    "python features.py CS 1 1 --gpt 0 --number 5000\n",
    "python features.py CS 2 1 --gpt 0 --number 5000\n",
    "\n",
    "python features.py CS 1 1 --gpt 1 --number 5000\n",
    "python features.py CS 1 2 --gpt 1 --number 5000\n",
    "python features.py CS 1 3 --gpt 1 --number 5000\n",
    "python features.py CS 1 4 --gpt 1 --number 5000\n",
    "\n",
    "python features.py CS 2 1 --gpt 1 --number 5000\n",
    "python features.py CS 2 2 --gpt 1 --number 5000\n",
    "python features.py CS 2 3 --gpt 1 --number 5000\n",
    "python features.py CS 2 4 --gpt 1 --number 5000\n",
    "\n",
    "python features.py CS 3 1 --gpt 1 --number 5000\n",
    "python features.py CS 3 2 --gpt 1 --number 5000\n",
    "python features.py CS 3 3 --gpt 1 --number 5000\n",
    "python features.py CS 3 4 --gpt 1 --number 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bc229b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T05:45:05.892478Z",
     "iopub.status.busy": "2025-12-31T05:45:05.891836Z",
     "iopub.status.idle": "2025-12-31T05:45:06.294357Z",
     "shell.execute_reply": "2025-12-31T05:45:06.291678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 31 05:45:05 UTC 2025\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150G\t.\r\n"
     ]
    }
   ],
   "source": [
    "!date\n",
    "!du -hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3b80e39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T05:45:06.301463Z",
     "iopub.status.busy": "2025-12-31T05:45:06.300820Z",
     "iopub.status.idle": "2025-12-31T05:46:25.478074Z",
     "shell.execute_reply": "2025-12-31T05:46:25.475889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(domain='CS_5000', task=1, prompt=0, expid='Unified_CS_Test_Task1_PromptALL', sample=50000,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " early=100.0, save=1, modelid=0, pretrain=1, saved_model='../CheckGPT_presaved_files/saved_experimen"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts/basic/CS_Task1_Prompt_1234/Best_CS_Task1.pth', dataamount=100, trans=0, splitr=0.8, ablr=1.0, lr="
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002, nepochs=100, test=1, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=1, v1=0, adam=1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", seed=100, dropout=0.5, batchsize=512)\n",
      "Batch: [0/10], Time used: 4.4875s\n",
      "CS_50001 Epoch:0, Test "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 100.0000%, Acc_GPT: 100.0000%, Acc_Human: 100.0000%, F1: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(domain='CS_5000', task=2, prompt=0, expid='Unified_CS_Test_Task2_PromptALL', sample=50000,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " early=100.0, save=1, modelid=0, pretrain=1, saved_model='../CheckGPT_presaved_files/saved_experimen"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts/basic/CS_Task2_Prompt1234/Best_CS_Task2.pth', dataamount=100, trans=0, splitr=0.8, ablr=1.0, lr=0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0002, nepochs=100, test=1, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=1, v1=0, adam=1,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " seed=100, dropout=0.5, batchsize=512)\n",
      "Batch: [0/10], Time used: 4.4228s\n",
      "CS_50002 Epoch:0, Test a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ccuracy: 99.8000%, Acc_GPT: 99.9500%, Acc_Human: 99.2000%, F1: 0.9988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(domain='CS_5000', task=3, prompt=0, expid='Unified_CS_Test_Task3_PromptALL', sample=50000,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " early=100.0, save=1, modelid=0, pretrain=1, saved_model='../CheckGPT_presaved_files/saved_experimen"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts/basic/CS_Task3_Prompt1234/Best_CS_Task3.pth', dataamount=100, trans=0, splitr=0.8, ablr=1.0, lr=0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0002, nepochs=100, test=1, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=1, v1=0, adam=1,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " seed=100, dropout=0.5, batchsize=512)\n",
      "Batch: [0/10], Time used: 6.4007s\n",
      "CS_50003 Epoch:0, Test a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ccuracy: 99.5400%, Acc_GPT: 99.7750%, Acc_Human: 98.6000%, F1: 0.9971\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "python dnn.py CS_5000 1 0 Unified_CS_Test_Task1_PromptALL --pretrain 1 --test 1 --saved-model ../CheckGPT_presaved_files/saved_experiments/basic/CS_Task1_Prompt_1234/Best_CS_Task1.pth\n",
    "python dnn.py CS_5000 2 0 Unified_CS_Test_Task2_PromptALL --pretrain 1 --test 1 --saved-model ../CheckGPT_presaved_files/saved_experiments/basic/CS_Task2_Prompt1234/Best_CS_Task2.pth\n",
    "python dnn.py CS_5000 3 0 Unified_CS_Test_Task3_PromptALL --pretrain 1 --test 1 --saved-model ../CheckGPT_presaved_files/saved_experiments/basic/CS_Task3_Prompt1234/Best_CS_Task3.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b15829d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T05:46:25.484073Z",
     "iopub.status.busy": "2025-12-31T05:46:25.483504Z",
     "iopub.status.idle": "2025-12-31T05:46:25.881737Z",
     "shell.execute_reply": "2025-12-31T05:46:25.878783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 31 05:46:25 UTC 2025\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150G\t.\r\n"
     ]
    }
   ],
   "source": [
    "!date\n",
    "!du -hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3649e75b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T05:46:25.888043Z",
     "iopub.status.busy": "2025-12-31T05:46:25.887380Z",
     "iopub.status.idle": "2025-12-31T05:46:44.280944Z",
     "shell.execute_reply": "2025-12-31T05:46:44.278703Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "shopt -s extglob\n",
    "rm -rf embeddings/!(README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfba870b",
   "metadata": {},
   "source": [
    "## PHX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dffe9c4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T05:46:44.287440Z",
     "iopub.status.busy": "2025-12-31T05:46:44.286865Z",
     "iopub.status.idle": "2025-12-31T05:46:44.682001Z",
     "shell.execute_reply": "2025-12-31T05:46:44.679297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 31 05:46:44 UTC 2025\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13G\t.\r\n"
     ]
    }
   ],
   "source": [
    "!date\n",
    "!du -hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ea81522",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T05:46:44.688837Z",
     "iopub.status.busy": "2025-12-31T05:46:44.688213Z",
     "iopub.status.idle": "2025-12-31T06:18:26.708564Z",
     "shell.execute_reply": "2025-12-31T06:18:26.706486Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: ground, data num: 50000\n",
      "ground. Complete 0/5000 (0.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 0.4079110622406006s. Overlong: 0\n",
      "ground. Complete 200/5000 (4.0%). Time used: 5.376545429229736s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Overlong: 0\n",
      "ground. Complete 400/5000 (8.0%). Time used: 10.231054306030273s. Overlong: 0\n",
      "ground. "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete 600/5000 (12.0%). Time used: 15.16041374206543s. Overlong: 1\n",
      "ground. Complete 800/5000 (16."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 20.290281057357788s. Overlong: 1\n",
      "ground. Complete 1000/5000 (20.0%). Time used: 25.3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9200735092163s. Overlong: 1\n",
      "ground. Complete 1200/5000 (24.0%). Time used: 30.304385662078857s. Over"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long: 1\n",
      "ground. Complete 1400/5000 (28.0%). Time used: 35.44050979614258s. Overlong: 2\n",
      "ground. Compl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ete 1600/5000 (32.0%). Time used: 40.16854500770569s. Overlong: 3\n",
      "ground. Complete 1800/5000 (36.0%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Time used: 45.25236701965332s. Overlong: 3\n",
      "ground. Complete 2000/5000 (40.0%). Time used: 50.36945"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295333862s. Overlong: 3\n",
      "ground. Complete 2200/5000 (44.0%). Time used: 55.46863389015198s. Overlong:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4\n",
      "ground. Complete 2400/5000 (48.0%). Time used: 60.606168031692505s. Overlong: 4\n",
      "ground. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2600/5000 (52.0%). Time used: 65.86978530883789s. Overlong: 4\n",
      "ground. Complete 2800/5000 (56.0%). Ti"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me used: 70.61618733406067s. Overlong: 4\n",
      "ground. Complete 3000/5000 (60.0%). Time used: 75.221173048"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01941s. Overlong: 4\n",
      "ground. Complete 3200/5000 (64.0%). Time used: 80.20675277709961s. Overlong: 4\n",
      "g"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round. Complete 3400/5000 (68.0%). Time used: 85.13524174690247s. Overlong: 4\n",
      "ground. Complete 3600/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 (72.0%). Time used: 90.14579558372498s. Overlong: 4\n",
      "ground. Complete 3800/5000 (76.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 95.1741418838501s. Overlong: 4\n",
      "ground. Complete 4000/5000 (80.0%). Time used: 100.2355523109436s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Overlong: 4\n",
      "ground. Complete 4200/5000 (84.0%). Time used: 105.25754165649414s. Overlong: 5\n",
      "ground"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Complete 4400/5000 (88.0%). Time used: 110.18788647651672s. Overlong: 6\n",
      "ground. Complete 4600/5000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (92.0%). Time used: 115.42939519882202s. Overlong: 6\n",
      "ground. Complete 4800/5000 (96.0%). Time used:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 120.99799704551697s. Overlong: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: ground_task2, data num: 50000\n",
      "ground_task2. Complete 0/5000 (0."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 0.4058341979980469s. Overlong: 0\n",
      "ground_task2. Complete 200/5000 (4.0%). Time used: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.622919082641602s. Overlong: 0\n",
      "ground_task2. Complete 400/5000 (8.0%). Time used: 8.735323190689087"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s. Overlong: 0\n",
      "ground_task2. Complete 600/5000 (12.0%). Time used: 12.840733051300049s. Overlong: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_task2. Complete 800/5000 (16.0%). Time used: 17.000024795532227s. Overlong: 0\n",
      "ground_task2. C"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omplete 1000/5000 (20.0%). Time used: 21.150738954544067s. Overlong: 0\n",
      "ground_task2. Complete 1200/5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000 (24.0%). Time used: 25.251081228256226s. Overlong: 0\n",
      "ground_task2. Complete 1400/5000 (28.0%). T"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ime used: 29.40407919883728s. Overlong: 0\n",
      "ground_task2. Complete 1600/5000 (32.0%). Time used: 33.45"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "890927314758s. Overlong: 0\n",
      "ground_task2. Complete 1800/5000 (36.0%). Time used: 37.629956007003784s."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Overlong: 0\n",
      "ground_task2. Complete 2000/5000 (40.0%). Time used: 41.84642577171326s. Overlong: 0\n",
      "gr"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ound_task2. Complete 2200/5000 (44.0%). Time used: 46.00391912460327s. Overlong: 0\n",
      "ground_task2. Com"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plete 2400/5000 (48.0%). Time used: 50.155484437942505s. Overlong: 0\n",
      "ground_task2. Complete 2600/500"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (52.0%). Time used: 54.35788917541504s. Overlong: 0\n",
      "ground_task2. Complete 2800/5000 (56.0%). Time"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " used: 58.393272161483765s. Overlong: 0\n",
      "ground_task2. Complete 3000/5000 (60.0%). Time used: 62.3604"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4478416443s. Overlong: 0\n",
      "ground_task2. Complete 3200/5000 (64.0%). Time used: 66.47501420974731s. Ov"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erlong: 0\n",
      "ground_task2. Complete 3400/5000 (68.0%). Time used: 70.58474802970886s. Overlong: 0\n",
      "groun"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_task2. Complete 3600/5000 (72.0%). Time used: 74.73158073425293s. Overlong: 0\n",
      "ground_task2. Comple"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "te 3800/5000 (76.0%). Time used: 78.8935341835022s. Overlong: 0\n",
      "ground_task2. Complete 4000/5000 (80"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 83.06815886497498s. Overlong: 0\n",
      "ground_task2. Complete 4200/5000 (84.0%). Time used"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 87.21252059936523s. Overlong: 0\n",
      "ground_task2. Complete 4400/5000 (88.0%). Time used: 91.2974729537"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9639s. Overlong: 0\n",
      "ground_task2. Complete 4600/5000 (92.0%). Time used: 95.46117782592773s. Overlong"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 0\n",
      "ground_task2. Complete 4800/5000 (96.0%). Time used: 99.77710151672363s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task1_prompt1, data num: 50000\n",
      "gpt_task1_prompt1. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.40795326232910156s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 200/5000 (4."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 5.202618360519409s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 400/5000 (8.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 9.992843627929688s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 600/5000 (12.0%). Time used: 14.7383"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27026367188s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 800/5000 (16.0%). Time used: 19.46583724021911"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 1000/5000 (20.0%). Time used: 24.16267418861389s. Overlo"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng: 0\n",
      "gpt_task1_prompt1. Complete 1200/5000 (24.0%). Time used: 28.856420040130615s. Overlong: 0\n",
      "gpt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_task1_prompt1. Complete 1400/5000 (28.0%). Time used: 33.5667245388031s. Overlong: 1\n",
      "gpt_task1_prom"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt1. Complete 1600/5000 (32.0%). Time used: 38.20319628715515s. Overlong: 1\n",
      "gpt_task1_prompt1. Compl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ete 1800/5000 (36.0%). Time used: 42.98491835594177s. Overlong: 2\n",
      "gpt_task1_prompt1. Complete 2000/5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000 (40.0%). Time used: 47.70466375350952s. Overlong: 2\n",
      "gpt_task1_prompt1. Complete 2200/5000 (44.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "). Time used: 52.49395298957825s. Overlong: 2\n",
      "gpt_task1_prompt1. Complete 2400/5000 (48.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 57.25910496711731s. Overlong: 2\n",
      "gpt_task1_prompt1. Complete 2600/5000 (52.0%). Time used: 62.088"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09757232666s. Overlong: 2\n",
      "gpt_task1_prompt1. Complete 2800/5000 (56.0%). Time used: 66.7792472839355"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5s. Overlong: 3\n",
      "gpt_task1_prompt1. Complete 3000/5000 (60.0%). Time used: 71.36728739738464s. Overlo"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng: 3\n",
      "gpt_task1_prompt1. Complete 3200/5000 (64.0%). Time used: 76.07740139961243s. Overlong: 3\n",
      "gpt_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task1_prompt1. Complete 3400/5000 (68.0%). Time used: 80.89645385742188s. Overlong: 3\n",
      "gpt_task1_prom"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt1. Complete 3600/5000 (72.0%). Time used: 85.5917558670044s. Overlong: 3\n",
      "gpt_task1_prompt1. Comple"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "te 3800/5000 (76.0%). Time used: 90.24944972991943s. Overlong: 3\n",
      "gpt_task1_prompt1. Complete 4000/50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 (80.0%). Time used: 95.08404731750488s. Overlong: 3\n",
      "gpt_task1_prompt1. Complete 4200/5000 (84.0%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Time used: 99.81815385818481s. Overlong: 3\n",
      "gpt_task1_prompt1. Complete 4400/5000 (88.0%). Time use"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 104.49693036079407s. Overlong: 4\n",
      "gpt_task1_prompt1. Complete 4600/5000 (92.0%). Time used: 109.22"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354745864868s. Overlong: 4\n",
      "gpt_task1_prompt1. Complete 4800/5000 (96.0%). Time used: 114.11480522155"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "762s. Overlong: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task1_prompt2, data num: 50000\n",
      "gpt_task1_prompt2. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.40675973892211914s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 200/5000 (4."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 5.36121678352356s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 400/5000 (8.0%). Time use"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 10.308095216751099s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 600/5000 (12.0%). Time used: 15.5163"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31911087036s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 800/5000 (16.0%). Time used: 20.80260086059570"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 1000/5000 (20.0%). Time used: 25.903817176818848s. Overl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ong: 0\n",
      "gpt_task1_prompt2. Complete 1200/5000 (24.0%). Time used: 31.12319254875183s. Overlong: 1\n",
      "gpt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_task1_prompt2. Complete 1400/5000 (28.0%). Time used: 36.098576068878174s. Overlong: 1\n",
      "gpt_task1_pr"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ompt2. Complete 1600/5000 (32.0%). Time used: 40.867823123931885s. Overlong: 1\n",
      "gpt_task1_prompt2. Co"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mplete 1800/5000 (36.0%). Time used: 45.77469253540039s. Overlong: 1\n",
      "gpt_task1_prompt2. Complete 200"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (40.0%). Time used: 50.72005534172058s. Overlong: 1\n",
      "gpt_task1_prompt2. Complete 2200/5000 (44"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 55.582958936691284s. Overlong: 1\n",
      "gpt_task1_prompt2. Complete 2400/5000 (48.0%). Tim"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e used: 60.447078704833984s. Overlong: 2\n",
      "gpt_task1_prompt2. Complete 2600/5000 (52.0%). Time used: 6"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.4260151386261s. Overlong: 2\n",
      "gpt_task1_prompt2. Complete 2800/5000 (56.0%). Time used: 70.317919492"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72156s. Overlong: 3\n",
      "gpt_task1_prompt2. Complete 3000/5000 (60.0%). Time used: 75.19239974021912s. Ov"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erlong: 3\n",
      "gpt_task1_prompt2. Complete 3200/5000 (64.0%). Time used: 80.04831171035767s. Overlong: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_task1_prompt2. Complete 3400/5000 (68.0%). Time used: 84.79789853096008s. Overlong: 3\n",
      "gpt_task1_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt2. Complete 3600/5000 (72.0%). Time used: 89.61066794395447s. Overlong: 4\n",
      "gpt_task1_prompt2. C"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omplete 3800/5000 (76.0%). Time used: 94.42752647399902s. Overlong: 4\n",
      "gpt_task1_prompt2. Complete 40"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00/5000 (80.0%). Time used: 99.1903817653656s. Overlong: 4\n",
      "gpt_task1_prompt2. Complete 4200/5000 (84"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 104.09562849998474s. Overlong: 5\n",
      "gpt_task1_prompt2. Complete 4400/5000 (88.0%). Tim"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e used: 108.96639347076416s. Overlong: 5\n",
      "gpt_task1_prompt2. Complete 4600/5000 (92.0%). Time used: 1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.78398776054382s. Overlong: 5\n",
      "gpt_task1_prompt2. Complete 4800/5000 (96.0%). Time used: 118.811963"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55819702s. Overlong: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "atability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initia"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t/.venv/lib/python3.9/site-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "atability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initia"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task1_prompt3, data num: 50000\n",
      "gpt_task1_prompt3. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.41200852394104004s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 200/5000 (4."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 5.049177885055542s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 400/5000 (8.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 9.64646601676941s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 600/5000 (12.0%). Time used: 14.24276"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9002914429s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 800/5000 (16.0%). Time used: 18.867071866989136"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 1000/5000 (20.0%). Time used: 23.436882495880127s. Overlo"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng: 0\n",
      "gpt_task1_prompt3. Complete 1200/5000 (24.0%). Time used: 28.084125518798828s. Overlong: 0\n",
      "gpt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_task1_prompt3. Complete 1400/5000 (28.0%). Time used: 32.74727988243103s. Overlong: 0\n",
      "gpt_task1_pro"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpt3. Complete 1600/5000 (32.0%). Time used: 37.273401498794556s. Overlong: 0\n",
      "gpt_task1_prompt3. Com"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plete 1800/5000 (36.0%). Time used: 41.89186668395996s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 2000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/5000 (40.0%). Time used: 46.44167423248291s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 2200/5000 (44."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 51.053080558776855s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 2400/5000 (48.0%). Time"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " used: 55.67913055419922s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 2600/5000 (52.0%). Time used: 60."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28314185142517s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 2800/5000 (56.0%). Time used: 64.8746480941"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7725s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 3000/5000 (60.0%). Time used: 69.40680074691772s. Ove"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rlong: 0\n",
      "gpt_task1_prompt3. Complete 3200/5000 (64.0%). Time used: 73.96048665046692s. Overlong: 0\n",
      "g"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt_task1_prompt3. Complete 3400/5000 (68.0%). Time used: 78.49751830101013s. Overlong: 0\n",
      "gpt_task1_p"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rompt3. Complete 3600/5000 (72.0%). Time used: 83.09300780296326s. Overlong: 0\n",
      "gpt_task1_prompt3. Co"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mplete 3800/5000 (76.0%). Time used: 87.76481628417969s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 400"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (80.0%). Time used: 92.34130001068115s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 4200/5000 (84"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 96.93861222267151s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 4400/5000 (88.0%). Time"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " used: 101.52373814582825s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 4600/5000 (92.0%). Time used: 10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.12440800666809s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 4800/5000 (96.0%). Time used: 110.7693390"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8462524s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task1_prompt4, data num: 50000\n",
      "gpt_task1_prompt4. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.41388893127441406s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 200/5000 (4."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 5.414479494094849s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 400/5000 (8.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 10.569316148757935s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 600/5000 (12.0%). Time used: 15.566"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172361373901s. Overlong: 1\n",
      "gpt_task1_prompt4. Complete 800/5000 (16.0%). Time used: 20.7039091587066"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65s. Overlong: 1\n",
      "gpt_task1_prompt4. Complete 1000/5000 (20.0%). Time used: 25.80852770805359s. Overl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ong: 1\n",
      "gpt_task1_prompt4. Complete 1200/5000 (24.0%). Time used: 30.9243586063385s. Overlong: 1\n",
      "gpt_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task1_prompt4. Complete 1400/5000 (28.0%). Time used: 35.94074273109436s. Overlong: 1\n",
      "gpt_task1_prom"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt4. Complete 1600/5000 (32.0%). Time used: 40.89608335494995s. Overlong: 1\n",
      "gpt_task1_prompt4. Compl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ete 1800/5000 (36.0%). Time used: 46.004568576812744s. Overlong: 1\n",
      "gpt_task1_prompt4. Complete 2000/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 (40.0%). Time used: 51.24699378013611s. Overlong: 1\n",
      "gpt_task1_prompt4. Complete 2200/5000 (44.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 56.3740599155426s. Overlong: 1\n",
      "gpt_task1_prompt4. Complete 2400/5000 (48.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 61.60260891914368s. Overlong: 1\n",
      "gpt_task1_prompt4. Complete 2600/5000 (52.0%). Time used: 66.958"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16135406494s. Overlong: 2\n",
      "gpt_task1_prompt4. Complete 2800/5000 (56.0%). Time used: 71.95108294487s."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Overlong: 2\n",
      "gpt_task1_prompt4. Complete 3000/5000 (60.0%). Time used: 76.81021571159363s. Overlong:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3\n",
      "gpt_task1_prompt4. Complete 3200/5000 (64.0%). Time used: 81.70805072784424s. Overlong: 3\n",
      "gpt_tas"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k1_prompt4. Complete 3400/5000 (68.0%). Time used: 86.69683980941772s. Overlong: 4\n",
      "gpt_task1_prompt4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Complete 3600/5000 (72.0%). Time used: 91.76444602012634s. Overlong: 4\n",
      "gpt_task1_prompt4. Complete"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3800/5000 (76.0%). Time used: 96.97589921951294s. Overlong: 4\n",
      "gpt_task1_prompt4. Complete 4000/5000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (80.0%). Time used: 102.1173255443573s. Overlong: 4\n",
      "gpt_task1_prompt4. Complete 4200/5000 (84.0%). "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used: 107.17108345031738s. Overlong: 4\n",
      "gpt_task1_prompt4. Complete 4400/5000 (88.0%). Time used"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 112.37886190414429s. Overlong: 4\n",
      "gpt_task1_prompt4. Complete 4600/5000 (92.0%). Time used: 117.370"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30458450317s. Overlong: 4\n",
      "gpt_task1_prompt4. Complete 4800/5000 (96.0%). Time used: 122.772542476654"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05s. Overlong: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task2_prompt1, data num: 50000\n",
      "gpt_task2_prompt1. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.3999595642089844s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 200/5000 (4.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 4.833475112915039s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 400/5000 (8.0%). Time use"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 9.139209508895874s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 600/5000 (12.0%). Time used: 13.46066"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7371749878s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 800/5000 (16.0%). Time used: 17.914021968841553"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s. Overlong: 1\n",
      "gpt_task2_prompt1. Complete 1000/5000 (20.0%). Time used: 22.329986333847046s. Overlo"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng: 1\n",
      "gpt_task2_prompt1. Complete 1200/5000 (24.0%). Time used: 26.669158220291138s. Overlong: 1\n",
      "gpt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_task2_prompt1. Complete 1400/5000 (28.0%). Time used: 31.08127498626709s. Overlong: 1\n",
      "gpt_task2_pro"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpt1. Complete 1600/5000 (32.0%). Time used: 35.310726165771484s. Overlong: 1\n",
      "gpt_task2_prompt1. Com"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plete 1800/5000 (36.0%). Time used: 39.77223014831543s. Overlong: 2\n",
      "gpt_task2_prompt1. Complete 2000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/5000 (40.0%). Time used: 44.283952951431274s. Overlong: 2\n",
      "gpt_task2_prompt1. Complete 2200/5000 (44"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 48.75413227081299s. Overlong: 3\n",
      "gpt_task2_prompt1. Complete 2400/5000 (48.0%). Time"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " used: 53.223642110824585s. Overlong: 3\n",
      "gpt_task2_prompt1. Complete 2600/5000 (52.0%). Time used: 57"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".690998554229736s. Overlong: 3\n",
      "gpt_task2_prompt1. Complete 2800/5000 (56.0%). Time used: 61.94647932"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "052612s. Overlong: 3\n",
      "gpt_task2_prompt1. Complete 3000/5000 (60.0%). Time used: 66.05269026756287s. O"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verlong: 3\n",
      "gpt_task2_prompt1. Complete 3200/5000 (64.0%). Time used: 70.35528564453125s. Overlong: 3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gpt_task2_prompt1. Complete 3400/5000 (68.0%). Time used: 74.65163922309875s. Overlong: 3\n",
      "gpt_task2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_prompt1. Complete 3600/5000 (72.0%). Time used: 78.99767255783081s. Overlong: 3\n",
      "gpt_task2_prompt1. "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete 3800/5000 (76.0%). Time used: 83.41302943229675s. Overlong: 3\n",
      "gpt_task2_prompt1. Complete 4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000/5000 (80.0%). Time used: 87.9522807598114s. Overlong: 3\n",
      "gpt_task2_prompt1. Complete 4200/5000 (8"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0%). Time used: 92.36549997329712s. Overlong: 3\n",
      "gpt_task2_prompt1. Complete 4400/5000 (88.0%). Tim"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e used: 96.70538353919983s. Overlong: 3\n",
      "gpt_task2_prompt1. Complete 4600/5000 (92.0%). Time used: 10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14308261871338s. Overlong: 4\n",
      "gpt_task2_prompt1. Complete 4800/5000 (96.0%). Time used: 105.8710861"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2060547s. Overlong: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task2_prompt2, data num: 50000\n",
      "gpt_task2_prompt2. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.3974184989929199s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 200/5000 (4.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 4.890508413314819s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 400/5000 (8.0%). Time use"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 9.248972654342651s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 600/5000 (12.0%). Time used: 13.62351"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91822052s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 800/5000 (16.0%). Time used: 18.140984535217285s."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Overlong: 0\n",
      "gpt_task2_prompt2. Complete 1000/5000 (20.0%). Time used: 22.665215253829956s. Overlong"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 1\n",
      "gpt_task2_prompt2. Complete 1200/5000 (24.0%). Time used: 27.07396411895752s. Overlong: 1\n",
      "gpt_ta"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk2_prompt2. Complete 1400/5000 (28.0%). Time used: 31.64869737625122s. Overlong: 2\n",
      "gpt_task2_prompt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Complete 1600/5000 (32.0%). Time used: 35.90726590156555s. Overlong: 2\n",
      "gpt_task2_prompt2. Complet"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 1800/5000 (36.0%). Time used: 40.396830797195435s. Overlong: 2\n",
      "gpt_task2_prompt2. Complete 2000/50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 (40.0%). Time used: 44.93047738075256s. Overlong: 2\n",
      "gpt_task2_prompt2. Complete 2200/5000 (44.0%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Time used: 49.44237542152405s. Overlong: 2\n",
      "gpt_task2_prompt2. Complete 2400/5000 (48.0%). Time use"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 53.976534366607666s. Overlong: 2\n",
      "gpt_task2_prompt2. Complete 2600/5000 (52.0%). Time used: 58.558"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7854385376s. Overlong: 3\n",
      "gpt_task2_prompt2. Complete 2800/5000 (56.0%). Time used: 62.80618476867676"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s. Overlong: 3\n",
      "gpt_task2_prompt2. Complete 3000/5000 (60.0%). Time used: 66.9757239818573s. Overlong"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 3\n",
      "gpt_task2_prompt2. Complete 3200/5000 (64.0%). Time used: 71.3547477722168s. Overlong: 3\n",
      "gpt_tas"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k2_prompt2. Complete 3400/5000 (68.0%). Time used: 75.75217795372009s. Overlong: 3\n",
      "gpt_task2_prompt2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Complete 3600/5000 (72.0%). Time used: 80.15266370773315s. Overlong: 3\n",
      "gpt_task2_prompt2. Complete"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3800/5000 (76.0%). Time used: 84.61911869049072s. Overlong: 3\n",
      "gpt_task2_prompt2. Complete 4000/5000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (80.0%). Time used: 89.04826879501343s. Overlong: 3\n",
      "gpt_task2_prompt2. Complete 4200/5000 (84.0%). "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used: 93.56301307678223s. Overlong: 3\n",
      "gpt_task2_prompt2. Complete 4400/5000 (88.0%). Time used:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 97.97756457328796s. Overlong: 3\n",
      "gpt_task2_prompt2. Complete 4600/5000 (92.0%). Time used: 102.51955"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270767212s. Overlong: 3\n",
      "gpt_task2_prompt2. Complete 4800/5000 (96.0%). Time used: 107.32922339439392"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s. Overlong: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task2_prompt3, data num: 50000\n",
      "gpt_task2_prompt3. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.39911627769470215s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 200/5000 (4."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 4.858046531677246s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 400/5000 (8.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 9.235915422439575s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 600/5000 (12.0%). Time used: 13.5987"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97798156738s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 800/5000 (16.0%). Time used: 18.06342816352844"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 1000/5000 (20.0%). Time used: 22.542537450790405s. Overl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ong: 0\n",
      "gpt_task2_prompt3. Complete 1200/5000 (24.0%). Time used: 26.938659191131592s. Overlong: 0\n",
      "gp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_task2_prompt3. Complete 1400/5000 (28.0%). Time used: 31.403459787368774s. Overlong: 0\n",
      "gpt_task2_p"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rompt3. Complete 1600/5000 (32.0%). Time used: 35.63541769981384s. Overlong: 0\n",
      "gpt_task2_prompt3. Co"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mplete 1800/5000 (36.0%). Time used: 40.12158393859863s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 200"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (40.0%). Time used: 44.640012979507446s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 2200/5000 (4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0%). Time used: 49.060940742492676s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 2400/5000 (48.0%). Ti"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me used: 53.56918215751648s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 2600/5000 (52.0%). Time used: 5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.1888906955719s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 2800/5000 (56.0%). Time used: 62.397134780"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88379s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 3000/5000 (60.0%). Time used: 66.61426091194153s. Ov"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erlong: 0\n",
      "gpt_task2_prompt3. Complete 3200/5000 (64.0%). Time used: 70.95295858383179s. Overlong: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_task2_prompt3. Complete 3400/5000 (68.0%). Time used: 75.24685955047607s. Overlong: 0\n",
      "gpt_task2_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt3. Complete 3600/5000 (72.0%). Time used: 79.61196422576904s. Overlong: 0\n",
      "gpt_task2_prompt3. C"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omplete 3800/5000 (76.0%). Time used: 84.0423903465271s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 400"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (80.0%). Time used: 88.47332978248596s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 4200/5000 (84"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 92.87446546554565s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 4400/5000 (88.0%). Time"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " used: 97.2775650024414s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 4600/5000 (92.0%). Time used: 101."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79373168945312s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 4800/5000 (96.0%). Time used: 106.656675815"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58228s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task2_prompt4, data num: 50000\n",
      "gpt_task2_prompt4. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.4065871238708496s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 200/5000 (4.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 4.8640429973602295s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 400/5000 (8.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 9.268532991409302s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 600/5000 (12.0%). Time used: 13.7497"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32255935669s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 800/5000 (16.0%). Time used: 18.30244922637939"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 1000/5000 (20.0%). Time used: 22.861153841018677s. Overl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ong: 0\n",
      "gpt_task2_prompt4. Complete 1200/5000 (24.0%). Time used: 27.316250801086426s. Overlong: 0\n",
      "gp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_task2_prompt4. Complete 1400/5000 (28.0%). Time used: 31.882215976715088s. Overlong: 0\n",
      "gpt_task2_p"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rompt4. Complete 1600/5000 (32.0%). Time used: 36.22188448905945s. Overlong: 0\n",
      "gpt_task2_prompt4. Co"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mplete 1800/5000 (36.0%). Time used: 40.735225677490234s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 20"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00/5000 (40.0%). Time used: 45.32412362098694s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 2200/5000 (4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0%). Time used: 49.895498275756836s. Overlong: 2\n",
      "gpt_task2_prompt4. Complete 2400/5000 (48.0%). Ti"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me used: 54.45124411582947s. Overlong: 2\n",
      "gpt_task2_prompt4. Complete 2600/5000 (52.0%). Time used: 5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.05783128738403s. Overlong: 3\n",
      "gpt_task2_prompt4. Complete 2800/5000 (56.0%). Time used: 63.38056993"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484497s. Overlong: 3\n",
      "gpt_task2_prompt4. Complete 3000/5000 (60.0%). Time used: 67.64572596549988s. O"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verlong: 4\n",
      "gpt_task2_prompt4. Complete 3200/5000 (64.0%). Time used: 72.06736660003662s. Overlong: 4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gpt_task2_prompt4. Complete 3400/5000 (68.0%). Time used: 76.5679943561554s. Overlong: 4\n",
      "gpt_task2_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt4. Complete 3600/5000 (72.0%). Time used: 81.08328247070312s. Overlong: 4\n",
      "gpt_task2_prompt4. C"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omplete 3800/5000 (76.0%). Time used: 85.66868185997009s. Overlong: 4\n",
      "gpt_task2_prompt4. Complete 40"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00/5000 (80.0%). Time used: 90.18302655220032s. Overlong: 4\n",
      "gpt_task2_prompt4. Complete 4200/5000 (8"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0%). Time used: 94.7420928478241s. Overlong: 4\n",
      "gpt_task2_prompt4. Complete 4400/5000 (88.0%). Time"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " used: 99.22972559928894s. Overlong: 4\n",
      "gpt_task2_prompt4. Complete 4600/5000 (92.0%). Time used: 103"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".71493721008301s. Overlong: 4\n",
      "gpt_task2_prompt4. Complete 4800/5000 (96.0%). Time used: 108.52242064"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "476013s. Overlong: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task3_prompt1, data num: 50000\n",
      "gpt_task3_prompt1. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.428804874420166s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 200/5000 (4.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "). Time used: 5.4991185665130615s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 400/5000 (8.0%). Time use"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 10.38687252998352s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 600/5000 (12.0%). Time used: 15.40779"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6144485474s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 800/5000 (16.0%). Time used: 20.583019971847534"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 1000/5000 (20.0%). Time used: 25.700377702713013s. Overlo"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng: 0\n",
      "gpt_task3_prompt1. Complete 1200/5000 (24.0%). Time used: 30.717527389526367s. Overlong: 0\n",
      "gpt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_task3_prompt1. Complete 1400/5000 (28.0%). Time used: 35.87104940414429s. Overlong: 1\n",
      "gpt_task3_pro"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpt1. Complete 1600/5000 (32.0%). Time used: 40.63695740699768s. Overlong: 2\n",
      "gpt_task3_prompt1. Comp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lete 1800/5000 (36.0%). Time used: 45.75584673881531s. Overlong: 2\n",
      "gpt_task3_prompt1. Complete 2000/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 (40.0%). Time used: 50.88172101974487s. Overlong: 4\n",
      "gpt_task3_prompt1. Complete 2200/5000 (44.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 55.990416526794434s. Overlong: 5\n",
      "gpt_task3_prompt1. Complete 2400/5000 (48.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 61.11627984046936s. Overlong: 5\n",
      "gpt_task3_prompt1. Complete 2600/5000 (52.0%). Time used: 66.4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0669536590576s. Overlong: 5\n",
      "gpt_task3_prompt1. Complete 2800/5000 (56.0%). Time used: 71.21056985855"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103s. Overlong: 5\n",
      "gpt_task3_prompt1. Complete 3000/5000 (60.0%). Time used: 75.86801433563232s. Over"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long: 5\n",
      "gpt_task3_prompt1. Complete 3200/5000 (64.0%). Time used: 80.84153771400452s. Overlong: 5\n",
      "gp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_task3_prompt1. Complete 3400/5000 (68.0%). Time used: 85.79382538795471s. Overlong: 5\n",
      "gpt_task3_pr"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ompt1. Complete 3600/5000 (72.0%). Time used: 90.82776927947998s. Overlong: 5\n",
      "gpt_task3_prompt1. Com"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plete 3800/5000 (76.0%). Time used: 95.81196975708008s. Overlong: 5\n",
      "gpt_task3_prompt1. Complete 4000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/5000 (80.0%). Time used: 100.86790418624878s. Overlong: 5\n",
      "gpt_task3_prompt1. Complete 4200/5000 (84"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 105.8806140422821s. Overlong: 5\n",
      "gpt_task3_prompt1. Complete 4400/5000 (88.0%). Time"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " used: 110.85096025466919s. Overlong: 6\n",
      "gpt_task3_prompt1. Complete 4600/5000 (92.0%). Time used: 11"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.07553219795227s. Overlong: 6\n",
      "gpt_task3_prompt1. Complete 4800/5000 (96.0%). Time used: 121.5859940"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0520325s. Overlong: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task3_prompt2, data num: 50000\n",
      "gpt_task3_prompt2. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.4173471927642822s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 200/5000 (4.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 5.608703136444092s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 400/5000 (8.0%). Time use"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 10.656304836273193s. Overlong: 2\n",
      "gpt_task3_prompt2. Complete 600/5000 (12.0%). Time used: 15.7843"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11532974243s. Overlong: 5\n",
      "gpt_task3_prompt2. Complete 800/5000 (16.0%). Time used: 21.10416269302368"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s. Overlong: 5\n",
      "gpt_task3_prompt2. Complete 1000/5000 (20.0%). Time used: 26.37646484375s. Overlong: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "gpt_task3_prompt2. Complete 1200/5000 (24.0%). Time used: 31.492483615875244s. Overlong: 5\n",
      "gpt_tas"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k3_prompt2. Complete 1400/5000 (28.0%). Time used: 36.818907022476196s. Overlong: 6\n",
      "gpt_task3_prompt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Complete 1600/5000 (32.0%). Time used: 41.70013976097107s. Overlong: 6\n",
      "gpt_task3_prompt2. Complet"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 1800/5000 (36.0%). Time used: 46.9443256855011s. Overlong: 7\n",
      "gpt_task3_prompt2. Complete 2000/5000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (40.0%). Time used: 52.22304725646973s. Overlong: 8\n",
      "gpt_task3_prompt2. Complete 2200/5000 (44.0%). "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used: 57.515217542648315s. Overlong: 9\n",
      "gpt_task3_prompt2. Complete 2400/5000 (48.0%). Time used"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 62.837852478027344s. Overlong: 9\n",
      "gpt_task3_prompt2. Complete 2600/5000 (52.0%). Time used: 68.3222"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4416732788s. Overlong: 9\n",
      "gpt_task3_prompt2. Complete 2800/5000 (56.0%). Time used: 73.23045444488525"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s. Overlong: 9\n",
      "gpt_task3_prompt2. Complete 3000/5000 (60.0%). Time used: 77.96211338043213s. Overlon"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g: 9\n",
      "gpt_task3_prompt2. Complete 3200/5000 (64.0%). Time used: 83.14526796340942s. Overlong: 11\n",
      "gpt_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task3_prompt2. Complete 3400/5000 (68.0%). Time used: 88.2043309211731s. Overlong: 11\n",
      "gpt_task3_prom"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt2. Complete 3600/5000 (72.0%). Time used: 93.42670893669128s. Overlong: 11\n",
      "gpt_task3_prompt2. Comp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lete 3800/5000 (76.0%). Time used: 98.58059358596802s. Overlong: 12\n",
      "gpt_task3_prompt2. Complete 4000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/5000 (80.0%). Time used: 103.77386927604675s. Overlong: 13\n",
      "gpt_task3_prompt2. Complete 4200/5000 (8"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0%). Time used: 108.99526381492615s. Overlong: 15\n",
      "gpt_task3_prompt2. Complete 4400/5000 (88.0%). T"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ime used: 114.08932423591614s. Overlong: 17\n",
      "gpt_task3_prompt2. Complete 4600/5000 (92.0%). Time used"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 119.54106068611145s. Overlong: 18\n",
      "gpt_task3_prompt2. Complete 4800/5000 (96.0%). Time used: 125.24"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "817252159119s. Overlong: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task3_prompt3, data num: 50000\n",
      "gpt_task3_prompt3. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.4100501537322998s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete 200/5000 (4.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 5.543437957763672s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete 400/5000 (8.0%). Time use"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 10.586731195449829s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete 600/5000 (12.0%). Time used: 15.6288"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91468048096s. Overlong: 2\n",
      "gpt_task3_prompt3. Complete 800/5000 (16.0%). Time used: 20.90258622169494"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6s. Overlong: 2\n",
      "gpt_task3_prompt3. Complete 1000/5000 (20.0%). Time used: 26.10431408882141s. Overlo"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng: 3\n",
      "gpt_task3_prompt3. Complete 1200/5000 (24.0%). Time used: 31.182981252670288s. Overlong: 3\n",
      "gpt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_task3_prompt3. Complete 1400/5000 (28.0%). Time used: 36.48478412628174s. Overlong: 5\n",
      "gpt_task3_pro"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpt3. Complete 1600/5000 (32.0%). Time used: 41.29568529129028s. Overlong: 6\n",
      "gpt_task3_prompt3. Comp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lete 1800/5000 (36.0%). Time used: 46.557124853134155s. Overlong: 6\n",
      "gpt_task3_prompt3. Complete 2000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/5000 (40.0%). Time used: 51.7944221496582s. Overlong: 8\n",
      "gpt_task3_prompt3. Complete 2200/5000 (44.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 57.021661043167114s. Overlong: 10\n",
      "gpt_task3_prompt3. Complete 2400/5000 (48.0%). Time"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " used: 62.27868175506592s. Overlong: 10\n",
      "gpt_task3_prompt3. Complete 2600/5000 (52.0%). Time used: 67"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".70983934402466s. Overlong: 10\n",
      "gpt_task3_prompt3. Complete 2800/5000 (56.0%). Time used: 72.57154345"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51239s. Overlong: 11\n",
      "gpt_task3_prompt3. Complete 3000/5000 (60.0%). Time used: 77.30225133895874s. O"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verlong: 11\n",
      "gpt_task3_prompt3. Complete 3200/5000 (64.0%). Time used: 82.39290809631348s. Overlong: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "gpt_task3_prompt3. Complete 3400/5000 (68.0%). Time used: 87.39596366882324s. Overlong: 12\n",
      "gpt_ta"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk3_prompt3. Complete 3600/5000 (72.0%). Time used: 92.5364305973053s. Overlong: 12\n",
      "gpt_task3_prompt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Complete 3800/5000 (76.0%). Time used: 97.61693453788757s. Overlong: 13\n",
      "gpt_task3_prompt3. Comple"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "te 4000/5000 (80.0%). Time used: 102.75989079475403s. Overlong: 13\n",
      "gpt_task3_prompt3. Complete 4200/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 (84.0%). Time used: 107.92539882659912s. Overlong: 13\n",
      "gpt_task3_prompt3. Complete 4400/5000 (88"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 113.00217199325562s. Overlong: 14\n",
      "gpt_task3_prompt3. Complete 4600/5000 (92.0%). Ti"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me used: 118.36185455322266s. Overlong: 15\n",
      "gpt_task3_prompt3. Complete 4800/5000 (96.0%). Time used:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 124.03348970413208s. Overlong: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task3_prompt4, data num: 50000\n",
      "gpt_task3_prompt4. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.4151492118835449s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 200/5000 (4.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 5.834233522415161s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 400/5000 (8.0%). Time use"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 11.145932912826538s. Overlong: 3\n",
      "gpt_task3_prompt4. Complete 600/5000 (12.0%). Time used: 16.4527"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46391296387s. Overlong: 6\n",
      "gpt_task3_prompt4. Complete 800/5000 (16.0%). Time used: 21.99131822586059"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6s. Overlong: 8\n",
      "gpt_task3_prompt4. Complete 1000/5000 (20.0%). Time used: 27.502148628234863s. Overl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ong: 9\n",
      "gpt_task3_prompt4. Complete 1200/5000 (24.0%). Time used: 32.88244080543518s. Overlong: 10\n",
      "gp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_task3_prompt4. Complete 1400/5000 (28.0%). Time used: 38.481772661209106s. Overlong: 13\n",
      "gpt_task3_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt4. Complete 1600/5000 (32.0%). Time used: 43.58694314956665s. Overlong: 13\n",
      "gpt_task3_prompt4. "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete 1800/5000 (36.0%). Time used: 49.221277475357056s. Overlong: 13\n",
      "gpt_task3_prompt4. Complete"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2000/5000 (40.0%). Time used: 54.723589181900024s. Overlong: 17\n",
      "gpt_task3_prompt4. Complete 2200/50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 (44.0%). Time used: 60.27310037612915s. Overlong: 20\n",
      "gpt_task3_prompt4. Complete 2400/5000 (48.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "). Time used: 65.77414727210999s. Overlong: 21\n",
      "gpt_task3_prompt4. Complete 2600/5000 (52.0%). Time u"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sed: 71.42690563201904s. Overlong: 23\n",
      "gpt_task3_prompt4. Complete 2800/5000 (56.0%). Time used: 76.6"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3085913658142s. Overlong: 24\n",
      "gpt_task3_prompt4. Complete 3000/5000 (60.0%). Time used: 81.6744821071"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6248s. Overlong: 24\n",
      "gpt_task3_prompt4. Complete 3200/5000 (64.0%). Time used: 87.04740595817566s. Ov"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erlong: 24\n",
      "gpt_task3_prompt4. Complete 3400/5000 (68.0%). Time used: 92.32132744789124s. Overlong: 2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "gpt_task3_prompt4. Complete 3600/5000 (72.0%). Time used: 97.7906424999237s. Overlong: 29\n",
      "gpt_task"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_prompt4. Complete 3800/5000 (76.0%). Time used: 103.25311946868896s. Overlong: 31\n",
      "gpt_task3_prompt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Complete 4000/5000 (80.0%). Time used: 108.67205452919006s. Overlong: 32\n",
      "gpt_task3_prompt4. Compl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ete 4200/5000 (84.0%). Time used: 114.19126987457275s. Overlong: 33\n",
      "gpt_task3_prompt4. Complete 4400"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/5000 (88.0%). Time used: 119.48308610916138s. Overlong: 36\n",
      "gpt_task3_prompt4. Complete 4600/5000 (9"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0%). Time used: 125.29275512695312s. Overlong: 37\n",
      "gpt_task3_prompt4. Complete 4800/5000 (96.0%). T"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ime used: 131.19326448440552s. Overlong: 42\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "\n",
    "python features.py PHX 1 1 --gpt 0 --number 5000\n",
    "python features.py PHX 2 1 --gpt 0 --number 5000\n",
    "\n",
    "python features.py PHX 1 1 --gpt 1 --number 5000\n",
    "python features.py PHX 1 2 --gpt 1 --number 5000\n",
    "python features.py PHX 1 3 --gpt 1 --number 5000\n",
    "python features.py PHX 1 4 --gpt 1 --number 5000\n",
    "\n",
    "python features.py PHX 2 1 --gpt 1 --number 5000\n",
    "python features.py PHX 2 2 --gpt 1 --number 5000\n",
    "python features.py PHX 2 3 --gpt 1 --number 5000\n",
    "python features.py PHX 2 4 --gpt 1 --number 5000\n",
    "\n",
    "python features.py PHX 3 1 --gpt 1 --number 5000\n",
    "python features.py PHX 3 2 --gpt 1 --number 5000\n",
    "python features.py PHX 3 3 --gpt 1 --number 5000\n",
    "python features.py PHX 3 4 --gpt 1 --number 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ebf0675",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T06:18:26.716633Z",
     "iopub.status.busy": "2025-12-31T06:18:26.716335Z",
     "iopub.status.idle": "2025-12-31T06:18:27.113881Z",
     "shell.execute_reply": "2025-12-31T06:18:27.111265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 31 06:18:26 UTC 2025\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150G\t.\r\n"
     ]
    }
   ],
   "source": [
    "!date\n",
    "!du -hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d199d286",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T06:18:27.121241Z",
     "iopub.status.busy": "2025-12-31T06:18:27.120603Z",
     "iopub.status.idle": "2025-12-31T06:19:47.960333Z",
     "shell.execute_reply": "2025-12-31T06:19:47.958165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(domain='PHX_5000', task=1, prompt=0, expid='Unified_PHX_Test_Task1_PromptALL', sample=5000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, early=100.0, save=1, modelid=0, pretrain=1, saved_model='../CheckGPT_presaved_files/saved_experim"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ents/basic/PHX_Task1_Prompt1234/Best_PHY_Task1.pth', dataamount=100, trans=0, splitr=0.8, ablr=1.0, "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr=0.0002, nepochs=100, test=1, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=1, v1=0, ada"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m=1, seed=100, dropout=0.5, batchsize=512)\n",
      "Batch: [0/10], Time used: 5.6260s\n",
      "PHX_50001 Epoch:0, T"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est accuracy: 99.9800%, Acc_GPT: 99.9750%, Acc_Human: 100.0000%, F1: 0.9999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(domain='PHX_5000', task=2, prompt=0, expid='Unified_PHX_Test_Task2_PromptALL', sample=5000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, early=100.0, save=1, modelid=0, pretrain=1, saved_model='../CheckGPT_presaved_files/saved_experim"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ents/basic/PHX_Task2_Prompt1234/Best_PHY_Task2.pth', dataamount=100, trans=0, splitr=0.8, ablr=1.0, "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr=0.0002, nepochs=100, test=1, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=1, v1=0, ada"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m=1, seed=100, dropout=0.5, batchsize=512)\n",
      "Batch: [0/10], Time used: 5.5897s\n",
      "PHX_50002 Epoch:0, T"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est accuracy: 99.9400%, Acc_GPT: 99.9750%, Acc_Human: 99.8000%, F1: 0.9996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(domain='PHX_5000', task=3, prompt=0, expid='Unified_PHX_Test_Task3_PromptALL', sample=5000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, early=100.0, save=1, modelid=0, pretrain=1, saved_model='../CheckGPT_presaved_files/saved_experim"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ents/basic/PHX_Task3_Prompt1234/Best_PHY_Task3.pth', dataamount=100, trans=0, splitr=0.8, ablr=1.0, "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr=0.0002, nepochs=100, test=1, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=1, v1=0, ada"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m=1, seed=100, dropout=0.5, batchsize=512)\n",
      "Batch: [0/10], Time used: 4.5550s\n",
      "PHX_50003 Epoch:0, T"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est accuracy: 99.9800%, Acc_GPT: 100.0000%, Acc_Human: 99.9000%, F1: 0.9999\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "\n",
    "python dnn.py PHX_5000 1 0 Unified_PHX_Test_Task1_PromptALL --pretrain 1 --test 1 --saved-model ../CheckGPT_presaved_files/saved_experiments/basic/PHX_Task1_Prompt1234/Best_PHY_Task1.pth\n",
    "python dnn.py PHX_5000 2 0 Unified_PHX_Test_Task2_PromptALL --pretrain 1 --test 1 --saved-model ../CheckGPT_presaved_files/saved_experiments/basic/PHX_Task2_Prompt1234/Best_PHY_Task2.pth\n",
    "python dnn.py PHX_5000 3 0 Unified_PHX_Test_Task3_PromptALL --pretrain 1 --test 1 --saved-model ../CheckGPT_presaved_files/saved_experiments/basic/PHX_Task3_Prompt1234/Best_PHY_Task3.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c42adae5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T06:19:47.966669Z",
     "iopub.status.busy": "2025-12-31T06:19:47.966104Z",
     "iopub.status.idle": "2025-12-31T06:19:48.364525Z",
     "shell.execute_reply": "2025-12-31T06:19:48.361869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 31 06:19:47 UTC 2025\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150G\t.\r\n"
     ]
    }
   ],
   "source": [
    "!date\n",
    "!du -hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df7bb3b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T06:19:48.371625Z",
     "iopub.status.busy": "2025-12-31T06:19:48.370967Z",
     "iopub.status.idle": "2025-12-31T06:20:06.806648Z",
     "shell.execute_reply": "2025-12-31T06:20:06.804294Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "shopt -s extglob\n",
    "rm -rf embeddings/!(README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fc31d9",
   "metadata": {},
   "source": [
    "## HSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57407aeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T06:20:06.812406Z",
     "iopub.status.busy": "2025-12-31T06:20:06.811843Z",
     "iopub.status.idle": "2025-12-31T06:20:07.210836Z",
     "shell.execute_reply": "2025-12-31T06:20:07.208128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 31 06:20:06 UTC 2025\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13G\t.\r\n"
     ]
    }
   ],
   "source": [
    "!date\n",
    "!du -hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "436dd561",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T06:20:07.217278Z",
     "iopub.status.busy": "2025-12-31T06:20:07.216647Z",
     "iopub.status.idle": "2025-12-31T06:52:56.529617Z",
     "shell.execute_reply": "2025-12-31T06:52:56.526510Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: ground, data num: 50000\n",
      "ground. Complete 0/5000 (0.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 0.41822218894958496s. Overlong: 0\n",
      "ground. Complete 200/5000 (4.0%). Time used: 6.300006866455078"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s. Overlong: 36\n",
      "ground. Complete 400/5000 (8.0%). Time used: 11.646297693252563s. Overlong: 52\n",
      "groun"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d. Complete 600/5000 (12.0%). Time used: 16.87310266494751s. Overlong: 64\n",
      "ground. Complete 800/5000 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16.0%). Time used: 22.268430471420288s. Overlong: 79\n",
      "ground. Complete 1000/5000 (20.0%). Time used:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 27.490957260131836s. Overlong: 90\n",
      "ground. Complete 1200/5000 (24.0%). Time used: 32.73361420631409s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Overlong: 106\n",
      "ground. Complete 1400/5000 (28.0%). Time used: 37.95859503746033s. Overlong: 120\n",
      "gro"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "und. Complete 1600/5000 (32.0%). Time used: 43.24884486198425s. Overlong: 138\n",
      "ground. Complete 1800/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 (36.0%). Time used: 48.35261416435242s. Overlong: 150\n",
      "ground. Complete 2000/5000 (40.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 53.569910526275635s. Overlong: 166\n",
      "ground. Complete 2200/5000 (44.0%). Time used: 58.838353872"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299194s. Overlong: 176\n",
      "ground. Complete 2400/5000 (48.0%). Time used: 63.871649503707886s. Overlong:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 196\n",
      "ground. Complete 2600/5000 (52.0%). Time used: 69.21919274330139s. Overlong: 217\n",
      "ground. Comple"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "te 2800/5000 (56.0%). Time used: 74.27466106414795s. Overlong: 230\n",
      "ground. Complete 3000/5000 (60.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "). Time used: 79.56489539146423s. Overlong: 249\n",
      "ground. Complete 3200/5000 (64.0%). Time used: 84.97"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "681188583374s. Overlong: 268\n",
      "ground. Complete 3400/5000 (68.0%). Time used: 90.31528854370117s. Over"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long: 294\n",
      "ground. Complete 3600/5000 (72.0%). Time used: 95.6120445728302s. Overlong: 304\n",
      "ground. Co"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mplete 3800/5000 (76.0%). Time used: 100.94791650772095s. Overlong: 318\n",
      "ground. Complete 4000/5000 ("
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.0%). Time used: 106.1926851272583s. Overlong: 336\n",
      "ground. Complete 4200/5000 (84.0%). Time used: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111.24701642990112s. Overlong: 354\n",
      "ground. Complete 4400/5000 (88.0%). Time used: 116.65042924880981"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s. Overlong: 386\n",
      "ground. Complete 4600/5000 (92.0%). Time used: 122.07153367996216s. Overlong: 411\n",
      "g"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round. Complete 4800/5000 (96.0%). Time used: 127.29980206489563s. Overlong: 432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: ground_task2, data num: 50000\n",
      "ground_task2. Complete 0/5000 (0."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 0.4129924774169922s. Overlong: 0\n",
      "ground_task2. Complete 200/5000 (4.0%). Time used: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.113121509552002s. Overlong: 4\n",
      "ground_task2. Complete 400/5000 (8.0%). Time used: 9.639907121658325"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s. Overlong: 5\n",
      "ground_task2. Complete 600/5000 (12.0%). Time used: 13.901794195175171s. Overlong: 11"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ground_task2. Complete 800/5000 (16.0%). Time used: 18.251179695129395s. Overlong: 14\n",
      "ground_task2."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Complete 1000/5000 (20.0%). Time used: 22.499521017074585s. Overlong: 16\n",
      "ground_task2. Complete 120"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (24.0%). Time used: 26.718454360961914s. Overlong: 18\n",
      "ground_task2. Complete 1400/5000 (28.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "). Time used: 31.037989139556885s. Overlong: 19\n",
      "ground_task2. Complete 1600/5000 (32.0%). Time used:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 35.35886025428772s. Overlong: 21\n",
      "ground_task2. Complete 1800/5000 (36.0%). Time used: 39.6464235782"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6233s. Overlong: 23\n",
      "ground_task2. Complete 2000/5000 (40.0%). Time used: 43.916139125823975s. Overlo"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng: 25\n",
      "ground_task2. Complete 2200/5000 (44.0%). Time used: 48.14712452888489s. Overlong: 26\n",
      "ground_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task2. Complete 2400/5000 (48.0%). Time used: 52.32264995574951s. Overlong: 27\n",
      "ground_task2. Complet"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 2600/5000 (52.0%). Time used: 56.655712842941284s. Overlong: 29\n",
      "ground_task2. Complete 2800/5000 ("
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.0%). Time used: 60.968098163604736s. Overlong: 31\n",
      "ground_task2. Complete 3000/5000 (60.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 65.2011489868164s. Overlong: 33\n",
      "ground_task2. Complete 3200/5000 (64.0%). Time used: 69.627733"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70742798s. Overlong: 37\n",
      "ground_task2. Complete 3400/5000 (68.0%). Time used: 73.90341329574585s. Ove"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rlong: 39\n",
      "ground_task2. Complete 3600/5000 (72.0%). Time used: 78.37152075767517s. Overlong: 44\n",
      "grou"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nd_task2. Complete 3800/5000 (76.0%). Time used: 82.59667825698853s. Overlong: 45\n",
      "ground_task2. Comp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lete 4000/5000 (80.0%). Time used: 86.89913964271545s. Overlong: 45\n",
      "ground_task2. Complete 4200/5000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (84.0%). Time used: 91.24813914299011s. Overlong: 46\n",
      "ground_task2. Complete 4400/5000 (88.0%). Time"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " used: 95.4905002117157s. Overlong: 48\n",
      "ground_task2. Complete 4600/5000 (92.0%). Time used: 99.78584"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "933280945s. Overlong: 48\n",
      "ground_task2. Complete 4800/5000 (96.0%). Time used: 104.3707287311554s. Ov"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erlong: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task1_prompt1, data num: 50000\n",
      "gpt_task1_prompt1. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.41144633293151855s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 200/5000 (4."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 5.62385368347168s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 400/5000 (8.0%). Time use"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 10.656436204910278s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 600/5000 (12.0%). Time used: 15.6815"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67668914795s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 800/5000 (16.0%). Time used: 20.78595876693725"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 1000/5000 (20.0%). Time used: 25.866365671157837s. Overl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ong: 0\n",
      "gpt_task1_prompt1. Complete 1200/5000 (24.0%). Time used: 31.039650201797485s. Overlong: 0\n",
      "gp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_task1_prompt1. Complete 1400/5000 (28.0%). Time used: 36.04404425621033s. Overlong: 0\n",
      "gpt_task1_pr"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ompt1. Complete 1600/5000 (32.0%). Time used: 41.101869344711304s. Overlong: 0\n",
      "gpt_task1_prompt1. Co"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mplete 1800/5000 (36.0%). Time used: 46.23018765449524s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 200"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (40.0%). Time used: 51.365257263183594s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 2200/5000 (4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0%). Time used: 56.508971214294434s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 2400/5000 (48.0%). Ti"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me used: 61.61475729942322s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 2600/5000 (52.0%). Time used: 6"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.84455013275146s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 2800/5000 (56.0%). Time used: 72.01858711"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242676s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 3000/5000 (60.0%). Time used: 77.22298550605774s. O"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verlong: 0\n",
      "gpt_task1_prompt1. Complete 3200/5000 (64.0%). Time used: 82.2364239692688s. Overlong: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_task1_prompt1. Complete 3400/5000 (68.0%). Time used: 87.21080875396729s. Overlong: 1\n",
      "gpt_task1_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt1. Complete 3600/5000 (72.0%). Time used: 92.3130874633789s. Overlong: 1\n",
      "gpt_task1_prompt1. Co"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mplete 3800/5000 (76.0%). Time used: 97.3283326625824s. Overlong: 2\n",
      "gpt_task1_prompt1. Complete 4000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/5000 (80.0%). Time used: 102.48107409477234s. Overlong: 2\n",
      "gpt_task1_prompt1. Complete 4200/5000 (84"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 107.63372111320496s. Overlong: 2\n",
      "gpt_task1_prompt1. Complete 4400/5000 (88.0%). Tim"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e used: 112.51103234291077s. Overlong: 2\n",
      "gpt_task1_prompt1. Complete 4600/5000 (92.0%). Time used: 1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.66245079040527s. Overlong: 2\n",
      "gpt_task1_prompt1. Complete 4800/5000 (96.0%). Time used: 122.668971"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77696228s. Overlong: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task1_prompt2, data num: 50000\n",
      "gpt_task1_prompt2. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.4128227233886719s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 200/5000 (4.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 5.343507766723633s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 400/5000 (8.0%). Time use"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 10.07991647720337s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 600/5000 (12.0%). Time used: 15.02092"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3614501953s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 800/5000 (16.0%). Time used: 19.946305990219116"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 1000/5000 (20.0%). Time used: 24.793237447738647s. Overlo"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng: 0\n",
      "gpt_task1_prompt2. Complete 1200/5000 (24.0%). Time used: 29.670499801635742s. Overlong: 1\n",
      "gpt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_task1_prompt2. Complete 1400/5000 (28.0%). Time used: 34.49991321563721s. Overlong: 1\n",
      "gpt_task1_pro"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpt2. Complete 1600/5000 (32.0%). Time used: 39.282673597335815s. Overlong: 1\n",
      "gpt_task1_prompt2. Com"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plete 1800/5000 (36.0%). Time used: 44.0831823348999s. Overlong: 1\n",
      "gpt_task1_prompt2. Complete 2000/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 (40.0%). Time used: 48.94015192985535s. Overlong: 1\n",
      "gpt_task1_prompt2. Complete 2200/5000 (44.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 53.768938064575195s. Overlong: 1\n",
      "gpt_task1_prompt2. Complete 2400/5000 (48.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 58.666154623031616s. Overlong: 1\n",
      "gpt_task1_prompt2. Complete 2600/5000 (52.0%). Time used: 63."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453319787979126s. Overlong: 1\n",
      "gpt_task1_prompt2. Complete 2800/5000 (56.0%). Time used: 68.154804706"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57349s. Overlong: 1\n",
      "gpt_task1_prompt2. Complete 3000/5000 (60.0%). Time used: 73.06128191947937s. Ov"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erlong: 3\n",
      "gpt_task1_prompt2. Complete 3200/5000 (64.0%). Time used: 77.77583575248718s. Overlong: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_task1_prompt2. Complete 3400/5000 (68.0%). Time used: 82.5005612373352s. Overlong: 3\n",
      "gpt_task1_p"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rompt2. Complete 3600/5000 (72.0%). Time used: 87.31950807571411s. Overlong: 4\n",
      "gpt_task1_prompt2. Co"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mplete 3800/5000 (76.0%). Time used: 92.15221166610718s. Overlong: 5\n",
      "gpt_task1_prompt2. Complete 400"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (80.0%). Time used: 96.87805509567261s. Overlong: 5\n",
      "gpt_task1_prompt2. Complete 4200/5000 (84"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 101.57877087593079s. Overlong: 5\n",
      "gpt_task1_prompt2. Complete 4400/5000 (88.0%). Tim"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e used: 106.2554075717926s. Overlong: 5\n",
      "gpt_task1_prompt2. Complete 4600/5000 (92.0%). Time used: 11"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.28094840049744s. Overlong: 6\n",
      "gpt_task1_prompt2. Complete 4800/5000 (96.0%). Time used: 116.1449644"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5655823s. Overlong: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task1_prompt3, data num: 50000\n",
      "gpt_task1_prompt3. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.420623779296875s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 200/5000 (4.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "). Time used: 5.934104919433594s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 400/5000 (8.0%). Time used"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 11.297044038772583s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 600/5000 (12.0%). Time used: 16.68591"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "570854187s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 800/5000 (16.0%). Time used: 22.007060766220093s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Overlong: 0\n",
      "gpt_task1_prompt3. Complete 1000/5000 (20.0%). Time used: 27.474369764328003s. Overlon"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g: 0\n",
      "gpt_task1_prompt3. Complete 1200/5000 (24.0%). Time used: 32.84857106208801s. Overlong: 0\n",
      "gpt_t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ask1_prompt3. Complete 1400/5000 (28.0%). Time used: 38.21440625190735s. Overlong: 0\n",
      "gpt_task1_promp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3. Complete 1600/5000 (32.0%). Time used: 43.55726885795593s. Overlong: 0\n",
      "gpt_task1_prompt3. Comple"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "te 1800/5000 (36.0%). Time used: 49.014238595962524s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 2000/5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000 (40.0%). Time used: 54.447038888931274s. Overlong: 1\n",
      "gpt_task1_prompt3. Complete 2200/5000 (44.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 59.757617473602295s. Overlong: 1\n",
      "gpt_task1_prompt3. Complete 2400/5000 (48.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 65.15216708183289s. Overlong: 2\n",
      "gpt_task1_prompt3. Complete 2600/5000 (52.0%). Time used: 70.4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7704482078552s. Overlong: 2\n",
      "gpt_task1_prompt3. Complete 2800/5000 (56.0%). Time used: 75.95248675346"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375s. Overlong: 2\n",
      "gpt_task1_prompt3. Complete 3000/5000 (60.0%). Time used: 81.37696933746338s. Over"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long: 3\n",
      "gpt_task1_prompt3. Complete 3200/5000 (64.0%). Time used: 86.83526539802551s. Overlong: 3\n",
      "gp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_task1_prompt3. Complete 3400/5000 (68.0%). Time used: 92.21213173866272s. Overlong: 3\n",
      "gpt_task1_pr"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ompt3. Complete 3600/5000 (72.0%). Time used: 97.54646277427673s. Overlong: 3\n",
      "gpt_task1_prompt3. Com"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plete 3800/5000 (76.0%). Time used: 102.83925724029541s. Overlong: 3\n",
      "gpt_task1_prompt3. Complete 400"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (80.0%). Time used: 108.22882151603699s. Overlong: 3\n",
      "gpt_task1_prompt3. Complete 4200/5000 (8"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0%). Time used: 113.59584593772888s. Overlong: 3\n",
      "gpt_task1_prompt3. Complete 4400/5000 (88.0%). Ti"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me used: 118.97269868850708s. Overlong: 3\n",
      "gpt_task1_prompt3. Complete 4600/5000 (92.0%). Time used: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124.36760306358337s. Overlong: 3\n",
      "gpt_task1_prompt3. Complete 4800/5000 (96.0%). Time used: 129.75838"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18435669s. Overlong: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task1_prompt4, data num: 50000\n",
      "gpt_task1_prompt4. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.43866896629333496s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 200/5000 (4."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 6.021502494812012s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 400/5000 (8.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 11.666019678115845s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 600/5000 (12.0%). Time used: 17.411"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "585569381714s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 800/5000 (16.0%). Time used: 23.0260801315307"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 1000/5000 (20.0%). Time used: 28.625555515289307s. Overl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ong: 0\n",
      "gpt_task1_prompt4. Complete 1200/5000 (24.0%). Time used: 34.26713943481445s. Overlong: 0\n",
      "gpt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_task1_prompt4. Complete 1400/5000 (28.0%). Time used: 39.9722535610199s. Overlong: 0\n",
      "gpt_task1_prom"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt4. Complete 1600/5000 (32.0%). Time used: 45.64731574058533s. Overlong: 1\n",
      "gpt_task1_prompt4. Compl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ete 1800/5000 (36.0%). Time used: 51.30210089683533s. Overlong: 2\n",
      "gpt_task1_prompt4. Complete 2000/5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000 (40.0%). Time used: 56.801716804504395s. Overlong: 2\n",
      "gpt_task1_prompt4. Complete 2200/5000 (44.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 62.35755681991577s. Overlong: 2\n",
      "gpt_task1_prompt4. Complete 2400/5000 (48.0%). Time u"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sed: 67.99718499183655s. Overlong: 2\n",
      "gpt_task1_prompt4. Complete 2600/5000 (52.0%). Time used: 73.70"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9627866745s. Overlong: 2\n",
      "gpt_task1_prompt4. Complete 2800/5000 (56.0%). Time used: 79.34292674064636"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s. Overlong: 2\n",
      "gpt_task1_prompt4. Complete 3000/5000 (60.0%). Time used: 84.86019206047058s. Overlon"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g: 2\n",
      "gpt_task1_prompt4. Complete 3200/5000 (64.0%). Time used: 90.4538950920105s. Overlong: 2\n",
      "gpt_ta"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk1_prompt4. Complete 3400/5000 (68.0%). Time used: 95.91881513595581s. Overlong: 2\n",
      "gpt_task1_prompt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Complete 3600/5000 (72.0%). Time used: 101.42551398277283s. Overlong: 2\n",
      "gpt_task1_prompt4. Comple"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "te 3800/5000 (76.0%). Time used: 106.89991879463196s. Overlong: 2\n",
      "gpt_task1_prompt4. Complete 4000/5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000 (80.0%). Time used: 112.36196398735046s. Overlong: 2\n",
      "gpt_task1_prompt4. Complete 4200/5000 (84.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 117.81222701072693s. Overlong: 2\n",
      "gpt_task1_prompt4. Complete 4400/5000 (88.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 123.20690274238586s. Overlong: 2\n",
      "gpt_task1_prompt4. Complete 4600/5000 (92.0%). Time used: 128"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".59616947174072s. Overlong: 2\n",
      "gpt_task1_prompt4. Complete 4800/5000 (96.0%). Time used: 134.16342663"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764954s. Overlong: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. U"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se hydra.compose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task2_prompt1, data num: 50000\n",
      "gpt_task2_prompt1. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.41304445266723633s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 200/5000 (4."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 5.761606454849243s. Overlong: 11\n",
      "gpt_task2_prompt1. Complete 400/5000 (8.0%). Time u"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sed: 10.879387140274048s. Overlong: 17\n",
      "gpt_task2_prompt1. Complete 600/5000 (12.0%). Time used: 15.6"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47183656692505s. Overlong: 22\n",
      "gpt_task2_prompt1. Complete 800/5000 (16.0%). Time used: 20.5041894912"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71973s. Overlong: 31\n",
      "gpt_task2_prompt1. Complete 1000/5000 (20.0%). Time used: 25.261223316192627s. "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlong: 33\n",
      "gpt_task2_prompt1. Complete 1200/5000 (24.0%). Time used: 30.07480216026306s. Overlong:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 37\n",
      "gpt_task2_prompt1. Complete 1400/5000 (28.0%). Time used: 34.96984028816223s. Overlong: 42\n",
      "gpt_t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ask2_prompt1. Complete 1600/5000 (32.0%). Time used: 39.9885675907135s. Overlong: 46\n",
      "gpt_task2_promp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1. Complete 1800/5000 (36.0%). Time used: 44.856032848358154s. Overlong: 48\n",
      "gpt_task2_prompt1. Comp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lete 2000/5000 (40.0%). Time used: 49.67634963989258s. Overlong: 51\n",
      "gpt_task2_prompt1. Complete 2200"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/5000 (44.0%). Time used: 54.4660062789917s. Overlong: 52\n",
      "gpt_task2_prompt1. Complete 2400/5000 (48."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 59.218955993652344s. Overlong: 55\n",
      "gpt_task2_prompt1. Complete 2600/5000 (52.0%). Tim"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e used: 64.02967238426208s. Overlong: 64\n",
      "gpt_task2_prompt1. Complete 2800/5000 (56.0%). Time used: 6"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.9479558467865s. Overlong: 72\n",
      "gpt_task2_prompt1. Complete 3000/5000 (60.0%). Time used: 73.78231453"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "895569s. Overlong: 79\n",
      "gpt_task2_prompt1. Complete 3200/5000 (64.0%). Time used: 78.64190316200256s. "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlong: 89\n",
      "gpt_task2_prompt1. Complete 3400/5000 (68.0%). Time used: 83.6572265625s. Overlong: 97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_task2_prompt1. Complete 3600/5000 (72.0%). Time used: 88.58900690078735s. Overlong: 103\n",
      "gpt_task"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_prompt1. Complete 3800/5000 (76.0%). Time used: 93.40160989761353s. Overlong: 107\n",
      "gpt_task2_prompt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Complete 4000/5000 (80.0%). Time used: 98.34815502166748s. Overlong: 109\n",
      "gpt_task2_prompt1. Compl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ete 4200/5000 (84.0%). Time used: 103.23756098747253s. Overlong: 112\n",
      "gpt_task2_prompt1. Complete 440"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (88.0%). Time used: 108.0349280834198s. Overlong: 117\n",
      "gpt_task2_prompt1. Complete 4600/5000 ("
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.0%). Time used: 113.0261881351471s. Overlong: 136\n",
      "gpt_task2_prompt1. Complete 4800/5000 (96.0%). "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used: 118.14023065567017s. Overlong: 143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task2_prompt2, data num: 50000\n",
      "gpt_task2_prompt2. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.41690516471862793s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 200/5000 (4."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 5.765982627868652s. Overlong: 3\n",
      "gpt_task2_prompt2. Complete 400/5000 (8.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 10.913437366485596s. Overlong: 6\n",
      "gpt_task2_prompt2. Complete 600/5000 (12.0%). Time used: 15.734"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01665687561s. Overlong: 7\n",
      "gpt_task2_prompt2. Complete 800/5000 (16.0%). Time used: 20.61129736900329"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6s. Overlong: 8\n",
      "gpt_task2_prompt2. Complete 1000/5000 (20.0%). Time used: 25.372201204299927s. Overl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ong: 9\n",
      "gpt_task2_prompt2. Complete 1200/5000 (24.0%). Time used: 30.110445261001587s. Overlong: 11\n",
      "g"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt_task2_prompt2. Complete 1400/5000 (28.0%). Time used: 34.993802547454834s. Overlong: 11\n",
      "gpt_task2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_prompt2. Complete 1600/5000 (32.0%). Time used: 39.93051362037659s. Overlong: 16\n",
      "gpt_task2_prompt2."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Complete 1800/5000 (36.0%). Time used: 44.77650046348572s. Overlong: 17\n",
      "gpt_task2_prompt2. Complete"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2000/5000 (40.0%). Time used: 49.64746022224426s. Overlong: 19\n",
      "gpt_task2_prompt2. Complete 2200/500"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (44.0%). Time used: 54.38914632797241s. Overlong: 20\n",
      "gpt_task2_prompt2. Complete 2400/5000 (48.0%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Time used: 59.06305813789368s. Overlong: 21\n",
      "gpt_task2_prompt2. Complete 2600/5000 (52.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 63.880451679229736s. Overlong: 24\n",
      "gpt_task2_prompt2. Complete 2800/5000 (56.0%). Time used: 68.7"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5179243087769s. Overlong: 29\n",
      "gpt_task2_prompt2. Complete 3000/5000 (60.0%). Time used: 73.4345049858"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0933s. Overlong: 32\n",
      "gpt_task2_prompt2. Complete 3200/5000 (64.0%). Time used: 78.347158908844s. Over"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long: 36\n",
      "gpt_task2_prompt2. Complete 3400/5000 (68.0%). Time used: 83.15407133102417s. Overlong: 39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_task2_prompt2. Complete 3600/5000 (72.0%). Time used: 88.13802099227905s. Overlong: 43\n",
      "gpt_task2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_prompt2. Complete 3800/5000 (76.0%). Time used: 92.93538665771484s. Overlong: 44\n",
      "gpt_task2_prompt2."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Complete 4000/5000 (80.0%). Time used: 97.81876468658447s. Overlong: 45\n",
      "gpt_task2_prompt2. Complete"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4200/5000 (84.0%). Time used: 102.74237871170044s. Overlong: 45\n",
      "gpt_task2_prompt2. Complete 4400/50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 (88.0%). Time used: 107.53592419624329s. Overlong: 50\n",
      "gpt_task2_prompt2. Complete 4600/5000 (92.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 112.2248387336731s. Overlong: 52\n",
      "gpt_task2_prompt2. Complete 4800/5000 (96.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 117.28693318367004s. Overlong: 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task2_prompt3, data num: 50000\n",
      "gpt_task2_prompt3. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.4021153450012207s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 200/5000 (4.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 5.5619659423828125s. Overlong: 4\n",
      "gpt_task2_prompt3. Complete 400/5000 (8.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 10.561884641647339s. Overlong: 6\n",
      "gpt_task2_prompt3. Complete 600/5000 (12.0%). Time used: 15.212"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994813919067s. Overlong: 7\n",
      "gpt_task2_prompt3. Complete 800/5000 (16.0%). Time used: 19.8892776966094"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97s. Overlong: 12\n",
      "gpt_task2_prompt3. Complete 1000/5000 (20.0%). Time used: 24.472289085388184s. Ove"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rlong: 13\n",
      "gpt_task2_prompt3. Complete 1200/5000 (24.0%). Time used: 29.037262201309204s. Overlong: 1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "gpt_task2_prompt3. Complete 1400/5000 (28.0%). Time used: 33.770721435546875s. Overlong: 16\n",
      "gpt_ta"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk2_prompt3. Complete 1600/5000 (32.0%). Time used: 38.591410875320435s. Overlong: 20\n",
      "gpt_task2_prom"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt3. Complete 1800/5000 (36.0%). Time used: 43.358482122421265s. Overlong: 21\n",
      "gpt_task2_prompt3. Com"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plete 2000/5000 (40.0%). Time used: 48.062512159347534s. Overlong: 24\n",
      "gpt_task2_prompt3. Complete 22"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00/5000 (44.0%). Time used: 52.602381229400635s. Overlong: 25\n",
      "gpt_task2_prompt3. Complete 2400/5000 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48.0%). Time used: 57.138530254364014s. Overlong: 27\n",
      "gpt_task2_prompt3. Complete 2600/5000 (52.0%)."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Time used: 61.87669134140015s. Overlong: 32\n",
      "gpt_task2_prompt3. Complete 2800/5000 (56.0%). Time use"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 66.58338665962219s. Overlong: 37\n",
      "gpt_task2_prompt3. Complete 3000/5000 (60.0%). Time used: 71.183"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30478668213s. Overlong: 41\n",
      "gpt_task2_prompt3. Complete 3200/5000 (64.0%). Time used: 75.864297151565"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55s. Overlong: 44\n",
      "gpt_task2_prompt3. Complete 3400/5000 (68.0%). Time used: 80.5402443408966s. Overl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ong: 47\n",
      "gpt_task2_prompt3. Complete 3600/5000 (72.0%). Time used: 85.4174473285675s. Overlong: 50\n",
      "gp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_task2_prompt3. Complete 3800/5000 (76.0%). Time used: 90.03333067893982s. Overlong: 53\n",
      "gpt_task2_p"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rompt3. Complete 4000/5000 (80.0%). Time used: 94.79949641227722s. Overlong: 54\n",
      "gpt_task2_prompt3. C"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omplete 4200/5000 (84.0%). Time used: 99.56093716621399s. Overlong: 54\n",
      "gpt_task2_prompt3. Complete 4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/5000 (88.0%). Time used: 104.15705013275146s. Overlong: 57\n",
      "gpt_task2_prompt3. Complete 4600/5000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (92.0%). Time used: 108.88863325119019s. Overlong: 60\n",
      "gpt_task2_prompt3. Complete 4800/5000 (96.0%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Time used: 113.97397971153259s. Overlong: 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task2_prompt4, data num: 50000\n",
      "gpt_task2_prompt4. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.40902185440063477s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 200/5000 (4."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 5.61726713180542s. Overlong: 1\n",
      "gpt_task2_prompt4. Complete 400/5000 (8.0%). Time use"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 10.857694149017334s. Overlong: 1\n",
      "gpt_task2_prompt4. Complete 600/5000 (12.0%). Time used: 15.7052"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61945724487s. Overlong: 2\n",
      "gpt_task2_prompt4. Complete 800/5000 (16.0%). Time used: 20.60914778709411"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6s. Overlong: 4\n",
      "gpt_task2_prompt4. Complete 1000/5000 (20.0%). Time used: 25.270854473114014s. Overl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ong: 9\n",
      "gpt_task2_prompt4. Complete 1200/5000 (24.0%). Time used: 29.968560218811035s. Overlong: 11\n",
      "g"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt_task2_prompt4. Complete 1400/5000 (28.0%). Time used: 34.82892942428589s. Overlong: 13\n",
      "gpt_task2_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt4. Complete 1600/5000 (32.0%). Time used: 39.715630531311035s. Overlong: 15\n",
      "gpt_task2_prompt4."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Complete 1800/5000 (36.0%). Time used: 44.65987706184387s. Overlong: 17\n",
      "gpt_task2_prompt4. Complete"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2000/5000 (40.0%). Time used: 49.42200803756714s. Overlong: 20\n",
      "gpt_task2_prompt4. Complete 2200/500"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (44.0%). Time used: 54.226781129837036s. Overlong: 22\n",
      "gpt_task2_prompt4. Complete 2400/5000 (48.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "). Time used: 59.05051279067993s. Overlong: 23\n",
      "gpt_task2_prompt4. Complete 2600/5000 (52.0%). Time u"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sed: 63.92385673522949s. Overlong: 26\n",
      "gpt_task2_prompt4. Complete 2800/5000 (56.0%). Time used: 68.7"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5374794006348s. Overlong: 29\n",
      "gpt_task2_prompt4. Complete 3000/5000 (60.0%). Time used: 73.5104568004"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6082s. Overlong: 31\n",
      "gpt_task2_prompt4. Complete 3200/5000 (64.0%). Time used: 78.38970398902893s. Ov"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erlong: 36\n",
      "gpt_task2_prompt4. Complete 3400/5000 (68.0%). Time used: 83.26363945007324s. Overlong: 3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "gpt_task2_prompt4. Complete 3600/5000 (72.0%). Time used: 88.2789843082428s. Overlong: 42\n",
      "gpt_task"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_prompt4. Complete 3800/5000 (76.0%). Time used: 93.1047625541687s. Overlong: 42\n",
      "gpt_task2_prompt4."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Complete 4000/5000 (80.0%). Time used: 98.02725481987s. Overlong: 42\n",
      "gpt_task2_prompt4. Complete 42"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00/5000 (84.0%). Time used: 102.98623490333557s. Overlong: 43\n",
      "gpt_task2_prompt4. Complete 4400/5000 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88.0%). Time used: 107.7467954158783s. Overlong: 49\n",
      "gpt_task2_prompt4. Complete 4600/5000 (92.0%). "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used: 112.50281834602356s. Overlong: 52\n",
      "gpt_task2_prompt4. Complete 4800/5000 (96.0%). Time use"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 117.63154435157776s. Overlong: 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task3_prompt1, data num: 50000\n",
      "gpt_task3_prompt1. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.43692708015441895s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 200/5000 (4."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 5.495603322982788s. Overlong: 7\n",
      "gpt_task3_prompt1. Complete 400/5000 (8.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 10.540793180465698s. Overlong: 12\n",
      "gpt_task3_prompt1. Complete 600/5000 (12.0%). Time used: 15.32"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0924520492554s. Overlong: 13\n",
      "gpt_task3_prompt1. Complete 800/5000 (16.0%). Time used: 20.26316142082"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2144s. Overlong: 16\n",
      "gpt_task3_prompt1. Complete 1000/5000 (20.0%). Time used: 25.08765721321106s. Ov"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erlong: 17\n",
      "gpt_task3_prompt1. Complete 1200/5000 (24.0%). Time used: 29.888795852661133s. Overlong: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "gpt_task3_prompt1. Complete 1400/5000 (28.0%). Time used: 34.800360441207886s. Overlong: 20\n",
      "gpt_t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ask3_prompt1. Complete 1600/5000 (32.0%). Time used: 39.651611328125s. Overlong: 25\n",
      "gpt_task3_prompt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Complete 1800/5000 (36.0%). Time used: 44.511741399765015s. Overlong: 28\n",
      "gpt_task3_prompt1. Compl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ete 2000/5000 (40.0%). Time used: 49.36877679824829s. Overlong: 28\n",
      "gpt_task3_prompt1. Complete 2200/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 (44.0%). Time used: 54.16689586639404s. Overlong: 30\n",
      "gpt_task3_prompt1. Complete 2400/5000 (48."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 58.847896337509155s. Overlong: 30\n",
      "gpt_task3_prompt1. Complete 2600/5000 (52.0%). Tim"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e used: 63.70413517951965s. Overlong: 34\n",
      "gpt_task3_prompt1. Complete 2800/5000 (56.0%). Time used: 6"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.44522404670715s. Overlong: 36\n",
      "gpt_task3_prompt1. Complete 3000/5000 (60.0%). Time used: 73.1078510"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2844238s. Overlong: 39\n",
      "gpt_task3_prompt1. Complete 3200/5000 (64.0%). Time used: 77.9911584854126s. "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlong: 42\n",
      "gpt_task3_prompt1. Complete 3400/5000 (68.0%). Time used: 82.77446007728577s. Overlong:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44\n",
      "gpt_task3_prompt1. Complete 3600/5000 (72.0%). Time used: 87.57492566108704s. Overlong: 54\n",
      "gpt_t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ask3_prompt1. Complete 3800/5000 (76.0%). Time used: 92.3954930305481s. Overlong: 58\n",
      "gpt_task3_promp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1. Complete 4000/5000 (80.0%). Time used: 97.30215430259705s. Overlong: 62\n",
      "gpt_task3_prompt1. Compl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ete 4200/5000 (84.0%). Time used: 102.11793231964111s. Overlong: 63\n",
      "gpt_task3_prompt1. Complete 4400"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/5000 (88.0%). Time used: 106.96470069885254s. Overlong: 69\n",
      "gpt_task3_prompt1. Complete 4600/5000 (9"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0%). Time used: 111.71732592582703s. Overlong: 72\n",
      "gpt_task3_prompt1. Complete 4800/5000 (96.0%). T"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ime used: 116.69293117523193s. Overlong: 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task3_prompt2, data num: 50000\n",
      "gpt_task3_prompt2. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.41476917266845703s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 200/5000 (4."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 5.87186336517334s. Overlong: 10\n",
      "gpt_task3_prompt2. Complete 400/5000 (8.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 11.095176219940186s. Overlong: 23\n",
      "gpt_task3_prompt2. Complete 600/5000 (12.0%). Time used: 16.04"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220724105835s. Overlong: 29\n",
      "gpt_task3_prompt2. Complete 800/5000 (16.0%). Time used: 21.300303697586"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06s. Overlong: 36\n",
      "gpt_task3_prompt2. Complete 1000/5000 (20.0%). Time used: 26.327510595321655s. Ove"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rlong: 38\n",
      "gpt_task3_prompt2. Complete 1200/5000 (24.0%). Time used: 31.378758430480957s. Overlong: 4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "gpt_task3_prompt2. Complete 1400/5000 (28.0%). Time used: 36.46771764755249s. Overlong: 51\n",
      "gpt_tas"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k3_prompt2. Complete 1600/5000 (32.0%). Time used: 41.57623362541199s. Overlong: 60\n",
      "gpt_task3_prompt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Complete 1800/5000 (36.0%). Time used: 46.57016086578369s. Overlong: 66\n",
      "gpt_task3_prompt2. Comple"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "te 2000/5000 (40.0%). Time used: 51.73234295845032s. Overlong: 77\n",
      "gpt_task3_prompt2. Complete 2200/5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000 (44.0%). Time used: 56.83926963806152s. Overlong: 80\n",
      "gpt_task3_prompt2. Complete 2400/5000 (48.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 61.742189168930054s. Overlong: 87\n",
      "gpt_task3_prompt2. Complete 2600/5000 (52.0%). Time"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " used: 66.75731587409973s. Overlong: 91\n",
      "gpt_task3_prompt2. Complete 2800/5000 (56.0%). Time used: 71"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".61454701423645s. Overlong: 99\n",
      "gpt_task3_prompt2. Complete 3000/5000 (60.0%). Time used: 76.57361483"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573914s. Overlong: 104\n",
      "gpt_task3_prompt2. Complete 3200/5000 (64.0%). Time used: 81.46291518211365s."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Overlong: 111\n",
      "gpt_task3_prompt2. Complete 3400/5000 (68.0%). Time used: 86.65292024612427s. Overlon"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g: 126\n",
      "gpt_task3_prompt2. Complete 3600/5000 (72.0%). Time used: 91.80971240997314s. Overlong: 134\n",
      "g"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt_task3_prompt2. Complete 3800/5000 (76.0%). Time used: 96.88052606582642s. Overlong: 138\n",
      "gpt_task3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_prompt2. Complete 4000/5000 (80.0%). Time used: 102.06725907325745s. Overlong: 143\n",
      "gpt_task3_prompt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Complete 4200/5000 (84.0%). Time used: 107.1387050151825s. Overlong: 152\n",
      "gpt_task3_prompt2. Compl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ete 4400/5000 (88.0%). Time used: 112.02344083786011s. Overlong: 161\n",
      "gpt_task3_prompt2. Complete 460"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (92.0%). Time used: 117.1043016910553s. Overlong: 173\n",
      "gpt_task3_prompt2. Complete 4800/5000 ("
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.0%). Time used: 122.13457894325256s. Overlong: 186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task3_prompt3, data num: 50000\n",
      "gpt_task3_prompt3. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.41388440132141113s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete 200/5000 (4."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 5.851221323013306s. Overlong: 14\n",
      "gpt_task3_prompt3. Complete 400/5000 (8.0%). Time u"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sed: 11.026718854904175s. Overlong: 25\n",
      "gpt_task3_prompt3. Complete 600/5000 (12.0%). Time used: 15.9"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25524711608887s. Overlong: 26\n",
      "gpt_task3_prompt3. Complete 800/5000 (16.0%). Time used: 21.0093832015"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9912s. Overlong: 31\n",
      "gpt_task3_prompt3. Complete 1000/5000 (20.0%). Time used: 25.880159378051758s. O"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verlong: 33\n",
      "gpt_task3_prompt3. Complete 1200/5000 (24.0%). Time used: 30.871805429458618s. Overlong:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 36\n",
      "gpt_task3_prompt3. Complete 1400/5000 (28.0%). Time used: 35.9315459728241s. Overlong: 38\n",
      "gpt_ta"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk3_prompt3. Complete 1600/5000 (32.0%). Time used: 40.90024971961975s. Overlong: 45\n",
      "gpt_task3_promp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3. Complete 1800/5000 (36.0%). Time used: 45.853132486343384s. Overlong: 53\n",
      "gpt_task3_prompt3. Comp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lete 2000/5000 (40.0%). Time used: 50.91711640357971s. Overlong: 58\n",
      "gpt_task3_prompt3. Complete 2200"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/5000 (44.0%). Time used: 55.8381712436676s. Overlong: 62\n",
      "gpt_task3_prompt3. Complete 2400/5000 (48."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 60.77545356750488s. Overlong: 64\n",
      "gpt_task3_prompt3. Complete 2600/5000 (52.0%). Time"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " used: 65.71020603179932s. Overlong: 71\n",
      "gpt_task3_prompt3. Complete 2800/5000 (56.0%). Time used: 70"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".50539469718933s. Overlong: 79\n",
      "gpt_task3_prompt3. Complete 3000/5000 (60.0%). Time used: 75.40948438"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644409s. Overlong: 82\n",
      "gpt_task3_prompt3. Complete 3200/5000 (64.0%). Time used: 80.31353378295898s. "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlong: 87\n",
      "gpt_task3_prompt3. Complete 3400/5000 (68.0%). Time used: 85.40034651756287s. Overlong:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96\n",
      "gpt_task3_prompt3. Complete 3600/5000 (72.0%). Time used: 90.54924488067627s. Overlong: 105\n",
      "gpt_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task3_prompt3. Complete 3800/5000 (76.0%). Time used: 95.51505136489868s. Overlong: 108\n",
      "gpt_task3_pr"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ompt3. Complete 4000/5000 (80.0%). Time used: 100.69465970993042s. Overlong: 115\n",
      "gpt_task3_prompt3. "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete 4200/5000 (84.0%). Time used: 105.74346089363098s. Overlong: 118\n",
      "gpt_task3_prompt3. Complet"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 4400/5000 (88.0%). Time used: 110.6108512878418s. Overlong: 125\n",
      "gpt_task3_prompt3. Complete 4600/5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000 (92.0%). Time used: 115.66883301734924s. Overlong: 133\n",
      "gpt_task3_prompt3. Complete 4800/5000 (96"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 120.78480792045593s. Overlong: 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task3_prompt4, data num: 50000\n",
      "gpt_task3_prompt4. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 (0.0%). Time used: 0.43297791481018066s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 200/5000 (4."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 6.094333648681641s. Overlong: 18\n",
      "gpt_task3_prompt4. Complete 400/5000 (8.0%). Time u"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sed: 11.427719354629517s. Overlong: 28\n",
      "gpt_task3_prompt4. Complete 600/5000 (12.0%). Time used: 16.5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7758641242981s. Overlong: 39\n",
      "gpt_task3_prompt4. Complete 800/5000 (16.0%). Time used: 21.90919828414"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "917s. Overlong: 45\n",
      "gpt_task3_prompt4. Complete 1000/5000 (20.0%). Time used: 27.09282398223877s. Ove"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rlong: 48\n",
      "gpt_task3_prompt4. Complete 1200/5000 (24.0%). Time used: 32.4429087638855s. Overlong: 52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_task3_prompt4. Complete 1400/5000 (28.0%). Time used: 37.67742466926575s. Overlong: 57\n",
      "gpt_task3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_prompt4. Complete 1600/5000 (32.0%). Time used: 42.89083743095398s. Overlong: 63\n",
      "gpt_task3_prompt4."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Complete 1800/5000 (36.0%). Time used: 48.03106474876404s. Overlong: 69\n",
      "gpt_task3_prompt4. Complete"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2000/5000 (40.0%). Time used: 53.37998557090759s. Overlong: 79\n",
      "gpt_task3_prompt4. Complete 2200/500"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (44.0%). Time used: 58.51414227485657s. Overlong: 81\n",
      "gpt_task3_prompt4. Complete 2400/5000 (48.0%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Time used: 63.58659076690674s. Overlong: 89\n",
      "gpt_task3_prompt4. Complete 2600/5000 (52.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 68.71636533737183s. Overlong: 91\n",
      "gpt_task3_prompt4. Complete 2800/5000 (56.0%). Time used: 73.74"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306988716125s. Overlong: 97\n",
      "gpt_task3_prompt4. Complete 3000/5000 (60.0%). Time used: 78.87027168273"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "926s. Overlong: 103\n",
      "gpt_task3_prompt4. Complete 3200/5000 (64.0%). Time used: 83.89268183708191s. Ov"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erlong: 111\n",
      "gpt_task3_prompt4. Complete 3400/5000 (68.0%). Time used: 89.24462890625s. Overlong: 125"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gpt_task3_prompt4. Complete 3600/5000 (72.0%). Time used: 94.54259967803955s. Overlong: 132\n",
      "gpt_tas"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k3_prompt4. Complete 3800/5000 (76.0%). Time used: 99.70325255393982s. Overlong: 137\n",
      "gpt_task3_promp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t4. Complete 4000/5000 (80.0%). Time used: 104.94925475120544s. Overlong: 144\n",
      "gpt_task3_prompt4. Com"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plete 4200/5000 (84.0%). Time used: 110.21540832519531s. Overlong: 154\n",
      "gpt_task3_prompt4. Complete 4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/5000 (88.0%). Time used: 115.24716830253601s. Overlong: 164\n",
      "gpt_task3_prompt4. Complete 4600/500"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (92.0%). Time used: 120.59539842605591s. Overlong: 173\n",
      "gpt_task3_prompt4. Complete 4800/5000 (96.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 125.84765696525574s. Overlong: 182\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "\n",
    "python features.py HSS 1 1 --gpt 0 --number 5000\n",
    "python features.py HSS 2 1 --gpt 0 --number 5000\n",
    "\n",
    "python features.py HSS 1 1 --gpt 1 --number 5000\n",
    "python features.py HSS 1 2 --gpt 1 --number 5000\n",
    "python features.py HSS 1 3 --gpt 1 --number 5000\n",
    "python features.py HSS 1 4 --gpt 1 --number 5000\n",
    "\n",
    "python features.py HSS 2 1 --gpt 1 --number 5000\n",
    "python features.py HSS 2 2 --gpt 1 --number 5000\n",
    "python features.py HSS 2 3 --gpt 1 --number 5000\n",
    "python features.py HSS 2 4 --gpt 1 --number 5000\n",
    "\n",
    "python features.py HSS 3 1 --gpt 1 --number 5000\n",
    "python features.py HSS 3 2 --gpt 1 --number 5000\n",
    "python features.py HSS 3 3 --gpt 1 --number 5000\n",
    "python features.py HSS 3 4 --gpt 1 --number 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3e8e67c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T06:52:56.536018Z",
     "iopub.status.busy": "2025-12-31T06:52:56.535361Z",
     "iopub.status.idle": "2025-12-31T06:52:56.940861Z",
     "shell.execute_reply": "2025-12-31T06:52:56.938149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 31 06:52:56 UTC 2025\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150G\t.\r\n"
     ]
    }
   ],
   "source": [
    "!date\n",
    "!du -hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5192c570",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T06:52:56.948018Z",
     "iopub.status.busy": "2025-12-31T06:52:56.947363Z",
     "iopub.status.idle": "2025-12-31T06:54:17.242363Z",
     "shell.execute_reply": "2025-12-31T06:54:17.240214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(domain='HSS_5000', task=1, prompt=0, expid='Unified_HSS_Test_Task1_PromptALL', sample=5000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, early=100.0, save=1, modelid=0, pretrain=1, saved_model='../CheckGPT_presaved_files/saved_experim"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ents/basic/HSS_Task1_Prompt1234/Best_LIT_Task1.pth', dataamount=100, trans=0, splitr=0.8, ablr=1.0, "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr=0.0002, nepochs=100, test=1, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=1, v1=0, ada"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m=1, seed=100, dropout=0.5, batchsize=512)\n",
      "Batch: [0/10], Time used: 5.5495s\n",
      "HSS_50001 Epoch:0, T"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est accuracy: 99.9600%, Acc_GPT: 100.0000%, Acc_Human: 99.8000%, F1: 0.9998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(domain='HSS_5000', task=2, prompt=0, expid='Unified_HSS_Test_Task2_PromptALL', sample=5000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, early=100.0, save=1, modelid=0, pretrain=1, saved_model='../CheckGPT_presaved_files/saved_experim"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ents/basic/HSS_Task2_Prompt1234/Best_LIT_Task2.pth', dataamount=100, trans=0, splitr=0.8, ablr=1.0, "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr=0.0002, nepochs=100, test=1, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=1, v1=0, ada"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m=1, seed=100, dropout=0.5, batchsize=512)\n",
      "Batch: [0/10], Time used: 5.6500s\n",
      "HSS_50002 Epoch:0, T"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est accuracy: 99.8800%, Acc_GPT: 99.9250%, Acc_Human: 99.7000%, F1: 0.9992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(domain='HSS_5000', task=3, prompt=0, expid='Unified_HSS_Test_Task3_PromptALL', sample=5000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, early=100.0, save=1, modelid=0, pretrain=1, saved_model='../CheckGPT_presaved_files/saved_experim"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ents/basic/HSS_Task3_Prompt1234/Best_LIT_Task3.pth', dataamount=100, trans=0, splitr=0.8, ablr=1.0, "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr=0.0002, nepochs=100, test=1, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=1, v1=0, ada"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m=1, seed=100, dropout=0.5, batchsize=512)\n",
      "Batch: [0/10], Time used: 4.5452s\n",
      "HSS_50003 Epoch:0, T"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est accuracy: 99.7000%, Acc_GPT: 99.9500%, Acc_Human: 98.7000%, F1: 0.9981\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "python dnn.py HSS_5000 1 0 Unified_HSS_Test_Task1_PromptALL --pretrain 1 --test 1 --saved-model ../CheckGPT_presaved_files/saved_experiments/basic/HSS_Task1_Prompt1234/Best_LIT_Task1.pth\n",
    "python dnn.py HSS_5000 2 0 Unified_HSS_Test_Task2_PromptALL --pretrain 1 --test 1 --saved-model ../CheckGPT_presaved_files/saved_experiments/basic/HSS_Task2_Prompt1234/Best_LIT_Task2.pth\n",
    "python dnn.py HSS_5000 3 0 Unified_HSS_Test_Task3_PromptALL --pretrain 1 --test 1 --saved-model ../CheckGPT_presaved_files/saved_experiments/basic/HSS_Task3_Prompt1234/Best_LIT_Task3.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad6bd7b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T06:54:17.248950Z",
     "iopub.status.busy": "2025-12-31T06:54:17.248380Z",
     "iopub.status.idle": "2025-12-31T06:54:17.644042Z",
     "shell.execute_reply": "2025-12-31T06:54:17.641281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 31 06:54:17 UTC 2025\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150G\t.\r\n"
     ]
    }
   ],
   "source": [
    "!date\n",
    "!du -hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4afe166",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T06:54:17.650350Z",
     "iopub.status.busy": "2025-12-31T06:54:17.649713Z",
     "iopub.status.idle": "2025-12-31T06:54:36.080449Z",
     "shell.execute_reply": "2025-12-31T06:54:36.077949Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "shopt -s extglob\n",
    "rm -rf embeddings/!(README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fcd5ab",
   "metadata": {},
   "source": [
    "# Cross-prompt & Cross-discipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23b25cb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T06:54:36.086319Z",
     "iopub.status.busy": "2025-12-31T06:54:36.085754Z",
     "iopub.status.idle": "2025-12-31T06:54:36.485980Z",
     "shell.execute_reply": "2025-12-31T06:54:36.483261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 31 06:54:36 UTC 2025\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13G\t.\r\n"
     ]
    }
   ],
   "source": [
    "!date\n",
    "!du -hs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8f7925",
   "metadata": {},
   "source": [
    "## CS Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38eef741",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T06:54:36.493291Z",
     "iopub.status.busy": "2025-12-31T06:54:36.492653Z",
     "iopub.status.idle": "2025-12-31T07:09:22.916935Z",
     "shell.execute_reply": "2025-12-31T07:09:22.913994Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: ground, data num: 50000\n",
      "ground. Complete 0/2000 (0.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 0.4023923873901367s. Overlong: 0\n",
      "ground. Complete 200/2000 (10.0%). Time used: 5.371552467346191"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s. Overlong: 0\n",
      "ground. Complete 400/2000 (20.0%). Time used: 9.830998420715332s. Overlong: 0\n",
      "ground."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Complete 600/2000 (30.0%). Time used: 14.285479545593262s. Overlong: 0\n",
      "ground. Complete 800/2000 (4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0%). Time used: 19.079613208770752s. Overlong: 0\n",
      "ground. Complete 1000/2000 (50.0%). Time used: 23"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".934487104415894s. Overlong: 1\n",
      "ground. Complete 1200/2000 (60.0%). Time used: 29.153687000274658s. O"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verlong: 3\n",
      "ground. Complete 1400/2000 (70.0%). Time used: 33.98317265510559s. Overlong: 3\n",
      "ground. Co"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mplete 1600/2000 (80.0%). Time used: 38.942795276641846s. Overlong: 3\n",
      "ground. Complete 1800/2000 (90"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 43.9098379611969s. Overlong: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: ground_task2, data num: 50000\n",
      "ground_task2. Complete 0/2000 (0."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 0.3972327709197998s. Overlong: 0\n",
      "ground_task2. Complete 200/2000 (10.0%). Time used:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4.570858478546143s. Overlong: 0\n",
      "ground_task2. Complete 400/2000 (20.0%). Time used: 8.5251812934875"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49s. Overlong: 0\n",
      "ground_task2. Complete 600/2000 (30.0%). Time used: 12.463358879089355s. Overlong: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "ground_task2. Complete 800/2000 (40.0%). Time used: 16.528457641601562s. Overlong: 0\n",
      "ground_task2."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Complete 1000/2000 (50.0%). Time used: 20.55259394645691s. Overlong: 0\n",
      "ground_task2. Complete 1200/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 (60.0%). Time used: 24.71529984474182s. Overlong: 0\n",
      "ground_task2. Complete 1400/2000 (70.0%). T"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ime used: 28.78916335105896s. Overlong: 0\n",
      "ground_task2. Complete 1600/2000 (80.0%). Time used: 32.90"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3334617614746s. Overlong: 0\n",
      "ground_task2. Complete 1800/2000 (90.0%). Time used: 36.98622536659241s."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task1_prompt1, data num: 50000\n",
      "gpt_task1_prompt1. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.41405415534973145s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 200/2000 (10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 5.29253625869751s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 400/2000 (20.0%). Time u"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sed: 10.194038391113281s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 600/2000 (30.0%). Time used: 14.99"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5039939880371s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 800/2000 (40.0%). Time used: 19.889092922210"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "693s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 1000/2000 (50.0%). Time used: 24.61943292617798s. Over"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long: 0\n",
      "gpt_task1_prompt1. Complete 1200/2000 (60.0%). Time used: 29.36533212661743s. Overlong: 0\n",
      "gp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_task1_prompt1. Complete 1400/2000 (70.0%). Time used: 34.1813530921936s. Overlong: 0\n",
      "gpt_task1_pro"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpt1. Complete 1600/2000 (80.0%). Time used: 38.92960000038147s. Overlong: 0\n",
      "gpt_task1_prompt1. Comp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lete 1800/2000 (90.0%). Time used: 43.79747247695923s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task1_prompt2, data num: 50000\n",
      "gpt_task1_prompt2. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.412625789642334s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 200/2000 (10.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 5.906924724578857s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 400/2000 (20.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 11.128959894180298s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 600/2000 (30.0%). Time used: 16.404"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24346923828s. Overlong: 1\n",
      "gpt_task1_prompt2. Complete 800/2000 (40.0%). Time used: 21.72699069976806"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6s. Overlong: 1\n",
      "gpt_task1_prompt2. Complete 1000/2000 (50.0%). Time used: 26.887437105178833s. Overl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ong: 1\n",
      "gpt_task1_prompt2. Complete 1200/2000 (60.0%). Time used: 32.12768340110779s. Overlong: 1\n",
      "gpt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_task1_prompt2. Complete 1400/2000 (70.0%). Time used: 37.50073003768921s. Overlong: 1\n",
      "gpt_task1_pro"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpt2. Complete 1600/2000 (80.0%). Time used: 42.57113599777222s. Overlong: 1\n",
      "gpt_task1_prompt2. Comp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lete 1800/2000 (90.0%). Time used: 47.810465812683105s. Overlong: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task1_prompt3, data num: 50000\n",
      "gpt_task1_prompt3. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.41264986991882324s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 200/2000 (10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 5.332850933074951s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 400/2000 (20.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 10.09136962890625s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 600/2000 (30.0%). Time used: 14.72"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7386236190796s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 800/2000 (40.0%). Time used: 19.387238264083"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "862s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 1000/2000 (50.0%). Time used: 24.06830906867981s. Over"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long: 0\n",
      "gpt_task1_prompt3. Complete 1200/2000 (60.0%). Time used: 28.82235598564148s. Overlong: 0\n",
      "gp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_task1_prompt3. Complete 1400/2000 (70.0%). Time used: 33.51364517211914s. Overlong: 0\n",
      "gpt_task1_pr"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ompt3. Complete 1600/2000 (80.0%). Time used: 38.24706411361694s. Overlong: 0\n",
      "gpt_task1_prompt3. Com"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plete 1800/2000 (90.0%). Time used: 42.96210956573486s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task1_prompt4, data num: 50000\n",
      "gpt_task1_prompt4. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.40857863426208496s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 200/2000 (10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 5.423527240753174s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 400/2000 (20.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 10.73270034790039s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 600/2000 (30.0%). Time used: 15.93"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27073097229s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 800/2000 (40.0%). Time used: 20.99556469917297"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 1000/2000 (50.0%). Time used: 26.19171166419983s. Overlo"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng: 0\n",
      "gpt_task1_prompt4. Complete 1200/2000 (60.0%). Time used: 31.32842445373535s. Overlong: 0\n",
      "gpt_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task1_prompt4. Complete 1400/2000 (70.0%). Time used: 36.45274043083191s. Overlong: 0\n",
      "gpt_task1_prom"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt4. Complete 1600/2000 (80.0%). Time used: 41.720760107040405s. Overlong: 0\n",
      "gpt_task1_prompt4. Comp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lete 1800/2000 (90.0%). Time used: 46.706063985824585s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task2_prompt1, data num: 50000\n",
      "gpt_task2_prompt1. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.3943960666656494s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 200/2000 (10."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 5.008385181427002s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 400/2000 (20.0%). Time u"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sed: 9.244862079620361s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 600/2000 (30.0%). Time used: 13.394"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97423171997s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 800/2000 (40.0%). Time used: 17.83939456939697"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 1000/2000 (50.0%). Time used: 22.194572925567627s. Overl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ong: 0\n",
      "gpt_task2_prompt1. Complete 1200/2000 (60.0%). Time used: 26.83256483078003s. Overlong: 0\n",
      "gpt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_task2_prompt1. Complete 1400/2000 (70.0%). Time used: 31.228874683380127s. Overlong: 0\n",
      "gpt_task2_pr"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ompt1. Complete 1600/2000 (80.0%). Time used: 35.74412655830383s. Overlong: 0\n",
      "gpt_task2_prompt1. Com"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plete 1800/2000 (90.0%). Time used: 40.24252152442932s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task2_prompt2, data num: 50000\n",
      "gpt_task2_prompt2. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.39916133880615234s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 200/2000 (10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 5.682801008224487s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 400/2000 (20.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 10.58514142036438s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 600/2000 (30.0%). Time used: 15.41"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6826248168945s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 800/2000 (40.0%). Time used: 20.471875429153"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 1000/2000 (50.0%). Time used: 25.486021757125854s. Ove"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rlong: 0\n",
      "gpt_task2_prompt2. Complete 1200/2000 (60.0%). Time used: 30.780822038650513s. Overlong: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_task2_prompt2. Complete 1400/2000 (70.0%). Time used: 35.82032132148743s. Overlong: 0\n",
      "gpt_task2_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt2. Complete 1600/2000 (80.0%). Time used: 40.94450092315674s. Overlong: 0\n",
      "gpt_task2_prompt2. C"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omplete 1800/2000 (90.0%). Time used: 46.0692663192749s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task2_prompt3, data num: 50000\n",
      "gpt_task2_prompt3. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.393721342086792s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 200/2000 (10.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 4.951857805252075s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 400/2000 (20.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 9.115225315093994s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 600/2000 (30.0%). Time used: 13.2425"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4822731018s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 800/2000 (40.0%). Time used: 17.579548835754395"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 1000/2000 (50.0%). Time used: 21.925442457199097s. Overlo"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng: 0\n",
      "gpt_task2_prompt3. Complete 1200/2000 (60.0%). Time used: 26.480113744735718s. Overlong: 0\n",
      "gpt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_task2_prompt3. Complete 1400/2000 (70.0%). Time used: 30.79222536087036s. Overlong: 0\n",
      "gpt_task2_pro"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpt3. Complete 1600/2000 (80.0%). Time used: 35.243125915527344s. Overlong: 0\n",
      "gpt_task2_prompt3. Com"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plete 1800/2000 (90.0%). Time used: 39.662238121032715s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task2_prompt4, data num: 50000\n",
      "gpt_task2_prompt4. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.40025925636291504s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 200/2000 (10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 4.899103164672852s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 400/2000 (20.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 9.147265911102295s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 600/2000 (30.0%). Time used: 13.31"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4531803131104s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 800/2000 (40.0%). Time used: 17.743031978607"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 1000/2000 (50.0%). Time used: 22.084766149520874s. Ove"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rlong: 0\n",
      "gpt_task2_prompt4. Complete 1200/2000 (60.0%). Time used: 26.67019295692444s. Overlong: 0\n",
      "g"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt_task2_prompt4. Complete 1400/2000 (70.0%). Time used: 31.02860164642334s. Overlong: 0\n",
      "gpt_task2_p"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rompt4. Complete 1600/2000 (80.0%). Time used: 35.505300998687744s. Overlong: 0\n",
      "gpt_task2_prompt4. C"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omplete 1800/2000 (90.0%). Time used: 39.9358594417572s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task3_prompt1, data num: 50000\n",
      "gpt_task3_prompt1. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.428511381149292s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 200/2000 (10.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 5.829982042312622s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 400/2000 (20.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 10.965664386749268s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 600/2000 (30.0%). Time used: 16.038"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10477256775s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 800/2000 (40.0%). Time used: 21.28891515731811"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 1000/2000 (50.0%). Time used: 26.55218195915222s. Overlo"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng: 0\n",
      "gpt_task3_prompt1. Complete 1200/2000 (60.0%). Time used: 32.078612327575684s. Overlong: 0\n",
      "gpt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_task3_prompt1. Complete 1400/2000 (70.0%). Time used: 37.376282930374146s. Overlong: 0\n",
      "gpt_task3_pr"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ompt1. Complete 1600/2000 (80.0%). Time used: 42.665120124816895s. Overlong: 0\n",
      "gpt_task3_prompt1. Co"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mplete 1800/2000 (90.0%). Time used: 47.98152947425842s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task3_prompt2, data num: 50000\n",
      "gpt_task3_prompt2. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.3957710266113281s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 200/2000 (10."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 5.24299955368042s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 400/2000 (20.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 9.758010387420654s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 600/2000 (30.0%). Time used: 14.2256"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75821304321s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 800/2000 (40.0%). Time used: 18.94037222862243"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 1000/2000 (50.0%). Time used: 23.673051118850708s. Overl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ong: 0\n",
      "gpt_task3_prompt2. Complete 1200/2000 (60.0%). Time used: 28.77226448059082s. Overlong: 0\n",
      "gpt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_task3_prompt2. Complete 1400/2000 (70.0%). Time used: 33.535887718200684s. Overlong: 0\n",
      "gpt_task3_pr"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ompt2. Complete 1600/2000 (80.0%). Time used: 38.349926710128784s. Overlong: 0\n",
      "gpt_task3_prompt2. Co"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mplete 1800/2000 (90.0%). Time used: 43.14796328544617s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task3_prompt3, data num: 50000\n",
      "gpt_task3_prompt3. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.40366697311401367s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete 200/2000 (10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 5.259217023849487s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete 400/2000 (20.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 9.757094860076904s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete 600/2000 (30.0%). Time used: 14.21"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "582579612732s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete 800/2000 (40.0%). Time used: 18.9214999675750"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete 1000/2000 (50.0%). Time used: 23.645394325256348s. Over"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long: 0\n",
      "gpt_task3_prompt3. Complete 1200/2000 (60.0%). Time used: 28.760905981063843s. Overlong: 0\n",
      "g"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt_task3_prompt3. Complete 1400/2000 (70.0%). Time used: 33.489372968673706s. Overlong: 0\n",
      "gpt_task3_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt3. Complete 1600/2000 (80.0%). Time used: 38.28827881813049s. Overlong: 0\n",
      "gpt_task3_prompt3. C"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omplete 1800/2000 (90.0%). Time used: 43.11017870903015s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task3_prompt4, data num: 50000\n",
      "gpt_task3_prompt4. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.4003479480743408s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 200/2000 (10."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 5.655636787414551s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 400/2000 (20.0%). Time u"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sed: 10.490007400512695s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 600/2000 (30.0%). Time used: 15.23"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0856657028198s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 800/2000 (40.0%). Time used: 20.290842533111"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 1000/2000 (50.0%). Time used: 25.450046062469482s. Ove"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rlong: 0\n",
      "gpt_task3_prompt4. Complete 1200/2000 (60.0%). Time used: 30.841161251068115s. Overlong: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_task3_prompt4. Complete 1400/2000 (70.0%). Time used: 36.00692796707153s. Overlong: 0\n",
      "gpt_task3_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt4. Complete 1600/2000 (80.0%). Time used: 41.15211033821106s. Overlong: 0\n",
      "gpt_task3_prompt4. C"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omplete 1800/2000 (90.0%). Time used: 46.24097990989685s. Overlong: 0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "\n",
    "python features.py CS 1 1 --gpt 0 --number 2000\n",
    "python features.py CS 2 1 --gpt 0 --number 2000\n",
    "\n",
    "python features.py CS 1 1 --gpt 1 --number 2000\n",
    "python features.py CS 1 2 --gpt 1 --number 2000\n",
    "python features.py CS 1 3 --gpt 1 --number 2000\n",
    "python features.py CS 1 4 --gpt 1 --number 2000\n",
    "\n",
    "python features.py CS 2 1 --gpt 1 --number 2000\n",
    "python features.py CS 2 2 --gpt 1 --number 2000\n",
    "python features.py CS 2 3 --gpt 1 --number 2000\n",
    "python features.py CS 2 4 --gpt 1 --number 2000\n",
    "\n",
    "python features.py CS 3 1 --gpt 1 --number 2000\n",
    "python features.py CS 3 2 --gpt 1 --number 2000\n",
    "python features.py CS 3 3 --gpt 1 --number 2000\n",
    "python features.py CS 3 4 --gpt 1 --number 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e501a7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T07:09:22.926266Z",
     "iopub.status.busy": "2025-12-31T07:09:22.925609Z",
     "iopub.status.idle": "2025-12-31T07:09:23.325086Z",
     "shell.execute_reply": "2025-12-31T07:09:23.322393Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 31 07:09:22 UTC 2025\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68G\t.\r\n"
     ]
    }
   ],
   "source": [
    "!date\n",
    "!du -hs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf055a4",
   "metadata": {},
   "source": [
    "## PHX Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e08bbb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T07:09:23.331835Z",
     "iopub.status.busy": "2025-12-31T07:09:23.331172Z",
     "iopub.status.idle": "2025-12-31T07:24:36.581309Z",
     "shell.execute_reply": "2025-12-31T07:24:36.578202Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: ground, data num: 50000\n",
      "ground. Complete 0/2000 (0.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 0.40926337242126465s. Overlong: 0\n",
      "ground. Complete 200/2000 (10.0%). Time used: 5.46803045272827"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15s. Overlong: 0\n",
      "ground. Complete 400/2000 (20.0%). Time used: 10.389135837554932s. Overlong: 0\n",
      "grou"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nd. Complete 600/2000 (30.0%). Time used: 15.36911153793335s. Overlong: 1\n",
      "ground. Complete 800/2000 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40.0%). Time used: 20.57278847694397s. Overlong: 1\n",
      "ground. Complete 1000/2000 (50.0%). Time used: 2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.730226755142212s. Overlong: 1\n",
      "ground. Complete 1200/2000 (60.0%). Time used: 30.713404417037964s. "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlong: 1\n",
      "ground. Complete 1400/2000 (70.0%). Time used: 35.907432317733765s. Overlong: 2\n",
      "ground. "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete 1600/2000 (80.0%). Time used: 40.662397623062134s. Overlong: 3\n",
      "ground. Complete 1800/2000 ("
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.0%). Time used: 45.80663776397705s. Overlong: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: ground_task2, data num: 50000\n",
      "ground_task2. Complete 0/2000 (0."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 0.4040532112121582s. Overlong: 0\n",
      "ground_task2. Complete 200/2000 (10.0%). Time used:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4.515538215637207s. Overlong: 0\n",
      "ground_task2. Complete 400/2000 (20.0%). Time used: 8.4964315891265"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87s. Overlong: 0\n",
      "ground_task2. Complete 600/2000 (30.0%). Time used: 12.491548299789429s. Overlong: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "ground_task2. Complete 800/2000 (40.0%). Time used: 16.540550231933594s. Overlong: 0\n",
      "ground_task2."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Complete 1000/2000 (50.0%). Time used: 20.580867052078247s. Overlong: 0\n",
      "ground_task2. Complete 1200"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/2000 (60.0%). Time used: 24.565894842147827s. Overlong: 0\n",
      "ground_task2. Complete 1400/2000 (70.0%)."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Time used: 28.61340641975403s. Overlong: 0\n",
      "ground_task2. Complete 1600/2000 (80.0%). Time used: 32."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56301188468933s. Overlong: 0\n",
      "ground_task2. Complete 1800/2000 (90.0%). Time used: 36.62626123428345s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task1_prompt1, data num: 50000\n",
      "gpt_task1_prompt1. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.42028379440307617s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 200/2000 (10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 5.894798278808594s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 400/2000 (20.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 11.33061933517456s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 600/2000 (30.0%). Time used: 16.71"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176767349243s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 800/2000 (40.0%). Time used: 22.0831627845764"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 1000/2000 (50.0%). Time used: 27.42288899421692s. Overl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ong: 0\n",
      "gpt_task1_prompt1. Complete 1200/2000 (60.0%). Time used: 32.76878643035889s. Overlong: 0\n",
      "gpt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_task1_prompt1. Complete 1400/2000 (70.0%). Time used: 38.127920627593994s. Overlong: 1\n",
      "gpt_task1_pr"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ompt1. Complete 1600/2000 (80.0%). Time used: 43.39280295372009s. Overlong: 1\n",
      "gpt_task1_prompt1. Com"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plete 1800/2000 (90.0%). Time used: 48.793896198272705s. Overlong: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task1_prompt2, data num: 50000\n",
      "gpt_task1_prompt2. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.41007161140441895s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 200/2000 (10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 5.364974021911621s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 400/2000 (20.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 10.26154112815857s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 600/2000 (30.0%). Time used: 15.17"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2381162643433s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 800/2000 (40.0%). Time used: 20.136171579360"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 1000/2000 (50.0%). Time used: 24.921793222427368s. Ove"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rlong: 0\n",
      "gpt_task1_prompt2. Complete 1200/2000 (60.0%). Time used: 29.880857229232788s. Overlong: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_task1_prompt2. Complete 1400/2000 (70.0%). Time used: 34.75873017311096s. Overlong: 1\n",
      "gpt_task1_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt2. Complete 1600/2000 (80.0%). Time used: 39.46623992919922s. Overlong: 1\n",
      "gpt_task1_prompt2. C"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omplete 1800/2000 (90.0%). Time used: 44.27053689956665s. Overlong: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task1_prompt3, data num: 50000\n",
      "gpt_task1_prompt3. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.41292834281921387s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 200/2000 (10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 5.017988920211792s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 400/2000 (20.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 9.598731517791748s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 600/2000 (30.0%). Time used: 14.18"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7166213989258s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 800/2000 (40.0%). Time used: 18.795043945312"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 1000/2000 (50.0%). Time used: 23.35634994506836s. Overlo"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng: 0\n",
      "gpt_task1_prompt3. Complete 1200/2000 (60.0%). Time used: 28.00927495956421s. Overlong: 0\n",
      "gpt_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task1_prompt3. Complete 1400/2000 (70.0%). Time used: 32.67288517951965s. Overlong: 0\n",
      "gpt_task1_prom"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt3. Complete 1600/2000 (80.0%). Time used: 37.194048166275024s. Overlong: 0\n",
      "gpt_task1_prompt3. Comp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lete 1800/2000 (90.0%). Time used: 41.80756616592407s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task1_prompt4, data num: 50000\n",
      "gpt_task1_prompt4. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.41276001930236816s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 200/2000 (10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 5.3917272090911865s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 400/2000 (20.0%). Time"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " used: 10.526175260543823s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 600/2000 (30.0%). Time used: 15."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496232032775879s. Overlong: 1\n",
      "gpt_task1_prompt4. Complete 800/2000 (40.0%). Time used: 20.6056349277"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49634s. Overlong: 1\n",
      "gpt_task1_prompt4. Complete 1000/2000 (50.0%). Time used: 25.686277866363525s. O"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verlong: 1\n",
      "gpt_task1_prompt4. Complete 1200/2000 (60.0%). Time used: 30.781094551086426s. Overlong: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "gpt_task1_prompt4. Complete 1400/2000 (70.0%). Time used: 35.76898980140686s. Overlong: 1\n",
      "gpt_task"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_prompt4. Complete 1600/2000 (80.0%). Time used: 40.703343868255615s. Overlong: 1\n",
      "gpt_task1_prompt4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Complete 1800/2000 (90.0%). Time used: 45.7919442653656s. Overlong: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task2_prompt1, data num: 50000\n",
      "gpt_task2_prompt1. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.40486598014831543s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 200/2000 (10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 4.863853454589844s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 400/2000 (20.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 9.214086294174194s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 600/2000 (30.0%). Time used: 13.57"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8394889831543s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 800/2000 (40.0%). Time used: 18.071538686752"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32s. Overlong: 1\n",
      "gpt_task2_prompt1. Complete 1000/2000 (50.0%). Time used: 22.529026985168457s. Over"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long: 1\n",
      "gpt_task2_prompt1. Complete 1200/2000 (60.0%). Time used: 26.9118914604187s. Overlong: 1\n",
      "gpt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_task2_prompt1. Complete 1400/2000 (70.0%). Time used: 31.365941047668457s. Overlong: 1\n",
      "gpt_task2_pr"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ompt1. Complete 1600/2000 (80.0%). Time used: 35.62987494468689s. Overlong: 1\n",
      "gpt_task2_prompt1. Com"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plete 1800/2000 (90.0%). Time used: 40.123825788497925s. Overlong: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task2_prompt2, data num: 50000\n",
      "gpt_task2_prompt2. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.41095447540283203s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 200/2000 (10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 5.564249753952026s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 400/2000 (20.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 10.564294576644897s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 600/2000 (30.0%). Time used: 15.5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52523136138916s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 800/2000 (40.0%). Time used: 20.68167233467"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 1000/2000 (50.0%). Time used: 25.813724279403687s. Ove"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rlong: 1\n",
      "gpt_task2_prompt2. Complete 1200/2000 (60.0%). Time used: 30.82469606399536s. Overlong: 1\n",
      "g"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt_task2_prompt2. Complete 1400/2000 (70.0%). Time used: 35.94097948074341s. Overlong: 2\n",
      "gpt_task2_p"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rompt2. Complete 1600/2000 (80.0%). Time used: 40.84832549095154s. Overlong: 2\n",
      "gpt_task2_prompt2. Co"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mplete 1800/2000 (90.0%). Time used: 45.94244027137756s. Overlong: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task2_prompt3, data num: 50000\n",
      "gpt_task2_prompt3. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.4109351634979248s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 200/2000 (10."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 4.932368516921997s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 400/2000 (20.0%). Time u"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sed: 9.358881711959839s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 600/2000 (30.0%). Time used: 13.763"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11731338501s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 800/2000 (40.0%). Time used: 18.27314186096191"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 1000/2000 (50.0%). Time used: 22.78345823287964s. Overlo"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng: 0\n",
      "gpt_task2_prompt3. Complete 1200/2000 (60.0%). Time used: 27.21019196510315s. Overlong: 0\n",
      "gpt_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task2_prompt3. Complete 1400/2000 (70.0%). Time used: 31.674729347229004s. Overlong: 0\n",
      "gpt_task2_pro"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpt3. Complete 1600/2000 (80.0%). Time used: 35.96129631996155s. Overlong: 0\n",
      "gpt_task2_prompt3. Comp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lete 1800/2000 (90.0%). Time used: 40.529261350631714s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task2_prompt4, data num: 50000\n",
      "gpt_task2_prompt4. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.39923572540283203s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 200/2000 (10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 4.873371839523315s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 400/2000 (20.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 9.206873893737793s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 600/2000 (30.0%). Time used: 13.69"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3289518356323s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 800/2000 (40.0%). Time used: 18.275987863540"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 1000/2000 (50.0%). Time used: 22.872305154800415s. Over"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long: 0\n",
      "gpt_task2_prompt4. Complete 1200/2000 (60.0%). Time used: 27.37506675720215s. Overlong: 0\n",
      "gp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_task2_prompt4. Complete 1400/2000 (70.0%). Time used: 31.96136212348938s. Overlong: 0\n",
      "gpt_task2_pr"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ompt4. Complete 1600/2000 (80.0%). Time used: 36.306623458862305s. Overlong: 0\n",
      "gpt_task2_prompt4. Co"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mplete 1800/2000 (90.0%). Time used: 40.79613924026489s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task3_prompt1, data num: 50000\n",
      "gpt_task3_prompt1. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.4232044219970703s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 200/2000 (10."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 6.028167724609375s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 400/2000 (20.0%). Time u"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sed: 11.529062032699585s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 600/2000 (30.0%). Time used: 17.13"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526678085327s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 800/2000 (40.0%). Time used: 22.8588633537292"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 1000/2000 (50.0%). Time used: 28.542460203170776s. Over"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long: 0\n",
      "gpt_task3_prompt1. Complete 1200/2000 (60.0%). Time used: 34.164151668548584s. Overlong: 0\n",
      "g"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt_task3_prompt1. Complete 1400/2000 (70.0%). Time used: 39.85909628868103s. Overlong: 1\n",
      "gpt_task3_p"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rompt1. Complete 1600/2000 (80.0%). Time used: 45.222482442855835s. Overlong: 2\n",
      "gpt_task3_prompt1. C"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omplete 1800/2000 (90.0%). Time used: 50.894752502441406s. Overlong: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task3_prompt2, data num: 50000\n",
      "gpt_task3_prompt2. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.415036678314209s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 200/2000 (10.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 5.562761068344116s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 400/2000 (20.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 10.570029735565186s. Overlong: 2\n",
      "gpt_task3_prompt2. Complete 600/2000 (30.0%). Time used: 15.672"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16181755066s. Overlong: 5\n",
      "gpt_task3_prompt2. Complete 800/2000 (40.0%). Time used: 20.96118855476379"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4s. Overlong: 5\n",
      "gpt_task3_prompt2. Complete 1000/2000 (50.0%). Time used: 26.21560025215149s. Overlo"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng: 5\n",
      "gpt_task3_prompt2. Complete 1200/2000 (60.0%). Time used: 31.314161777496338s. Overlong: 5\n",
      "gpt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_task3_prompt2. Complete 1400/2000 (70.0%). Time used: 36.612136125564575s. Overlong: 6\n",
      "gpt_task3_pr"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ompt2. Complete 1600/2000 (80.0%). Time used: 41.457719564437866s. Overlong: 6\n",
      "gpt_task3_prompt2. Co"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mplete 1800/2000 (90.0%). Time used: 46.67624878883362s. Overlong: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task3_prompt3, data num: 50000\n",
      "gpt_task3_prompt3. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.4282410144805908s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete 200/2000 (10."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 6.193037271499634s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete 400/2000 (20.0%). Time u"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sed: 11.801642179489136s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete 600/2000 (30.0%). Time used: 17.41"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "089129447937s. Overlong: 2\n",
      "gpt_task3_prompt3. Complete 800/2000 (40.0%). Time used: 23.1920323371887"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2s. Overlong: 2\n",
      "gpt_task3_prompt3. Complete 1000/2000 (50.0%). Time used: 28.917181730270386s. Overl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ong: 3\n",
      "gpt_task3_prompt3. Complete 1200/2000 (60.0%). Time used: 34.57103204727173s. Overlong: 3\n",
      "gpt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_task3_prompt3. Complete 1400/2000 (70.0%). Time used: 40.3828649520874s. Overlong: 5\n",
      "gpt_task3_prom"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt3. Complete 1600/2000 (80.0%). Time used: 45.77272367477417s. Overlong: 6\n",
      "gpt_task3_prompt3. Compl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ete 1800/2000 (90.0%). Time used: 51.54436278343201s. Overlong: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task3_prompt4, data num: 50000\n",
      "gpt_task3_prompt4. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.4174473285675049s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 200/2000 (10."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 5.801134824752808s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 400/2000 (20.0%). Time u"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sed: 11.086093664169312s. Overlong: 3\n",
      "gpt_task3_prompt4. Complete 600/2000 (30.0%). Time used: 16.37"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5445127487183s. Overlong: 6\n",
      "gpt_task3_prompt4. Complete 800/2000 (40.0%). Time used: 21.897176027297"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "974s. Overlong: 8\n",
      "gpt_task3_prompt4. Complete 1000/2000 (50.0%). Time used: 27.392820358276367s. Ove"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rlong: 9\n",
      "gpt_task3_prompt4. Complete 1200/2000 (60.0%). Time used: 32.76920199394226s. Overlong: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_task3_prompt4. Complete 1400/2000 (70.0%). Time used: 38.33893322944641s. Overlong: 13\n",
      "gpt_task3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_prompt4. Complete 1600/2000 (80.0%). Time used: 43.435160398483276s. Overlong: 13\n",
      "gpt_task3_prompt4"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Complete 1800/2000 (90.0%). Time used: 49.050374031066895s. Overlong: 13\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "\n",
    "python features.py PHX 1 1 --gpt 0 --number 2000\n",
    "python features.py PHX 2 1 --gpt 0 --number 2000\n",
    "\n",
    "python features.py PHX 1 1 --gpt 1 --number 2000\n",
    "python features.py PHX 1 2 --gpt 1 --number 2000\n",
    "python features.py PHX 1 3 --gpt 1 --number 2000\n",
    "python features.py PHX 1 4 --gpt 1 --number 2000\n",
    "\n",
    "python features.py PHX 2 1 --gpt 1 --number 2000\n",
    "python features.py PHX 2 2 --gpt 1 --number 2000\n",
    "python features.py PHX 2 3 --gpt 1 --number 2000\n",
    "python features.py PHX 2 4 --gpt 1 --number 2000\n",
    "\n",
    "python features.py PHX 3 1 --gpt 1 --number 2000\n",
    "python features.py PHX 3 2 --gpt 1 --number 2000\n",
    "python features.py PHX 3 3 --gpt 1 --number 2000\n",
    "python features.py PHX 3 4 --gpt 1 --number 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5603d475",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T07:24:36.590177Z",
     "iopub.status.busy": "2025-12-31T07:24:36.589461Z",
     "iopub.status.idle": "2025-12-31T07:24:36.986322Z",
     "shell.execute_reply": "2025-12-31T07:24:36.983599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 31 07:24:36 UTC 2025\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123G\t.\r\n"
     ]
    }
   ],
   "source": [
    "!date\n",
    "!du -hs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbd9ad6",
   "metadata": {},
   "source": [
    "## HSS Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0f1b4c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T07:24:36.993855Z",
     "iopub.status.busy": "2025-12-31T07:24:36.993202Z",
     "iopub.status.idle": "2025-12-31T07:40:30.142941Z",
     "shell.execute_reply": "2025-12-31T07:40:30.139918Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: ground, data num: 50000\n",
      "ground. Complete 0/2000 (0.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 0.41949033737182617s. Overlong: 0\n",
      "ground. Complete 200/2000 (10.0%). Time used: 6.40391564369201"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7s. Overlong: 36\n",
      "ground. Complete 400/2000 (20.0%). Time used: 11.825302839279175s. Overlong: 52\n",
      "gro"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "und. Complete 600/2000 (30.0%). Time used: 17.104740381240845s. Overlong: 64\n",
      "ground. Complete 800/20"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 (40.0%). Time used: 22.579594612121582s. Overlong: 79\n",
      "ground. Complete 1000/2000 (50.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 27.859611749649048s. Overlong: 90\n",
      "ground. Complete 1200/2000 (60.0%). Time used: 33.182828664779"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66s. Overlong: 106\n",
      "ground. Complete 1400/2000 (70.0%). Time used: 38.49061560630798s. Overlong: 120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground. Complete 1600/2000 (80.0%). Time used: 43.84942102432251s. Overlong: 138\n",
      "ground. Complete 18"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00/2000 (90.0%). Time used: 48.966679096221924s. Overlong: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: ground_task2, data num: 50000\n",
      "ground_task2. Complete 0/2000 (0."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 0.429882287979126s. Overlong: 0\n",
      "ground_task2. Complete 200/2000 (10.0%). Time used: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.1853721141815186s. Overlong: 4\n",
      "ground_task2. Complete 400/2000 (20.0%). Time used: 9.7668194770812"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99s. Overlong: 5\n",
      "ground_task2. Complete 600/2000 (30.0%). Time used: 14.072281122207642s. Overlong: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "ground_task2. Complete 800/2000 (40.0%). Time used: 18.46562647819519s. Overlong: 14\n",
      "ground_task2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Complete 1000/2000 (50.0%). Time used: 22.754899978637695s. Overlong: 16\n",
      "ground_task2. Complete 12"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00/2000 (60.0%). Time used: 27.011252403259277s. Overlong: 18\n",
      "ground_task2. Complete 1400/2000 (70.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 31.35698175430298s. Overlong: 19\n",
      "ground_task2. Complete 1600/2000 (80.0%). Time used:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 35.77071475982666s. Overlong: 21\n",
      "ground_task2. Complete 1800/2000 (90.0%). Time used: 40.1363687515"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2588s. Overlong: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task1_prompt1, data num: 50000\n",
      "gpt_task1_prompt1. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.4211595058441162s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 200/2000 (10."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 5.706155776977539s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 400/2000 (20.0%). Time u"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sed: 10.829398155212402s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 600/2000 (30.0%). Time used: 15.91"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3511514663696s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 800/2000 (40.0%). Time used: 21.089799642562"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "866s. Overlong: 0\n",
      "gpt_task1_prompt1. Complete 1000/2000 (50.0%). Time used: 26.242311239242554s. Ove"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rlong: 0\n",
      "gpt_task1_prompt1. Complete 1200/2000 (60.0%). Time used: 31.462287187576294s. Overlong: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_task1_prompt1. Complete 1400/2000 (70.0%). Time used: 36.58529877662659s. Overlong: 0\n",
      "gpt_task1_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt1. Complete 1600/2000 (80.0%). Time used: 41.71527409553528s. Overlong: 0\n",
      "gpt_task1_prompt1. C"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omplete 1800/2000 (90.0%). Time used: 46.90969276428223s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task1_prompt2, data num: 50000\n",
      "gpt_task1_prompt2. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.4145040512084961s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 200/2000 (10."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 5.3965582847595215s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 400/2000 (20.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 10.189168214797974s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 600/2000 (30.0%). Time used: 15.1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81978940963745s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 800/2000 (40.0%). Time used: 20.17757201194"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7632s. Overlong: 0\n",
      "gpt_task1_prompt2. Complete 1000/2000 (50.0%). Time used: 25.09404754638672s. Ove"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rlong: 0\n",
      "gpt_task1_prompt2. Complete 1200/2000 (60.0%). Time used: 30.039296865463257s. Overlong: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_task1_prompt2. Complete 1400/2000 (70.0%). Time used: 34.921855211257935s. Overlong: 1\n",
      "gpt_task1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_prompt2. Complete 1600/2000 (80.0%). Time used: 39.78591775894165s. Overlong: 1\n",
      "gpt_task1_prompt2. "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete 1800/2000 (90.0%). Time used: 44.68154335021973s. Overlong: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task1_prompt3, data num: 50000\n",
      "gpt_task1_prompt3. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.41960620880126953s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 200/2000 (10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 5.357908487319946s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 400/2000 (20.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 10.18001413345337s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 600/2000 (30.0%). Time used: 14.99"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130368232727s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 800/2000 (40.0%). Time used: 19.7475969791412"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35s. Overlong: 0\n",
      "gpt_task1_prompt3. Complete 1000/2000 (50.0%). Time used: 24.654351949691772s. Over"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long: 0\n",
      "gpt_task1_prompt3. Complete 1200/2000 (60.0%). Time used: 29.46916890144348s. Overlong: 0\n",
      "gp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_task1_prompt3. Complete 1400/2000 (70.0%). Time used: 34.219820737838745s. Overlong: 0\n",
      "gpt_task1_p"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rompt3. Complete 1600/2000 (80.0%). Time used: 39.016780376434326s. Overlong: 0\n",
      "gpt_task1_prompt3. C"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omplete 1800/2000 (90.0%). Time used: 43.93494391441345s. Overlong: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task1_prompt4, data num: 50000\n",
      "gpt_task1_prompt4. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.442380428314209s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 200/2000 (10.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 6.075568675994873s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 400/2000 (20.0%). Time us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed: 11.780690670013428s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 600/2000 (30.0%). Time used: 17.556"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "809425354004s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 800/2000 (40.0%). Time used: 23.1648790836334"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23s. Overlong: 0\n",
      "gpt_task1_prompt4. Complete 1000/2000 (50.0%). Time used: 28.811362981796265s. Over"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long: 0\n",
      "gpt_task1_prompt4. Complete 1200/2000 (60.0%). Time used: 34.514792919158936s. Overlong: 0\n",
      "g"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt_task1_prompt4. Complete 1400/2000 (70.0%). Time used: 40.23571848869324s. Overlong: 0\n",
      "gpt_task1_p"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rompt4. Complete 1600/2000 (80.0%). Time used: 45.93057894706726s. Overlong: 1\n",
      "gpt_task1_prompt4. Co"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mplete 1800/2000 (90.0%). Time used: 51.61777114868164s. Overlong: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task2_prompt1, data num: 50000\n",
      "gpt_task2_prompt1. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.43141889572143555s. Overlong: 0\n",
      "gpt_task2_prompt1. Complete 200/2000 (10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 5.827378273010254s. Overlong: 11\n",
      "gpt_task2_prompt1. Complete 400/2000 (20.0%). Time"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " used: 10.99382209777832s. Overlong: 17\n",
      "gpt_task2_prompt1. Complete 600/2000 (30.0%). Time used: 15."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "805123567581177s. Overlong: 22\n",
      "gpt_task2_prompt1. Complete 800/2000 (40.0%). Time used: 20.716212987"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89978s. Overlong: 31\n",
      "gpt_task2_prompt1. Complete 1000/2000 (50.0%). Time used: 25.52082371711731s. O"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verlong: 33\n",
      "gpt_task2_prompt1. Complete 1200/2000 (60.0%). Time used: 30.39039444923401s. Overlong: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "gpt_task2_prompt1. Complete 1400/2000 (70.0%). Time used: 35.33209753036499s. Overlong: 42\n",
      "gpt_ta"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk2_prompt1. Complete 1600/2000 (80.0%). Time used: 40.40227913856506s. Overlong: 46\n",
      "gpt_task2_promp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1. Complete 1800/2000 (90.0%). Time used: 45.32333850860596s. Overlong: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task2_prompt2, data num: 50000\n",
      "gpt_task2_prompt2. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.43809032440185547s. Overlong: 0\n",
      "gpt_task2_prompt2. Complete 200/2000 (10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 6.405470848083496s. Overlong: 3\n",
      "gpt_task2_prompt2. Complete 400/2000 (20.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 12.16608452796936s. Overlong: 6\n",
      "gpt_task2_prompt2. Complete 600/2000 (30.0%). Time used: 17.61"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3261461257935s. Overlong: 7\n",
      "gpt_task2_prompt2. Complete 800/2000 (40.0%). Time used: 23.124074459075"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "928s. Overlong: 8\n",
      "gpt_task2_prompt2. Complete 1000/2000 (50.0%). Time used: 28.536771297454834s. Ove"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rlong: 9\n",
      "gpt_task2_prompt2. Complete 1200/2000 (60.0%). Time used: 33.95982599258423s. Overlong: 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_task2_prompt2. Complete 1400/2000 (70.0%). Time used: 39.46987438201904s. Overlong: 11\n",
      "gpt_task2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_prompt2. Complete 1600/2000 (80.0%). Time used: 44.99937391281128s. Overlong: 16\n",
      "gpt_task2_prompt2."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Complete 1800/2000 (90.0%). Time used: 50.46992039680481s. Overlong: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task2_prompt3, data num: 50000\n",
      "gpt_task2_prompt3. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.48589062690734863s. Overlong: 0\n",
      "gpt_task2_prompt3. Complete 200/2000 (10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 6.281005382537842s. Overlong: 4\n",
      "gpt_task2_prompt3. Complete 400/2000 (20.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 11.91494369506836s. Overlong: 6\n",
      "gpt_task2_prompt3. Complete 600/2000 (30.0%). Time used: 17.23"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0877161026s. Overlong: 7\n",
      "gpt_task2_prompt3. Complete 800/2000 (40.0%). Time used: 22.598078727722168"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s. Overlong: 12\n",
      "gpt_task2_prompt3. Complete 1000/2000 (50.0%). Time used: 27.864938497543335s. Overl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ong: 13\n",
      "gpt_task2_prompt3. Complete 1200/2000 (60.0%). Time used: 33.12645864486694s. Overlong: 14\n",
      "g"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt_task2_prompt3. Complete 1400/2000 (70.0%). Time used: 38.53031921386719s. Overlong: 16\n",
      "gpt_task2_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt3. Complete 1600/2000 (80.0%). Time used: 43.973352909088135s. Overlong: 20\n",
      "gpt_task2_prompt3."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Complete 1800/2000 (90.0%). Time used: 49.391292095184326s. Overlong: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task2_prompt4, data num: 50000\n",
      "gpt_task2_prompt4. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.4861485958099365s. Overlong: 0\n",
      "gpt_task2_prompt4. Complete 200/2000 (10."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 5.761605978012085s. Overlong: 1\n",
      "gpt_task2_prompt4. Complete 400/2000 (20.0%). Time u"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sed: 10.971916913986206s. Overlong: 1\n",
      "gpt_task2_prompt4. Complete 600/2000 (30.0%). Time used: 15.78"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8079261779785s. Overlong: 2\n",
      "gpt_task2_prompt4. Complete 800/2000 (40.0%). Time used: 20.651016950607"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3s. Overlong: 4\n",
      "gpt_task2_prompt4. Complete 1000/2000 (50.0%). Time used: 25.295024633407593s. Overl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ong: 9\n",
      "gpt_task2_prompt4. Complete 1200/2000 (60.0%). Time used: 29.984164476394653s. Overlong: 11\n",
      "g"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt_task2_prompt4. Complete 1400/2000 (70.0%). Time used: 34.82735013961792s. Overlong: 13\n",
      "gpt_task2_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt4. Complete 1600/2000 (80.0%). Time used: 39.704878091812134s. Overlong: 15\n",
      "gpt_task2_prompt4."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Complete 1800/2000 (90.0%). Time used: 44.6389000415802s. Overlong: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task3_prompt1, data num: 50000\n",
      "gpt_task3_prompt1. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.42269349098205566s. Overlong: 0\n",
      "gpt_task3_prompt1. Complete 200/2000 (10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".0%). Time used: 5.438537359237671s. Overlong: 7\n",
      "gpt_task3_prompt1. Complete 400/2000 (20.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 10.461781024932861s. Overlong: 12\n",
      "gpt_task3_prompt1. Complete 600/2000 (30.0%). Time used: 15."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200439691543579s. Overlong: 13\n",
      "gpt_task3_prompt1. Complete 800/2000 (40.0%). Time used: 20.120471954"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345703s. Overlong: 16\n",
      "gpt_task3_prompt1. Complete 1000/2000 (50.0%). Time used: 24.924254179000854s."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Overlong: 17\n",
      "gpt_task3_prompt1. Complete 1200/2000 (60.0%). Time used: 29.689040899276733s. Overlon"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g: 17\n",
      "gpt_task3_prompt1. Complete 1400/2000 (70.0%). Time used: 34.60332250595093s. Overlong: 20\n",
      "gpt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_task3_prompt1. Complete 1600/2000 (80.0%). Time used: 39.421027421951294s. Overlong: 25\n",
      "gpt_task3_p"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rompt1. Complete 1800/2000 (90.0%). Time used: 44.236684799194336s. Overlong: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task3_prompt2, data num: 50000\n",
      "gpt_task3_prompt2. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.446307897567749s. Overlong: 0\n",
      "gpt_task3_prompt2. Complete 200/2000 (10.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%). Time used: 6.554479360580444s. Overlong: 10\n",
      "gpt_task3_prompt2. Complete 400/2000 (20.0%). Time u"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sed: 12.384152173995972s. Overlong: 23\n",
      "gpt_task3_prompt2. Complete 600/2000 (30.0%). Time used: 17.9"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7233510017395s. Overlong: 29\n",
      "gpt_task3_prompt2. Complete 800/2000 (40.0%). Time used: 23.80857586860"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6567s. Overlong: 36\n",
      "gpt_task3_prompt2. Complete 1000/2000 (50.0%). Time used: 29.444451808929443s. O"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verlong: 38\n",
      "gpt_task3_prompt2. Complete 1200/2000 (60.0%). Time used: 35.13449430465698s. Overlong: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "gpt_task3_prompt2. Complete 1400/2000 (70.0%). Time used: 40.821274280548096s. Overlong: 51\n",
      "gpt_t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ask3_prompt2. Complete 1600/2000 (80.0%). Time used: 46.51301574707031s. Overlong: 60\n",
      "gpt_task3_prom"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt2. Complete 1800/2000 (90.0%). Time used: 52.11237120628357s. Overlong: 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task3_prompt3, data num: 50000\n",
      "gpt_task3_prompt3. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.4206228256225586s. Overlong: 0\n",
      "gpt_task3_prompt3. Complete 200/2000 (10."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 5.895302772521973s. Overlong: 14\n",
      "gpt_task3_prompt3. Complete 400/2000 (20.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 11.058345794677734s. Overlong: 25\n",
      "gpt_task3_prompt3. Complete 600/2000 (30.0%). Time used: 15."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943894863128662s. Overlong: 26\n",
      "gpt_task3_prompt3. Complete 800/2000 (40.0%). Time used: 20.994965791"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70227s. Overlong: 31\n",
      "gpt_task3_prompt3. Complete 1000/2000 (50.0%). Time used: 25.847283363342285s. "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlong: 33\n",
      "gpt_task3_prompt3. Complete 1200/2000 (60.0%). Time used: 30.791597366333008s. Overlong"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 36\n",
      "gpt_task3_prompt3. Complete 1400/2000 (70.0%). Time used: 35.888657093048096s. Overlong: 38\n",
      "gpt"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_task3_prompt3. Complete 1600/2000 (80.0%). Time used: 40.85552644729614s. Overlong: 45\n",
      "gpt_task3_pr"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ompt3. Complete 1800/2000 (90.0%). Time used: 45.792847871780396s. Overlong: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/cc/.cache/torch/hub/pytorch_fairseq_main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.ta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r.gz HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rge.tar.gz from cache at /home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ose()\n",
      "  deprecation_warning(message=message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/core/defaul"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @packag"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more inf"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ormation\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'confi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/compose.py:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/CheckGPT-reproduction/artifact_checkgpt/.venv/lib/python3.9/site-packages/hydra/experimenta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/cc/CheckGPT-reproduction/artifact_checkg"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt/.venv/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cc/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "onfig' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automati"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quant"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_tr"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aining': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_ra"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nk': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_vi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ew': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_mo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunk"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "educe_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'v"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'c"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "urriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'up"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "date_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': Fals"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'res"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "der': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'check"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "point_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_che"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ckpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'avera"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'sc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ore_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_top"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': Fal"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "se, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_wo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rd_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, me"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_lo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_valid"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameter"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, upd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ate_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimize"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_sta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "te=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attenti"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_e"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mbed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nt_heads=True, data='/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_sourc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_ke"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classifi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cation_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_name': 'masked_lm', 'data': '/home/cc/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_beta"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "blicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fair"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 's"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imul_type': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re loading depends on it.\n",
      "Data name: gpt_task3_prompt4, data num: 50000\n",
      "gpt_task3_prompt4. Complete "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2000 (0.0%). Time used: 0.4283287525177002s. Overlong: 0\n",
      "gpt_task3_prompt4. Complete 200/2000 (10."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%). Time used: 6.690837383270264s. Overlong: 18\n",
      "gpt_task3_prompt4. Complete 400/2000 (20.0%). Time "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 12.629905939102173s. Overlong: 28\n",
      "gpt_task3_prompt4. Complete 600/2000 (30.0%). Time used: 18."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390199661254883s. Overlong: 39\n",
      "gpt_task3_prompt4. Complete 800/2000 (40.0%). Time used: 24.302851915"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359497s. Overlong: 45\n",
      "gpt_task3_prompt4. Complete 1000/2000 (50.0%). Time used: 30.0771746635437s. O"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verlong: 48\n",
      "gpt_task3_prompt4. Complete 1200/2000 (60.0%). Time used: 35.98161721229553s. Overlong: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "gpt_task3_prompt4. Complete 1400/2000 (70.0%). Time used: 41.79994082450867s. Overlong: 57\n",
      "gpt_ta"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk3_prompt4. Complete 1600/2000 (80.0%). Time used: 47.60618019104004s. Overlong: 63\n",
      "gpt_task3_promp"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t4. Complete 1800/2000 (90.0%). Time used: 53.362547159194946s. Overlong: 69\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "\n",
    "python features.py HSS 1 1 --gpt 0 --number 2000\n",
    "python features.py HSS 2 1 --gpt 0 --number 2000\n",
    "\n",
    "python features.py HSS 1 1 --gpt 1 --number 2000\n",
    "python features.py HSS 1 2 --gpt 1 --number 2000\n",
    "python features.py HSS 1 3 --gpt 1 --number 2000\n",
    "python features.py HSS 1 4 --gpt 1 --number 2000\n",
    "\n",
    "python features.py HSS 2 1 --gpt 1 --number 2000\n",
    "python features.py HSS 2 2 --gpt 1 --number 2000\n",
    "python features.py HSS 2 3 --gpt 1 --number 2000\n",
    "python features.py HSS 2 4 --gpt 1 --number 2000\n",
    "\n",
    "python features.py HSS 3 1 --gpt 1 --number 2000\n",
    "python features.py HSS 3 2 --gpt 1 --number 2000\n",
    "python features.py HSS 3 3 --gpt 1 --number 2000\n",
    "python features.py HSS 3 4 --gpt 1 --number 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a011f9e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T07:40:30.151533Z",
     "iopub.status.busy": "2025-12-31T07:40:30.150872Z",
     "iopub.status.idle": "2025-12-31T07:40:30.553413Z",
     "shell.execute_reply": "2025-12-31T07:40:30.550791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 31 07:40:30 UTC 2025\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177G\t.\r\n"
     ]
    }
   ],
   "source": [
    "!date\n",
    "!du -hs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dd323b",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69218490",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T07:40:30.560989Z",
     "iopub.status.busy": "2025-12-31T07:40:30.560358Z",
     "iopub.status.idle": "2025-12-31T07:42:52.506360Z",
     "shell.execute_reply": "2025-12-31T07:42:52.504369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(domain='ALL_2000', task=1, prompt=0, expid='Unified_ALL_Test_Task1_PromptALL', sample=2000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", early=100.0, save=1, modelid=0, pretrain=1, saved_model='../CheckGPT_presaved_files/saved_experime"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nts/basic/ALL_Task1_Prompt1234/Best_ALL_Task1.pth', dataamount=100, trans=0, splitr=0.8, ablr=1.0, l"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r=0.0002, nepochs=100, test=1, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=1, v1=0, adam"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=1, seed=100, dropout=0.5, batchsize=512)\n",
      "Batch: [0/12], Time used: 8.3600s\n",
      "ALL_20001 Epoch:0, Te"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "st accuracy: 100.0000%, Acc_GPT: 100.0000%, Acc_Human: 100.0000%, F1: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(domain='ALL_2000', task=2, prompt=0, expid='Unified_ALL_Test_Task2_PromptALL', sample=2000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", early=100.0, save=1, modelid=0, pretrain=1, saved_model='../CheckGPT_presaved_files/saved_experime"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nts/basic/ALL_Task2_Prompt1234/Best_ALL_Task2.pth', dataamount=100, trans=0, splitr=0.8, ablr=1.0, l"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r=0.0002, nepochs=100, test=1, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=1, v1=0, adam"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=1, seed=100, dropout=0.5, batchsize=512)\n",
      "Batch: [0/12], Time used: 4.6545s\n",
      "ALL_20002 Epoch:0, Te"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "st accuracy: 99.8333%, Acc_GPT: 99.8750%, Acc_Human: 99.6667%, F1: 0.9990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(domain='ALL_2000', task=3, prompt=0, expid='Unified_ALL_Test_Task3_PromptALL', sample=2000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", early=100.0, save=1, modelid=0, pretrain=1, saved_model='../CheckGPT_presaved_files/saved_experime"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nts/basic/ALL_Task3_Prompt1234/Best_ALL_Task3.pth', dataamount=100, trans=0, splitr=0.8, ablr=1.0, l"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r=0.0002, nepochs=100, test=1, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=1, v1=0, adam"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=1, seed=100, dropout=0.5, batchsize=512)\n",
      "Batch: [0/12], Time used: 5.6497s\n",
      "ALL_20003 Epoch:0, Te"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "st accuracy: 99.5833%, Acc_GPT: 99.6250%, Acc_Human: 99.4167%, F1: 0.9974\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "\n",
    "python dnn.py ALL_2000 1 0 Unified_ALL_Test_Task1_PromptALL --pretrain 1 --test 1 --saved-model ../CheckGPT_presaved_files/saved_experiments/basic/ALL_Task1_Prompt1234/Best_ALL_Task1.pth --sample 2000\n",
    "python dnn.py ALL_2000 2 0 Unified_ALL_Test_Task2_PromptALL --pretrain 1 --test 1 --saved-model ../CheckGPT_presaved_files/saved_experiments/basic/ALL_Task2_Prompt1234/Best_ALL_Task2.pth --sample 2000\n",
    "python dnn.py ALL_2000 3 0 Unified_ALL_Test_Task3_PromptALL --pretrain 1 --test 1 --saved-model ../CheckGPT_presaved_files/saved_experiments/basic/ALL_Task3_Prompt1234/Best_ALL_Task3.pth --sample 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7647f8c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T07:42:52.511124Z",
     "iopub.status.busy": "2025-12-31T07:42:52.510850Z",
     "iopub.status.idle": "2025-12-31T07:42:52.905825Z",
     "shell.execute_reply": "2025-12-31T07:42:52.903135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 31 07:42:52 UTC 2025\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177G\t.\r\n"
     ]
    }
   ],
   "source": [
    "!date\n",
    "!du -hs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c771d322",
   "metadata": {},
   "source": [
    "# Cross-prompt, -discipline, & -task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37118753",
   "metadata": {},
   "source": [
    "Uses the extracted features of previous experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2629bd6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T07:42:52.912514Z",
     "iopub.status.busy": "2025-12-31T07:42:52.911870Z",
     "iopub.status.idle": "2025-12-31T07:43:19.313436Z",
     "shell.execute_reply": "2025-12-31T07:43:19.310404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(domain='ALL_2000', task=0, prompt=0, expid='Unified_ALL_Test_TaskALL_PromptALL', sample=20"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00, early=100.0, save=1, modelid=0, pretrain=1, saved_model='../CheckGPT_presaved_files/saved_experi"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ments/basic/ALL_Task123_Prompt1234/Best_ALL_Task0.pth', dataamount=100, trans=0, splitr=0.8, ablr=1."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, lr=0.0002, nepochs=100, test=1, mdomain='CS', mtask=1, mprompt=0, mid='00001', printall=1, v1=0, "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adam=1, seed=100, dropout=0.5, batchsize=512)\n",
      "Batch: [0/33], Time used: 6.1931s\n",
      "ALL_20000 Epoch:0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", Test accuracy: 99.8214%, Acc_GPT: 99.8542%, Acc_Human: 99.6250%, F1: 0.9990\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "\n",
    "python dnn.py ALL_2000 0 0 Unified_ALL_Test_TaskALL_PromptALL --pretrain 1 --test 1 --saved-model ../CheckGPT_presaved_files/saved_experiments/basic/ALL_Task123_Prompt1234/Best_ALL_Task0.pth --sample 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef34680f",
   "metadata": {},
   "source": [
    "# Post-experiment\n",
    "\n",
    "Time-taken : 1684.7 minutes / 28 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acf9155",
   "metadata": {},
   "source": [
    "Free up disk space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54281ccb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-31T07:43:19.320399Z",
     "iopub.status.busy": "2025-12-31T07:43:19.319721Z",
     "iopub.status.idle": "2025-12-31T07:43:37.388810Z",
     "shell.execute_reply": "2025-12-31T07:43:37.386832Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ./artifact_checkgpt/.venv/bin/activate\n",
    "cd ~/CheckGPT-reproduction/artifact_checkgpt/CheckGPT/\n",
    "shopt -s extglob\n",
    "rm -rf embeddings/!(README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
